{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聚类满足隐式生成模型\n",
    "\n",
    "## 摘要  \n",
    "聚类是无监督学习的基石，它可以被认为是分离数据背后的多重生成机制。本文介绍了一个算法框架来训练隐式生成模型的混合体，这些模型是我们为变分自编码器特别设计的。基于一组额外的鉴别器，我们提出了一种竞争过程，其中模型只需要近似数据分布的一部分，从中可以产生真实的样本。作为一个副产品，每一个模型都更易于训练，聚类解释自然地产生于模型之间训练点的划分。实验表明，我们的方法合理地分割了训练分布，提高了样本的质量   \n",
    "  \n",
    "## 引言  \n",
    "  \n",
    "近年来，（内隐）生成模型在机器学习中引起了广泛的关注。最突出的两种方法是生成对抗网络（GANs）（Goodfellow等人，2014）和可变自动编码器（V AEs）（Kingma&Welling，2013）。这两种方法的目的都是尽量减少真实数据分布与模型学习数据之间的差异。模型分布通常用神经网络参数化，该神经网络将随机向量转换为训练数据（如图像）空间中的样本。可变AUT编码器使对数似然最大化，并且能够对具有连续潜在变量和难以处理的后验概率模型进行有效的近似推断。不幸的是，当应用于自然图像时，V-AEs会产生模糊的样本。甘采取完全不同的方法，依靠对抗性训练。这导致了令人印象深刻的实证结果。另一方面，对抗性训练是有代价的。甘更难训练，并遭受模式崩溃的问题。解决这个问题的一种方法是顺序（Tolstikhin et al.，2017）或并行（Hoang et al.，2017）训练多个生成模型。与GANs相比，如果不以足够高的概率对数据分布的整体支持进行建模，V AEs将遭受巨大的损失。因此，它们通常在数据分布支持之外的区域放置一个显著的概率质量  \n",
    "  \n",
    "我们的目标是缩小这一差距，开发一种通用的方法来并行训练多个生成模型，这些模型集中在训练分布的不同部分。我们将这个框架具体化为V-AEs。因此，每一个V-AE将能够在某些模式下崩溃，而生成器（解码器）的混合仍将近似于整个数据分布。借用聚类和因果关系文献的观点，我们假设数据是由独立机制生成的，即总体分布的生成过程由相互不通知也不相互影响的独立模块组成（Peters等人，2017）。考虑一个变量x0的特殊情况，它是由（混合）几个独立的源X1。，xk因果图中没有双亲。在这种情况下，因果生成模型可以写成    \n",
    "  \n",
    "    \n",
    " 注意，只有一个机制，p（X0 | X1。，XK），实现混合，仍然是一个有条件的；其他的减少到无条件分布，因为源没有父母。条件可以写成结构方程（Pearl，2000），形式为X0:=f（X1。，XK，j）≡Xj，其中j是一个离散噪声变量，取{1。，K}。j的分布决定了混合系数。这个结构方程将条件表示为一个由噪声函数表示的机制。假设每个训练点都是由Xj机制生成的，但是我们只能观察到所有这些实现的混合x0。恢复这些机制相当于学习一种特定类型的结构性因果生成模型，它可以形成更复杂因果模型的构建块（Schölkopf et al.，2016）。我们做了一个简化的假设，即不同机制的支撑不重叠，因此，如果我们观察到X0的两个相同实现，我们假设它们是由同一个机制生成的。  \n",
    "   \n",
    " ## 独立生成模型的训练  \n",
    "   \n",
    "   设X是由数据空间X的N个样本X组成的数据集，它是X0的实现。此外，假设Pd是在数据空间X上定义的未知数据分布，支持X通过最小化f-散度，用易于采样的分布Pmodel=Pk j=1αjpgj近似（Nowozin等人，2016）。每个组件pgjs都应该专门处理其中一个生成机制。直观地说，我们的训练过程与k-means算法有关。在k-means算法中，首先通过质心对训练数据进行解耦，然后根据分配更新质心。我们的方法非正式地描述在算法1中。对于给定的赋值函数cj，我们将dPdj定义为通过规范化dPd（x）cj（x）得到的密度。我们进一步定义了加权αjas，即dPdj的归一化常数。这可以通过计算分配给j-th生成器的训练点数量来进行经验估计。我们可以通过最小化f-散度的上界来解耦生成器的训练（附录中的证明）：  \n",
    "     \n",
    "   由于方程（3）中求和中的每一项都是独立的，因此可以独立地训练每个生成模型来近似dP（t）dj。在对生成模型进行训练后，我们对生成模型进行修正，并通过最大化似然估计来更新每个训练点的分配。我们训练一个鉴别器来区分Pgj和Pd的样本  \n",
    "     \n",
    "   注意，这种近似只有在训练点上计算时才有意义。因此，我们将DP（t）GJ（席）近似作为训练集上的经验估计。现在我们将每个训练点分配给产生最相似样本的机制J，即C（t + 1）j（席）＝1，如果j＝ARG Max LDP（t）GL（席），而0则不然。图1描绘了一个使用V AEs解码器作为生成器（我们称之为kV AEs）的竞争培训程序的草图。在附录中，我们进一步讨论聚类解释  \n",
    "     \n",
    "## 概念的实验证明  \n",
    "  \n",
    "  在图2中，我们描述了我们的算法的输出，这些数据来自一个具有5种不同模式的分布的合成数据，其中一种模式比其他模式更复杂。在表1a中，我们报告了生成模型分布下真实数据的对数似然  \n",
    "    \n",
    "  对于MNIST实验，我们不知道模式的数目。没有理由相信最佳模式数应该是位数。我们任意使用了15个模型，以捕捉数字之间的风格差异，这是根据（Tolstikhin et al.，2017）的见解。注意，不同的V ae确实专门针对数据分布的不同部分，因为类似的数字和样式往往被组合在一起。在表1b中，我们报告了FID得分（Heusel等人，2017年），并将其与统一训练的15 V AEs的集合进行了比较，单个V AE的容量与信号群中的相同（每层最多64个过滤器）和更大的VAE（每层最多512个过滤器）。我们注意到，我们的方法增加了单个模型的FID分数，并且与较大的模型相比具有竞争力。同时，我们的方法更具可伸缩性，因为它允许在不同的vae之间解耦整体模型的复杂性。所有的实验细节都在附录中  \n",
    "    \n",
    "  表1：（a）千伏声发射算法迭代100次后，生成分布下真实数据的对数似然，每层50个单元，100个装袋周期，1000个单次大V AE（每层150个单元）。（b） MNIST上的FID分数。训练集的随机分割会损害模型的性能，因为它不携带任何语义，而且每个模型的总体训练数据较少  \n",
    "    \n",
    "## 结论  \n",
    "  \n",
    "  在这篇文章中，我们介绍了一个使用隐式生成模型的聚类过程，鼓励他们生成更真实的样本。我们训练容量有限的网络，让它们相互竞争以获得更真实的样本。我们从经验上验证了该模型能够成功地恢复真实的生成机制，并且通常允许生成更接近数据分布支持的样本。在MNIST中，我们得到的FID分数与较大V AEs的FID分数具有竞争性。我们提出的方法是非常模块化的，有几个可能的扩展。例如，如果有足够的计算资源，可以显著增加生成模型的数量  \n",
    "    \n",
    "      \n",
    "## 聚类的解释  \n",
    "  \n",
    "  我们介绍的一般方法与聚类密切相关。在本节中，我们将根据我们的框架重新讨论经典的聚类概念。我们证明了我们在非度量空间中推广了k-均值，当空间是欧氏空间时，我们恢复了它。在聚类的生成性解释中，假设数据是从每个质心µ生成的，其具有加性高斯噪声向量，即x=µ+？。当不同质心之间的数据解耦时，这个公式自然会产生一个欧几里德成本。不幸的是，我们知道欧几里德距离不是一个很好的图像聚类指标。我们在这篇文章中的目标是在一个空间度量未知的环境中，通过生成机制找到数据的聚类。现在我们将展示如何从我们的框架中恢复k-means集群。假设数据是由高斯混合函数生成的。我们可以用一个变分界来下界：log（P（X））≥xixjqi（j）log？P（席，j）气（J）？其中q是后验函数的变分近似，j是分量的指数。然后可以简单地重写P（Xi，j）＝P（席席j）p（j）。然后，对于高斯混合模型，参数化高斯分布的P（席j j）。如果高斯是各向同性的，当协方差为零时，我们得到后验函数的变分逼近qi（j）退化为一个硬赋值。我们用一个隐式生成模型来参数化P（席j），而不是用高斯分布来逼近生成模型。如果Pjis是变分自动编码器的译码器，我们可以通过假设编码器的平均值恒定且与x无关来获得k-均值算法。注意，V-AEs被训练成最大化对数似然，就像在EM中一样。假设我们有一个高斯编码器，它将所有输入映射到一个点（退化高斯，σ=0）。现在，假设我们有解码器的身份。这个过程的结果是，当使用重参数化技巧训练AUT编码器时，必须最小化Ex∼Pdj？−logPgj（x |µj）？=前∼Pdj？1 2kx−µjk2。然后，使用EM计算（退化）变分分布的更新：qi（j=1）=limσ→0αe−kxi−jk/2σP jαje−kxi−µjk/2σ，并回顾logPgj=−kxi−µjk2/2，我们注意到退化后验是通过最大化似然获得的。我们不使用AUT编码器损耗，而是估计使用鉴别器来解释我们可能没有明确的距离概念这一事实。在欧几里德空间中，可以简单地在vae的输出（即质心）和训练点之间使用最近邻分类器。请注意，此过程正是k-均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
