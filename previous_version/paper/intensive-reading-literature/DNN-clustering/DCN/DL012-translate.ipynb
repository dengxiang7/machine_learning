{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迈向K均值友好空间：同时进行深度学习和聚类\n",
    "#### 摘要\n",
    "大多数学习方法都将降维（DR）和聚类分开（即顺序地）进行处理，但是最近的研究表明，共同优化两个任务可以显着提高两者的性能。 后一种类型的前提是，数据样本是通过易于表示的潜在表示的线性变换获得的。 但实际上，从潜在空间到数据的转换会更加复杂。 在这项工作中，我们假设此变换是未知的，并且可能是非线性函数。 为了恢复“集群友好”的潜在表示并更好地对数据进行聚类，我们提出了一种DR和K-means聚类联合方法，其中DR是通过学习深度神经网络（DNN）来完成的。 这样做的动机是保留联合优化两个任务的优势，同时利用深层神经网络近似任何非线性函数的能力。 这样，建议的方法就可以适用于广泛的生成模型类。 为此，我们精心设计了DNN结构和相关的联合优化准则，并提出了一种有效且可扩展的算法来处理所提出的优化问题。 使用五个不同的实际数据集进行的实验证明了该方法的有效性  \n",
    "    \n",
    "      \n",
    " #### 引言\n",
    " 集群是数据挖掘和机器学习中最基本的任务之一，应用程序层出不穷。 这也是一项众所周知的艰巨任务，其结果受许多因素影响，包括数据采集和表示，使用预处理（例如降维（DR），聚类标准和优化算法的选择以及初始化）[2]  ，7]。  K-means可以说是聚类算法的基础。 自1957年劳埃德（Lloyd）引入它（于1982年晚些时候出版[15]）以来，由于其简单性和有效性，K-means已被广泛使用，无论是单独使用还是与合适的预处理方法一起使用。  K均值适合于对均匀分布在某些质心周围的数据样本进行聚类（请参见图1中的第一个子图），但是许多现实生活中的数据集都没有这种“ K均值友好”的结构。 在将高维数据映射到适合执行K均值的特定空间上已经花费了很多精力。实现此目标的较早方法包括经典的DR算法，即主成分分析（PCA）和规范相关分析（CCA）。 非负矩阵分解（NMF）和稀疏编码（字典学习）等最新的DR工具也引起了很多关注。 除了学习线性DR算子（例如投影矩阵）的上述方法外，还考虑了非线性DR技术，例如在频谱聚类[19]和稀疏子空间聚类[28、32]中使用的那些技术。\n",
    "   近年来，受深层神经网络（DNN）在监督学习中的成功推动，未经监督的深度学习方法现已广泛用于聚类之前的DR。 例如，堆叠式自动编码器（SAE）[27]，深层CCA（DCCA）[1]和稀疏自动编码器[18]分别从PCA，CCA和稀疏编码中获取见解，并利用DNN来学习非线性映射 从数据域到低维潜在空间。 这些方法将其DNN视为与后续聚类阶段分开设计的预处理阶段。 希望这些DNN学习的数据的潜在表示形式自然适合于聚类。 但是，由于在学习过程中没有明确包含聚类驱动的目标，因此学习的DNN不一定会输出适合聚类的降维数据，这将在我们的实验中看到。在[30]中，Yang等。 等人考虑共同执行灾难恢复和群集。  [30]的基本原理是，如果存在一些潜在空间，在这些空间中实体很好地落入了簇，那么很自然地寻求揭示这些的DR转换，即，产生低的K均值聚类成本。 这样做的动机是利用潜在空间中的K均值成本作为先验，这有助于选择正确的DR，并推动DR产生K均值友好的表示。 通过执行联合DR和K-means聚类，已在[30]中观察到了显着改善的聚类结果。  [30]的局限性（另请参见[6，20]）是假定可观察数据是通过简单的线性变换从潜在聚类友好空间生成的。 尽管简单的线性变换在许多情况下效果很好，但在其他情况下，生成过程则更为复杂，涉及非线性映射。  在这项工作中：我们提出了一种DR和K-means聚类联合框架，其中DR部分是通过学习DNN而非线性模型来实现的。\n",
    "   基本原理是，DNN能够使用合理数量的参数来逼近任何非线性连续函数[8]，因此有望克服[30]中工作的局限性。 尽管实施此想法非常简单（比[30]更具挑战性，在DR [DR]部分仅需要学习线性模型的情况下），但我们的目标却动机良好：通过更好地建模数据转换过程，可以大大提高K 可以学习到-意味着友好的潜在空间-正如我们将演示的。 在图1中可以看到使用我们提出的方法可以预期的性能类型，在这里我们生成了四个二维数据簇，它们在二维欧几里得空间中很好地分离，然后进行变换 使用复杂的非线性映射将它们映射到100维空间[cf.\n",
    "   （4.9）]破坏簇结构。 我们对100-D数据应用了几种广为人知的DR方法，并观察了恢复的2-D空间（有关不同首字母缩写的说明，请参见第4.1节）。 可以看到，该算法输出的降维数据最适合应用K均值。 我们的具体贡献如下：优化标准设计：我们为基于DNN的DR和K均值聚类提出了一个优化标准。 该标准是降维，数据重建和集群结构促进正则化这三个部分的组合，该模型从[30]中的线性生成模型案例中获得了见解，但其上限更为强大 图灵非线性模型。 我们特意将重建部分包括在内，并使用解码网络将其实现，这对于避免琐碎的解决方案至关重要。\n",
    "   该标准也很灵活-可以扩展到公司不同的DNN结构和聚类标准，例如子空间聚类。有效且可扩展的优化过程：公式化的优化问题非常具有挑战性，因为它涉及由K均值部分引起的非线性激活函数层和整数约束。 我们提出了一个明智地设计的解决方案包，其中包括经验有效的初始化和新颖的交替随机梯度算法。 该算法结构简单，可以在线实施，并且具有很高的可扩展性。全面的实验和验证：我们提供了一组综合数据实验，并在五个不同的实际数据集（包括各种文档和图像副本）上验证了该方法。 对于我们实验的所有数据集，均观察到明显的改进。  \n",
    "     \n",
    "       \n",
    "         \n",
    " ### 背景和问题表述\n",
    " K均值聚类和DR给定一组数据向量{xi}，聚类的任务是将N个数据向量分为K个类别。K-means通过优化以下成本函数来完成此任务\n",
    "  ![](../img/km1.PNG)\n",
    "  其中si是仅具有一个非零元素的数据点i的赋值矢量，sj，i表示si的第j个元素，M的第k列即mk表示第k个簇的质心当数据点在欧几里得空间中的质心周围均匀分散时，K均值效果很好。 例如，当数据样本表现出如图1的第一个子图所示的几何分布时，K均值可以轻松地区分四个聚类。 因此，我们认为数据集的结构与“ K均值友好”相似。 但是，高维数据通常不是非常K均值友好的。在实践中，使用DR预处理（例如PCA或NMF）将xi的维数减小到低得多的维数空间，然后应用K均值通常会得到更好的结果,洞察力很简单：数据点在某些潜在特征空间中可能表现出更好的K均值友好结构，而DRA方法（例如PCA和NMF）有助于找到该空间。 除了上述经典的DR方法从本质上学习从潜在空间到数据域的线性生成模型外，非线性DR方法（如在光谱聚类和基于DNN的DR中使用的方法）也被广泛用作K-之前的预处理 均值或其他聚类算法;联合DR和聚类在文献[6，20，30]中也考虑了联合DR和聚类，而不是将DR用作预处理。 该工作线可以总结如下。 考虑生成模型，其中通过xi = W hi生成数据样本，其中W∈RM×R和hi∈RR，其中R M。\n",
    "   假设数据簇在潜在域（即hi所在的位置）中完全分开，但由于W引入的转换而失真参考文献[30]将联合优化问题表述为：\n",
    "   ![](../img/dl012_1.PNG)\n",
    "   其中X = [x1，...  。  。  ，xN]，H = [h1，。  。  。  ，hN]和λ≥0是用于平衡数据保真度和潜在簇结构的参数.第一项执行DR，第二项执行潜在聚类术语r1（·）和r2（·）是正则化（例如，非负性或稀疏性）以防止琐碎的解，例如H→0∈RR×N；在许多情况下，（2.2）中的方法大大增强了数据集群的性能。 但是，可以通过可观察数据的线性变换获得聚类友好空间的假设是限制性的，并且在实践中，可观察数据与潜在表示之间的关系可能是高度非线性的，这是内核方法和光谱方法的成功暗示的 聚类[19]。 我们如何超越联合线性灾难恢复和聚类的思想和见解来处理此类复杂（但更为现实）的案例？ 在这项工作中，我们解决了这个问题\n",
    "   \n",
    "   \n",
    "   ###### 建议的公式化\n",
    "   我们的想法是使用非线性映射，即，可观察数据xi及其可聚类的潜在表示hi之间的关系模型。\n",
    "   ![](../img/dl012_2.PNG)\n",
    "   其中f（·; W）表示映射函数，W是表征非线性映射的一组参数。可以考虑许多非线性映射函数，例如高斯和辛克核。 在这项工作中，我们建议使用DNN作为我们的映射功能，因为DNN具有使用合理数量的参数来逼近任何连续映射的能力[8]。\n",
    "  对于本文的其余部分，f（·; W）是DNN，W收集网络参数，即连接各层之间神经元的链接的权重以及每个隐藏层中的偏差。   \n",
    "    \n",
    "  使用上述表示法，诱使按照以下方式制定后期关节DR和聚类：\n",
    " ![](../img/DL012_3.PNG)\n",
    " 乍看之下，（2.3）中的公式似乎很直观。\n",
    "   但是，它的病情很严重，很容易导致琐碎的解决方案。\n",
    "   实际上，在[29]中并行发表的工作中提出的公式与（2.3）中的非常相似。 唯一的不同是，将si上的硬0-1类型分配约束放宽为概率类型的软约束，并且拟合标准基于Kullback-Leibler（KL）散度而不是最小二乘法–不幸的是，这样的公式 就像我们上面讨论的那样，仍然有一些简单的解决方案。  \n",
    "     \n",
    "       \n",
    "   在线性DR情况下，防止琐碎问题的最重要组成部分之一是重构部分，即![](../img/DL012_4.PNG)，该项可确保学习到的hi可以（近似地）使用W为基础来重构xi。这激励将重建项合并到基于DNN的DR和K-means中。在无人监督的DNN领域中，有几种完善的重建方法-例如，堆叠式自动编码器（SAE）是实现此目的的流行选择。\n",
    "   为了防止琐碎的低维表示（例如全零向量），SAE使用解码网络g（·; Z）将hi映射回数据域，并要求g（hi; Z）和xi彼此匹配 好在某些度量标准下，例如相互信息或基于最小二乘的度量标准。  \n",
    "     \n",
    "       \n",
    "  新的损失函数：![](../img/DL012_5.PNG)\n",
    "  为了简明起见，我们将f（xi; W）和g（hi; Z）分别简化为f（xi）和g（hi）。函数.. l（·）：RM→R是确定重建误差的某个损失函数。这个损失函数选择最小二乘损失![](../img/DL012_6.PNG)\n",
    "  也可以考虑其他选择，例如基于1-范数的拟合和KL散度。λ≥0是一个正则化参数，它在重构误差与找​​到K均值友好的潜在表示之间取得平衡。  \n",
    "    \n",
    " ##### 网络结构\n",
    " 网络结构图2给出了与（2.4）中的公式相对应的网络结构。  “瓶颈”层的左侧是将原始数据转换为低维空间的所谓编码层或前向层。 右侧是“解码”层，这些层尝试从潜在空间重建数据。  K-means任务在瓶颈层执行。 同时优化前向网络，解码网络和K均值成本。 在我们的实验中，解码网络的结构是编码网络的“镜像版本”，对于编码和解码网络，我们都使用基于整流线性单元（ReLU）激活的神经元[16]。 由于我们的目标是执行DNN驱动的K均值聚类，因此我们将图2中的网络称为续集中的深度聚类网络（DCN）。从网络结构的角度来看，DCN可以被视为在瓶颈层具有K均值友好结构促进功能的SAE。\n",
    "   我们应该指出，在（2.4）中提出的优化准则和图2中的网络非常灵活：可以使用其他类型的神经元激活，例如S形和二阶阶跃函数。 对于聚类部分，其他聚类标准，例如K子空间和软K均值[2，12]，也是可行的选择。 不过，由于我们的兴趣是提供概念证明，而不是穷尽组合的可能性，因此我们将重点关注后续的拟议DCN\n",
    "     \n",
    " ###### 优化\n",
    " 由于成本函数和约束条件都是非凸的，因此优化（2.4）非常重要。 此外，还需要考虑可伸缩性问题。 在本节中，我们提出了一个实用的图2：提出的深度集群网络（DCN）的结构。\n",
    "   优化程序，包括经验有效的初始化方法和基于交替优化的处理算法（2.4）。  \n",
    "     \n",
    "       \n",
    " 通过分层预训练进行初始化  \n",
    "   \n",
    "   为了处理（2.4）中的硬非凸优化问题，初始化通常至关重要。\n",
    "   为了初始化网络参数，即（W，Z），我们使用[3]中的分层预训练方法来训练自动编码器。 在大规模的有监督的学习任务中可以避免这种预训练技术。 但是，对于完全不受监督的拟议DCN，我们发现无论数据集的大小如何，分层预训练过程都非常重要。 分层预训练过程如下。 我们从使用单层自动编码器开始训练前向网络的第一层和解码网络的最后一层开始。 然后，我们将训练后的前向层的输出用作输入，以训练下一对编码和解码层，依此类推。 在预训练之后，我们对瓶颈层的输出执行K均值以获得M和{si}的初始值。  \n",
    "     \n",
    "       \n",
    "  3.2交替随机优化  \n",
    "    \n",
    "   即使初始化良好，处理问题（2.4）仍然非常困难。 由于块变量{si}约束在离散集上，因此不能将常用的随机梯度下降（SGD）算法直接应用于W，Z，M和{si}的联合优化。 我们的想法是结合交替优化和SGD的见解。 具体来说，我们建议针对M，{si}和（W，Z）中的一个（w.r.t.）优化子问题，同时将其他两组变量保持固定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
