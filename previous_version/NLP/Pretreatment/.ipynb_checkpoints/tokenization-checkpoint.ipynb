{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39b241d",
   "metadata": {},
   "source": [
    "# 分词\n",
    "将连续的长文本切分成词和不同的元组（n-gram。）。文档切分成段落，段落切分成句子，句子切分成短语，短语切分成词条和标点符号。将文本切分成词条的过程被称为分词。NLP的基础模块有：分词器、词汇表、分析器、词条、词项、词或n-gram。    \n",
    "  \n",
    "词汇表：将文本文档根据一系列规则进行切分后，得到的词汇集合称之为词汇表。  \n",
    "  \n",
    "词向量表示：将词汇表中的每个文本词以数值向量表示，这个向量称之为词向量。  \n",
    "  \n",
    "### 传统的分词方法\n",
    "    1.利用字符串的空白符作为边界\n",
    "        使用python的split方法以空白符为边界，对文本进行划分。(将划分出来的词（带顺序的词序）作为矩阵的行，同时将划分后的词去掉重复项作为矩阵的列，将行列相同词的位置标注为1，其余标注为0，得到每个词的数字向量表示形式（one-hot向量）。)（将自然语言句子转化为了数值序列）。基于上述ont-hot向量的句子表示方法保留了原始句子的所有细节，包括语法和词序。   \n",
    "         \n",
    "    2.标点符号的处理\n",
    "         标点符号（逗号、句号、分号）也可以当作分词符，有时也希望将标点符号当作单独的词条。\n",
    "         \n",
    "    3.缩略语\n",
    "        缩略语wasn't的分词\n",
    "        \n",
    "    4.n-gram（n元组）\n",
    "        包含n个元素的序列，为什么需要n-gram是因为在词条序列向量化为词袋向量时，如果不考虑n-gram的意义，将会丢失词序的许多意义。\n",
    "        \n",
    "    5.停用词  \n",
    "        停用词指的是那些出现频率非常高的常见词，但是对短语的含义而言，这些词承载的实质性信息内容却少得多。如 a 、an 、 the 等。\n",
    "        NLP剔除停用词，会减小从文本中提取信息时的计算压力。  \n",
    "          \n",
    "### 词汇表操作  \n",
    "词汇表的大小对NLP性能有重大的影响，所以减小词汇表的大小是非常有必要的。一种减少词汇表大小的方法是将词汇表归一化以便意义相似的词条归并成单个词条，这样做可以减少需要保留的词条数，另一面会减少语料库中意义相似但拼写不同的词条和n-gram之间的语义联系。减少词汇表规模可以有效地降低过  \n",
    "  \n",
    "$\\color{red} {词汇表归一化:} $ 大小写转换、词干还原、词形归并  \n",
    "  \n",
    "    1.大小写转换\n",
    "        单词出现在句首或者为了表示强调均采用大写形式来表示时，某个单词的大小写形式不太统一。将这种不统一的大小写形式统一化称为大小写转换。（有助于减小词汇表规模）。单词大写的时候有时具体不同的意义，doctor、Doctor，单词大写也表示一个专有名词即人名、地名、事物名等。大小写转换时需要区分此类名词。\n",
    "        \n",
    "    2.词干还原\n",
    "        消除词的复数形式、所有格词尾、不同的动词形式等带来的意义上的差别。这种识别词的不同形式背后的公共词干的归一化称为词干还原。（注意：也可能会带来模型的准确率下降，因为同一词根的不同拼写方式会被一视同仁，即使他们具有不同的意义。）\n",
    "        \n",
    "    3.词形归并\n",
    "        知道词义之间可以相互关联，那么能够将一些不同拼写的词但意义相同的词进行关联（即使他们的拼写完全不同）。这种更粗放的将词归一化成语义词根的方式称为词形归并。\n",
    "        由于考虑了词义，相对于词干还原和大小写归一化，词形归并时一种潜在的更具精确性的词归一化方法。\n",
    "        \n",
    "相比于词形归并，词干还原会犯更多的错误。无论时词形归并还是词干还原都会缩小词汇表的规模同时增加文本的歧义性。\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "### 词表示\n",
    "    1.词频向量（词袋向量）\n",
    "        用词的one-hot向量对词进行表示得到的文本数据会非常之大，并不适合计算机进行NLP处理。  \n",
    "        假设：一个句子的大部分意义都可以从词的本身获取，可以忽略词的顺序和语法，并将他们混合在一个‘袋子’中，每个句子或者每个文档对应一个袋。这个袋子里的词向量可以用来在忽略词序和语法的情况下任能概括文档的本质内容。\n",
    "        词频向量：使用上述的词袋向量将文档信息压缩。即：将所有词向量表示的one-hot向量相加，得到一个词袋向量表示（只有一个向量，而不是n个向量）这个词袋向量也被称作词频向量，因为他只计算了词的频率，而不是词的顺序。  \n",
    "        （两种词袋向量：1、所有one-hot向量相加的词频向量 2、所有one-hot向量进行or运算的二值词袋向量）\n",
    " \n",
    "          \n",
    "### 度量词袋之间的重合度    \n",
    "如果能够度量两个词袋之间的重合度，则在一定程度上可以两个文档之间的语义重合度。  \n",
    "       \n",
    "       用点积估计两个词袋向量的之间的重合度。（即估计两个词袋之间重合词数的计算）两个词袋向量的点积的值即为两个文档之间重合词的数目。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "909f6eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thomas', 'Jefferson', 'began', 'building', 'Moticello', 'at', 'the', 'age', 'of', '26.']\n"
     ]
    }
   ],
   "source": [
    "str = 'Thomas Jefferson began building Moticello at the age of 26.'\n",
    "Tokenization = str.split(' ')\n",
    "print(Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88774ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
