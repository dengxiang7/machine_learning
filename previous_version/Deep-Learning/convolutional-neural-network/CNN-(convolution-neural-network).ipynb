{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 卷积神经网络 （CNN）\n",
    "通用定理：任何一个$l维到n维的映射：R^l\\Rightarrow R^n$ 都可以用一个单隐层网络逼近只要隐层单元足够多。，但通常选择多层神经网络这样表示更简单，参数更少。   \n",
    "卷积公式：$\\iint \\; f(a,b)g(x-a,y-b) dadb$\n",
    "  \n",
    "  \n",
    "### 卷积层\n",
    "1、二维卷积层  \n",
    "$\\quad$它有宽和高两个空间维度，二维卷积层常用来处理图像。  \n",
    "  \n",
    "    \n",
    "$\\quad$ 卷积运算：  \n",
    "  \n",
    " (注这里的卷积运算是借鉴了卷积运算的互相关运算。如果要完全使用卷积运算：要使卷积运算的结果与互相关运算结果相同。则要先把核数组进行$180^\\circ$反转等于核数组左右反转上下反转之后的核数组；再进行卷积运算) \n",
    "![](../../img/Pattern_recognition/neural_networks/convolution1.PNG)  \n",
    "  \n",
    "    \n",
    "这里的核称为卷积核或者过滤器 重复的使用卷积核能有效的表征局部特征  \n",
    "           \n",
    " 通过做卷积运算我们可以做到共享参数，不论特征在何部位通过卷积运算都可以提取局部特征；\\*\\* 所以现在我们要学习的参数变成了核数组。   \n",
    "    \n",
    "      \n",
    "      \n",
    "2、$1\\times 1$卷积层  \n",
    "  \n",
    "  核窗口为$1\\times 1$的多通道卷积层：作用在通道维上；所以一个$1\\times 1$滤波器就是一个全连接；；但与全连接不同的是；全连接为每一个输入维配一个权重；而一个$1\\times 1$滤波器是为每一个输入通道配一个权重，（把输入通道维当作特征维）\n",
    "    \n",
    "### 卷积层的作用\n",
    "\n",
    "\n",
    "   \n",
    "     \n",
    "### 特征图\n",
    "二维卷积层输出的二维数组可以看作是输入空间维度在某一级上的表征，也叫特征图（feature map）,每次的阴影区叫做感受野  \n",
    "  \n",
    "    \n",
    "### 填充和步幅\n",
    "假设输入是$i_{h\\times w }$ 核窗口为$k_{h\\times w }$那么输出为 $O_{(i_h-k_h+1)\\times (i_w-k_w+1) }\\quad$所以卷积层的输出形状由输入形状和核形状决定  \n",
    "  \n",
    "    \n",
    "1、填充：指在输入形状的四周（就是$p_h$高上下填充的总行数和$p_w$宽两侧填充的总列数）填充元素（通常是0元素）\n",
    "![](../../img/Pattern_recognition/neural_networks/padding1.PNG)  \n",
    "  \n",
    "2、卷积核窗口，在输入窗口上从最左上方开始，依次从左到右，从上到下的顺序滑动做卷积运算。将每次滑动的行数和列数叫做步幅。    \n",
    "  \n",
    "    \n",
    " 当在高上的步幅$S_h$和宽上的步幅$S_w$输出窗口大小  $O_{\\frac{(i_h-k_h+p_h+S_h)}{S_h} \\;\\;\\times\\;\\; \\frac{(i_w-k_w+p_w+S_w)}{S_w} }$   \n",
    "   \n",
    "     \n",
    " ### 多输入通道和多输出通道\n",
    " 在上面讲的都是二维的情况，那么在多维的时候，输入和输出是什么情况。  \n",
    "   \n",
    " 例如：彩色图像，在高宽上还有RGB颜色通道；高$h$宽$w$的像素和一个颜色通道channel 故多维数组为：$(3\\times h\\times w)$  \n",
    "   \n",
    "多输入通道：  \n",
    "输入数据为多维数组时：在二维的基础上输入数据每多一维数据则多一个通道。相应的就要为这一个通道建立一个卷积核，由有多少个通道就有多少个卷积核（通道数等于卷积核数）且输出通道为1  \n",
    "![](../../img/Pattern_recognition/neural_networks/channel1.PNG)   \n",
    "  \n",
    "    \n",
    "多输出通道：  \n",
    "如果希望得到多个输出通道:  \n",
    "![](../../img/Pattern_recognition/neural_networks/channel2.PNG) \n",
    "  \n",
    "  \n",
    "    \n",
    "### 池化层\n",
    "缓解卷积层对位置过度敏感  \n",
    "池化层每次对固定的输入窗口（池化窗口）计算输出，但计算的方式不同：采用最大值（最大池化），或者平均值（平均池化）  \n",
    "![](../../img/Pattern_recognition/neural_networks/pooling1.PNG)    \n",
    "  \n",
    "    \n",
    "<br>  \n",
    "  \n",
    "<br>  \n",
    "\n",
    "### 全局池化层  \n",
    "  \n",
    "* 全局平均池化层代替全连接层  \n",
    "  \n",
    "      主要是用来解决全连接的问题，其主要是是将最后一层的特征图进行整张图的一个均值池化，形成一个特征点，将这些特征点组成最后的特征向量  假如，最后的一层的数据是10个6*6的特征图，global average pooling是将每一张特征图计算所有像素点的均值，输出一个数据值，这样10 个特征图就会输出10个数据点，将这些数据点组成一个1*10的向量的话，就成为一个特征向量，就可以送入到softmax的分类中计算了  \n",
    "        \n",
    "![](../img/CNN/CNN1.PNG)         \n",
    "  \n",
    "    \n",
    "        左边是全连接层，右边是用平均池化层代替全连接层。  \n",
    "          \n",
    "        这样做主要是进行全连接的替换，减少参数的数量，这样计算的话，global average pooling层是没有数据参数的  \n",
    "           \n",
    "        一方面可以避免过拟合，另一方面这也更符合CNN的工作结构，把每个feature map和类别输出进行了关联，而不是feature map的unit直接和类别输出进行关联。   \n",
    "  \n",
    "\n",
    "### 卷积的好处  \n",
    "1、稀疏连接  \n",
    "2、参数共享：减少参数的个数，降低模型复杂度，降低对训练样本数目的要求。  \n",
    "3、平等变换性  \n",
    "  \n",
    "  \n",
    " \n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
