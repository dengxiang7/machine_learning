{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 循环神经网络基础知识  \n",
    "卷积神经网络是为处理空间信息而设计的。卷积神经网络是为处理时序信息而设计的；循环神经网络引入状态变量来存储过去的信息，用这个存储过去信息的状态变量和当前的输入共同决定当前的输入。  \n",
    "  \n",
    "<br>  \n",
    "    \n",
    "    \n",
    "### 语言模型  \n",
    "语言模型是一个时序模型，由这个模型引入循环神经网络。  \n",
    "  \n",
    "语言模型：把一段文本看作离散的时间序列；给定长度为T的文本$(w_1,w_2,\\cdots,w_n) \\qquad w_1)$对应时刻$t_1$的输出或标签；语言模型计算该序列的概率$p(w_1,w_2,\\cdots,w_n) \\qquad w_1)$。   \n",
    "  \n",
    " <br>  \n",
    "   \n",
    "n元语法：假设一个词的出现只与前面n个词有关；（这个马尔可夫假设是不一定成立的，但这样假设它）。   \n",
    "  \n",
    "    \n",
    "<br>  \n",
    "  \n",
    "### 循环神经网络  \n",
    "  \n",
    "1、普通网络：$H=\\phi(XW_{xh}+b) \\qquad\\qquad O=HW_{bh}+b_q$  \n",
    "  \n",
    "<br>  \n",
    "<br>\n",
    "  \n",
    "2、含隐藏状态的网络：  \n",
    "假设时序t的输入为$x_t$,该时序的隐藏状态为$H_t$，且含有上一步的时序的隐藏状态$H_{t-1}$（以前信息）,为上一步的隐藏状态配一个权重$W_{hh}$ ;为这一步的输入配一个权重$W_{xh}$:  \n",
    "  \n",
    "  \n",
    "$$ 隐藏输出：H_t=\\phi(X_tW_{xh}+H_{t-1}W_{hh}+b_h)$$  \n",
    "  \n",
    "可以看出当前时间步的隐藏状态，使用了上一步的隐藏状态，故这个式子是循环计算的，而使用循环计算的网络，就是循环神经网络。  \n",
    "  \n",
    "  \n",
    "$$ 输出层：O=HW_{bh}+b_q$$    \n",
    "  \n",
    "    \n",
    "逻辑图：  \n",
    "![](../img/rnn/rnn1.PNG)  \n",
    "  \n",
    "隐藏状态的计算等价于：$X_t \\quad H_{t-1}$连接后的矩阵与；$W_{xh} \\quad W_{hh}$连接后的矩阵相乘  \n",
    "  \n",
    "<br>   \n",
    "   \n",
    " 循环神经网络每次时间步的参数都一样：$ w_{qh} \\quad  w_{xh} \\quad  w_{hh} \\quad b_h \\quad$每次时间步都一样  \n",
    "   \n",
    "     \n",
    "<br>  \n",
    "<br>    \n",
    "    \n",
    " **注意**：  \n",
    " 这里不能理解为每一个循环单元是相互独立的，也不是简单的对循环单元进行多次循环就行；但是每次时间步的参数$ w_{qh} \\;\\; w_{xh} \\;\\;  w_{hh} \\;\\; b_h \\;\\;$每次时间步都一样。  \n",
    "      \n",
    " 不是在每次循环时利用上一个隐藏变量进行预测之后就立马就行梯度计算，对参数进行优化。  \n",
    "   \n",
    " 而是：每次循环利用上次的隐藏变量得到一个预测值；在完成所有循环预测之后；统一计算损失梯度，对参数进行优化。  \n",
    "    \n",
    "      \n",
    " <br>  \n",
    " <br>  \n",
    "   \n",
    "### 梯度计算（循环后一次全部展开）   \n",
    "  \n",
    "一个简单模型：标签为$y_t\\;\\;$假设激活函数$\\phi(x)=x\\;\\;$则隐藏状态$h_t=W_{xh}X_t+W_{hh}H_{t-1}$输出层：$O_t=W_{qh}h_t$ ;时间步t的损失函数为：$l(o_t,y_t)$ ;则整个时间序列上损失：$L=\\frac{1}{T}\\sum_{t=1}^Tl(o_t,y_t)$ ;这个L就是时间序列上的损失叠加；也就是我们时间序列上的目标函数 (注意：这里的单个时间步的损失，和总损失，都是在完成所有循环之后计算的。不是单次循环就计算一个时间步损失)  \n",
    "  \n",
    "    \n",
    "损失图：  \n",
    "![](../img/rnn/rnn2.PNG)\n",
    "  \n",
    "<br>  \n",
    "      \n",
    "通过时间反向传播(在完成循环后：一次全部展开总损失，计算梯度更新参数)：  \n",
    "\n",
    "$\\quad$  要求的梯度有：$\\frac{\\partial L}{\\partial W_{hx}}\\quad$ $\\frac{\\partial L}{\\partial W_{hh}}\\quad$ $\\frac{\\partial L}{\\partial W_{qh}}\\quad$  \n",
    "  \n",
    "1、总L对各时间步求导：$\\frac{\\partial L}{\\partial o_{t}} \\;=\\;\\frac{1}{T} \\frac{\\partial l(o_t,y_t)}{ \\partial o_t }\\quad$   \n",
    "  \n",
    "2、总L对$W_{qh}$步求导$\\frac{\\partial L}{\\partial w_{qh}} \\;=\\;\\sum_{t=1}^T \\frac{\\partial L }{\\partial o_t}*h_t^T$  \n",
    "  \n",
    "3、$h_0$是参数；L对$h_0$求导，L对h依赖关系，利用链式法则($t<T:\\;\\;\\;h_{t+1}=W_{hx}X_t+W_{hh}H_{t}\\;\\;\\;\\;l(o_t,y_t) \\;\\;\\;O_t=W_{qh}h_t \\; $)：  \n",
    "$\\qquad$L对最终步T的隐藏状态$h_T$求导：$\\frac{\\partial L}{\\partial h_T} \\;=\\; \\frac{\\partial L }{\\partial o_T}*\\frac{\\partial o_T }{\\partial h_T}=W_{qh}^T* \\frac{\\partial L }{\\partial o_T}$  \n",
    "  \n",
    "$\\qquad$t<T时：$\\frac{\\partial L}{\\partial h_t}=\\frac{\\partial L}{\\partial o_t} *\\frac{\\partial o_t}{\\partial h_t} +\\frac{\\partial L }{\\partial h_{t+1}}*\\frac{\\partial h_{t+1}}{\\partial h_t} \\;=\\; W_{qh}^T\\frac{\\partial L }{\\partial o_{t}}+ W_{qhh}^T\\frac{\\partial L }{\\partial h_{t+1}}$  \n",
    "  \n",
    " \n",
    " $\\qquad$首先从总的损失函数展开的公式看：在T步时只需计算$L对o_T的梯度\\;\\;o_T\\;对\\;h_T$的梯度就行；但是在t<T的时间步时在展开的总损失中不仅要计算  \n",
    " $\\qquad L对o_t的梯度\\;\\;o_t\\;对\\;h_t$的还有$h_{t+1}$中对$h_t$的梯度  \n",
    "   \n",
    " $\\qquad \\Rightarrow \\frac{\\partial L}{\\partial h_t} =\\sum_{i=t}^T \\left( (W_{hh}^T)^{T-i}W_{qh}^T\\frac{\\partial L }{\\partial o_{T+t-i}} \\right)$  \n",
    "   \n",
    " $\\qquad$t较大梯度衰减,t较小梯度爆炸  \n",
    "  \n",
    "    \n",
    "4、总L对$W_{hx}$步求导:$\\frac{\\partial L}{\\partial W_{hx}}=\\sum_{t=1}^T\\frac{\\partial L}{\\partial h_{t}}x^T$  \n",
    "  \n",
    "5、总L对$W_{hh}$步求导:$\\frac{\\partial L}{\\partial W_{hh}}=\\sum_{t=1}^T\\frac{\\partial L}{\\partial h_{t}}h_{t-1}^T$   \n",
    "  \n",
    "  \n",
    "    \n",
    "<br>  \n",
    "\n",
    "<br>  \n",
    "\n",
    "<br>  \n",
    "\n",
    "<br>  \n",
    "  \n",
    "  \n",
    "\n",
    "      \n",
    "###  门控循环单元（GRU）   \n",
    "  \n",
    " 当时间步较大时出现梯度衰减，由于这个原因造成了循环神经网络很难捕捉到时间步距离较大时的依赖关系，为了解决时间步较大时的依赖关系提出了GRU和LSTM。（距离小的梯度爆炸解决方案是裁剪梯度）  \n",
    "  \n",
    " \n",
    "    \n",
    "#### 重置门 和更新门  \n",
    "输入为当前步的$X_t \\in R_{n \\times d}$和上一步隐藏状态$H_{t-1} \\in R_{n \\times h}$ (**时间序列上截至上一时间步的全部信息**);激活函数为Sigmoid函数   \n",
    "  \n",
    " \n",
    "\n",
    "重置门：激活函数Sigmoid函数，所以重置们的每个输出单元的输出是0或1  \n",
    "$\\qquad\\;\\;\\;\\; R_t=\\phi(X_tW_{xr} +H_{t-1}W_{hr} +b_r)$  \n",
    "  \n",
    " \n",
    "\n",
    "更新门：激活函数Sigmoid函数，所以重置们的每个输出单元的输出是0或1  \n",
    "$\\qquad\\;\\;\\;\\; Z_t=\\phi(X_tW_{xz} +H_{t-1}W_{hz} +b_z)$   \n",
    "   \n",
    "     \n",
    "![](../img/rnn/rnn3.PNG)  \n",
    "  \n",
    "<br>  \n",
    "      \n",
    "<br>  \n",
    "\n",
    "#### 候选隐藏状态   \n",
    "  \n",
    " 门控循环单元将计算候选隐藏状态类辅助稍后的本时间步的隐藏状态的计算，；当前时间步的重置门为0或1，与上一时间步的隐藏状态按元素乘，重置门为0的，对应隐藏状态为0，重置门为1的对应隐藏状态为1；按元素乘的结果再与当前时间步的输入$X_t$连接，在激活函数为tanh的全连接层相连，计算出候选隐藏状态，值的范围为[-1,1]。  \n",
    "   \n",
    " $\\quad\\quad \\tilde{H_t}=tanh(X_tW_{xh}+(R_t \\bigodot H_{t-1})W_{hh} +b_h)$   \n",
    "   \n",
    "重置门控制了上一时间步的隐藏状态是如何流入当前时间步的（上一时间步的隐藏状态可能包含了时间序列上截至上一时间步的全部历史信息），重置门用来丢弃和预测无关的历史信息   \n",
    "    \n",
    "![](../img/rnn/rnn4.PNG) \n",
    "  \n",
    "    \n",
    "<br>  \n",
    "  \n",
    "<br>  \n",
    "\n",
    "#### 隐藏状态  \n",
    "  \n",
    "  本次时间步的隐藏状态$H_t$的计算：使用本次时间步的更新门$Z_t$对上一时间步的隐藏状态$H_{t-1}$和本次时间步的候选隐藏状态$\\tilde{H_t}$做组合  \n",
    "    \n",
    "      \n",
    "   $\\quad\\quad H_t=Z_t \\bigodot H_{t-1} +(1-Z_t)\\bigodot \\tilde{H_t}$  \n",
    "     \n",
    " <br>  \n",
    " \n",
    " 更新门：可以控制当前的隐藏状态$H_t$，应当如何被包含当前时间步输入信息$X_t$的候选隐藏状态$\\tilde{H_t}$所更新.  \n",
    "   \n",
    " $\\qquad\\quad$假设t'到t(t'<t)中间的所有时间步的更新门$Z_{t'} $\\~$ Z_t$都一直近似1，那么在t'到t的时间步中包含这些时间步输入$X_{t'} $\\~$ X_t$的候选隐藏单元$\\tilde{H_t'}$\\~$\\tilde{H_t}$,  \n",
    " $\\qquad\\quad$几乎都没有流入当前时间步的隐藏单元$H_t$  \n",
    " $\\qquad\\quad$也就是说在t'-1时间步输入信息通过隐藏单元$H_{t-1}$,在时间序列上一直传递到当前时间步的隐藏状态；所以更新门捕捉到了较长时间序列上的依赖关系\n",
    "     \n",
    " ![](../img/rnn/rnn5.PNG)   \n",
    "   \n",
    "     \n",
    "<br>  \n",
    "\n",
    "<br>  \n",
    "\n",
    "重置门：有利用捕捉短时间的依赖关系  \n",
    "  \n",
    "更新门：有助于捕捉长时间的依赖关系   \n",
    "\n",
    "  <br>  \n",
    "    \n",
    "   <br> \n",
    "      \n",
    "      \n",
    "用上图中的门控逻辑单元（GRU）代替这个途中的第一个阴影列；    \n",
    "![](../img/rnn/rnn2.PNG)   \n",
    "  \n",
    "  \n",
    "  \n",
    "    \n",
    "<br>  \n",
    "\n",
    "<br>  \n",
    "\n",
    "<br>  \n",
    "\n",
    "<br>  \n",
    "  \n",
    "  \n",
    "\n",
    "<br>  \n",
    "\n",
    "### 长短期记忆（LSTM）  \n",
    "  \n",
    "与门控逻辑单元一样；都是为解决梯度衰减问题而提出的门控循环神经网络；两者并列。  \n",
    "  \n",
    "    \n",
    "<br>  \n",
    "  \n",
    "#### 输入门 、遗忘门、 输出门    \n",
    "  \n",
    "输入门，遗忘门，输出门的激活函数都是Sigmoid函数，所以这三个门的输出值都在\\[0,1\\]之间   \n",
    "  \n",
    "$ F_t=\\phi(X_tW_{xf}+H_{t-1}W_{hf}+b_f)$  \n",
    "  \n",
    "$ I_t=\\phi(X_tW_{xi}+H_{t-1}W_{hi}+b_i)$  \n",
    "  \n",
    "$ O_t=\\phi(X_tW_{xo}+H_{t-1}W_{ho}+b_o)$\n",
    "  \n",
    "![](../img/rnn/RNN8.PNG)   \n",
    "  \n",
    "    \n",
    "<br>  \n",
    "    \n",
    "<br>  \n",
    "  \n",
    "#### 候选记忆细胞  \n",
    "  \n",
    "候选记忆细胞 $\\hat{C}$,激活函数选用tanh函数，其值为在\\[-1,1\\]之间  \n",
    "    \n",
    "      \n",
    "$ \\hat{C}_t=\\tanh(X_tW_{xc}+H_{t-1}W_{hc}+b_c)$   \n",
    "  \n",
    "![](../img/rnn/RNN9.PNG)    \n",
    "  \n",
    "      \n",
    "<br>  \n",
    "  \n",
    "<br>  \n",
    "  \n",
    "#### 记忆细胞  \n",
    "  \n",
    "通过值域在 \\[0,1\\]的输入门、遗忘门、输出门控制隐藏状态的信息流动。  \n",
    "  \n",
    "当前时间步的记忆细胞：  \n",
    "\n",
    "1、遗忘门选择上一时间步记忆细胞是否流入当前时间步  \n",
    "  \n",
    "2、 输入门控制当前时间步的输入xt通过候选记忆细胞$\\hat{C_t}$如何流入当前记忆细胞$C_t$  \n",
    "  \n",
    "    \n",
    "      \n",
    "$\\quad\\quad C_t=F_t \\bigodot C_{t-1} +I_t\\bigodot \\hat{C_t}$     \n",
    "  \n",
    "    \n",
    "如果遗忘门一直近似1，输入门一直近似0，过去的记忆细胞将一直传递到当前时间步；可以应对梯度衰减，捕捉到加大时间度的依赖关系    \n",
    "      \n",
    "        \n",
    " ![](../img/rnn/RNN10.PNG)   \n",
    "   \n",
    " <br>      \n",
    "       \n",
    " ####   隐藏状态  \n",
    "   \n",
    "   通过输出门${O_t}$控制记忆细胞${C_t}$到当前时间步隐藏状态$H_t$的信息流动。  \n",
    "     \n",
    "     \n",
    "   \n",
    "  $\\quad\\quad H_t=C_t \\bigodot tanh(C_{t})$    \n",
    "    \n",
    "      \n",
    "当输出门近似1时，记忆细胞流入隐藏状态供输出层使用，当输出门近似0时，记忆细胞只自己保留   \n",
    "  \n",
    " ![](../img/rnn/RNN10.PNG)     \n",
    "   \n",
    "     \n",
    " <br>  \n",
    "   \n",
    "<br>  \n",
    "  \n",
    "用上图中的门控逻辑单元（TLSM）代替这个途中的第一个阴影列；    \n",
    "![](../img/rnn/rnn2.PNG)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
