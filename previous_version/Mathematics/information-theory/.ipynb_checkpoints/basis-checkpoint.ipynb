{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 信息熵  \n",
    "   \n",
    "   描述了信息量的多少。（一个信源有多少信息量） 通常一个信源发送什么符号是不确定的，**可以用概率衡量它的不确定性。概率大出现机会多不确定性小，概率小出现机会小不确定性大。**  \n",
    "   \n",
    "   在信源中：考虑所有发生情况的平均值。若信号源有n种取值。$u_1,u_2,……，u_n$ 对应概率为$p_1,p_2,……，p_n$ 则信息熵公式为：\n",
    "     \n",
    "  $$H(u)=E[-log p_i]=-\\sum_{i=1}^n p_ilog p_i$$  \n",
    "    \n",
    " **信息是确定性的增加。（类比热力学熵）**   \n",
    "   \n",
    "  一般而言，当一种信息出现概率更高时，表明它被传播的更广。  \n",
    "    \n",
    "  $$H(x)=-\\sum_{i=1}^n p(x_i)log p(x_i)$$  \n",
    "    \n",
    "$\\color{red}{\\text{信息熵：变量的不确定性越大，熵也就越大，把它搞清楚所需要的信息量也就越大}}$   \n",
    "  \n",
    "$\\color{blue}{\\text{信源符号出现概率小}\\rightarrow\\text{不确定性大}\\rightarrow\\text{信息熵大}\\rightarrow\\text{信息熵：表示的信息多}}$   \n",
    "  \n",
    "$\\color{red}{\\text{概率越小，信息量越多}}$   \n",
    "\n",
    "<br>  \n",
    "\n",
    "<br>\n",
    "\n",
    "## 互信息  \n",
    "  \n",
    "   在信息论中I(X;Y)测量了X和Y之间的信息总量（可以看成一个随机变量中包含的关于另一个随机变量的信息量，或者说一个随机变量由于已知的另一个随机变量而减少的不确定性。）。   \n",
    "   \n",
    "   $$I(X;Y)=\\sum_{y\\in Y}\\sum_{x\\in X}p(x,y)log \\frac{p(x,y)}{p(x)p(y)}$$  \n",
    "     \n",
    " <br>  \n",
    " \n",
    " 由上面这个总公式可以推导出下面这个公式成立。\n",
    "     \n",
    "   $$I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)=H(Y)+H(X)-H(YX)$$    \n",
    "     \n",
    "<br>  \n",
    "\n",
    "\n",
    "     \n",
    " **直观的解释是：当Y给定时，X的不确定性的减少量。如果X和Y是相互独立的那么I(X;Y)=0。相反如果X和Y确实有某种确定性的联系，最大化互信息是可以得到的。**    \n",
    "   \n",
    " **知道了一件事，另一件事的不确定性减少。**     \n",
    "   \n",
    "<br>  \n",
    "\n",
    "**性质：**  \n",
    "1、对称性  \n",
    "**2、非负性**  \n",
    "3、极值性：最大为一个随机变量的熵。（一个随机变量的熵在概率分布给定时是常数）  \n",
    "4、凸函数性\n",
    "    \n",
    "      \n",
    "<br>  \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "假如我们知道了$X\\text{~}P_G(X)$，我们希望$P_G(C|X)$有一个微乎其微的信息熵。            \n",
    "\n",
    "\n",
    "\n",
    "<br>  \n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "## 信息熵，互信息的联系与总结   \n",
    "  \n",
    " * **信息熵**：  \n",
    "   信息熵描述信息量的多少。（描述信息的不确定性）  \n",
    "   $H(u)=E[-log p_i]=-\\sum_{i=1}^n p_ilog p_i  \\qquad \\geq 0$  \n",
    "   信息熵越大$\\rightarrow$概率越小$\\rightarrow$信息量越多$\\rightarrow$ 不确定性越大。  \n",
    "   信息熵越小$\\rightarrow$概率越大$\\rightarrow$信息量越少$\\rightarrow$ 不确定性越小。  \n",
    "     \n",
    "   信息熵：变量的不确定性越大，熵也就越大，把它搞清楚所需要的信息量也就越大   \n",
    "     \n",
    "<br>  \n",
    "\n",
    "<br>  \n",
    "\n",
    "* **互信息：**  \n",
    "    互信息I(X;Y)测量了X和Y之间的信息总量（可以看成一个随机变量中包含的关于另一个随机变量的信息量，或者说一个随机变量由于已知的另一个随机变量而减少的不确定性。）。  \n",
    "    $I(X;Y)=\\sum_{y\\in Y}\\sum_{x\\in X}p(x,y)log \\frac{p(x,y)}{p(x)p(y)}$  \n",
    "     由上面这个总公式可以推导出下面这个公式成立。   \n",
    "   $I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)=H(Y)+H(X)-H(YX)$  \n",
    "    性质：  \n",
    "    1、对称性  \n",
    "    **2、非负性**  \n",
    "    3、极值性：最大为一个随机变量的熵。（一个随机变量的熵在概率分布给定时是常数）  \n",
    "    4、凸函数性   \n",
    "      \n",
    " <br>  \n",
    " \n",
    " \n",
    "   \n",
    "![](img/MI.PNG)   \n",
    "\n",
    " $H(X|Y)=H(X)-I(X;Y)$   \n",
    "   \n",
    "互信息：  I(X;Y)是一个增量，I(X;Y)越大，X中由于Y已知，而引起X的不确定性降低，导致X的信息量减少。  \n",
    "\n",
    "<br>\n",
    "\n",
    "互信息越大$\\rightarrow$不确定性减少越大$\\rightarrow$条件变量熵越小$\\rightarrow$概率越大\n",
    "  \n",
    "  \n",
    "<br>  \n",
    "\n",
    "互信息I(X;Y)：表示X由于Y已知而导致自身不确定的减少的增量。（X由于Y已知而减少的信息量。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
