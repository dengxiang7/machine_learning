{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用的优化目标\n",
    "真实的数据分布：$P_{data}(x)\\quad$是自然界中数据的真实分布情况。  \n",
    "  \n",
    "模型分布：$P_{model}(x|\\theta)\\quad$是人类创建一些分布模型用来靠近数据的真实分布，如均匀分布，高斯分布，指数分布等。  \n",
    "  \n",
    "经验分布：$\\hat{P}_{data}(x)\\quad$在训练数据集上对真实分布$P_{data}(x)\\;\\;$的估计  \n",
    "  \n",
    "三者的关系：在训练数据上借助模型分布$P_{model}(x|\\theta)\\quad$求得分布的参数$\\theta\\;\\;$求得$\\hat{P}_{data}(x)$借此逼近真实分布$P_{data}(x)\\quad$。  \n",
    "         \n",
    "        \n",
    "  <br>  \n",
    "  \n",
    "### 信息熵  \n",
    "信息熵：人们常常说信息很多，或者信息较少，但却很难说清楚信息到底有多少，而信息熵就是用来描述信息量的多少。热力学中的热熵是表示分子状态混乱程度的物理量。香农用信息熵的概念来描述信源的不确定度 。  一个信源发送出什么符号是不确定的，衡量它可以根据其出现的概率来度量。概率大，出现机会多，不确定性小；反之不确定性就大。  \n",
    "不确定性函数f是概率P的减函数，两个独立符号所产生的不确定性应等于各自不确定性之和故不确定性函数为对数函数：$f(p)=log(\\frac{1}{p})=-lpg(p)$  \n",
    "  \n",
    "在信源中，考虑的不是某一单个符号发生的不确定性，而是要考虑这个信源所有可能发生情况的平均不确定性。若信源符号有n种取值：U1…Ui…Un，对应概率为：P1…Pi…Pn，且各种符号的出现彼此独立。这时，信源的平均不确定性应当为单个符号不确定性-logPi的统计平均值（E），可称为信息熵，即  \n",
    "$$H(u)=E(-log_2p_i)=-\\sum_{i=1}^np_ilog_2p_i$$  \n",
    "  \n",
    "    \n",
    " <br>  \n",
    "   \n",
    "     \n",
    " ### 最大化似然函数\n",
    "$arg \\;\\;\\underset{\\theta}{max}p(x_1,x_2,\\cdots,x_n|\\theta)\\;=\\;arg \\;\\;\\underset{\\theta}{max}\\;p_{model}(x_1,x_2,\\cdots,x_n|\\theta)\\;=\\;arg \\;\\;\\underset{\\theta}{max}\\;\\sum_{i=1}^n log_2p_{model}(x_i|\\theta)\\;=\\;arg \\;\\;\\underset{\\theta}{max}\\;\\frac{1}{n}\\sum_{i=1}^n log_2p_{model}(x_i|\\theta)$  \n",
    "  \n",
    "  在训练集上假设样本都服从的模型分布，求得模型得最大参数，让更多数据都服从这个模型分布，逼近可能得真实分布\n",
    "  \n",
    "    \n",
    "### 交叉熵  \n",
    "主要用于度量两个概率分布间的差异性信息  \n",
    "  \n",
    "  经验分布作为交叉熵的训练集的先验信息，模型分布作为后验信息。  \n",
    "    \n",
    " 目标是最小化经验分布于模型分布得交叉熵：  \n",
    "   \n",
    "   $\\underset{\\theta}{min}=E_{x： \\hat{p}_{data}}\\;\\;[-log_2P_{model}(x|\\theta)]\\;=\\;\\sum_{i=1}^n- \\hat{p}_{data}(i)log_2p_{model}(x|\\theta)$    \n",
    "     \n",
    "<br>  \n",
    "\n",
    "<br>\n",
    "     \n",
    "       \n",
    "### KL距离(相对熵)\n",
    "又被称为Kullback-Leibler散度（Kullback-Leibler divergence）或信息散度（information divergence），是两个概率分布（probability distribution）间差异的非对称性度量 。在在信息理论中，相对熵等价于两个概率分布的信息熵（Shannon entropy）的差值  \n",
    "  \n",
    "  \n",
    "设 $P(x)和Q(x)$是随机变量$X$ 上的两个概率分布，则在离散和连续随机变量的情形下，相对熵的定义分别为 \n",
    "$$KL(P|Q|)=\\sum P(x)log_2\\frac{P(x)}{Q(x)}$$\n",
    "$$KL(P|Q|)=\\int P(x)log_2\\frac{P(x)}{Q(x)} dx$$  \n",
    "  \n",
    "    \n",
    "  优化目标最小化经验分布和模型分布的KL距离：  \n",
    "  $$\\underset{\\theta}{min} \\sum_{x:\\hat{p}_{data}}\\hat{p}_{data}(x)log_2 \\frac{\\hat{p}_{data}(x)}{p_{model}(x)}$$   \n",
    "    \n",
    "      \n",
    "        \n",
    "  <br>\n",
    "  优化目标 最大化似然函数 和  交叉熵  和 KL距离(相对熵) 等价   \n",
    "    \n",
    "      \n",
    "  <br>  \n",
    "  \n",
    "<br>  \n",
    " \n",
    "### 互信息  \n",
    "互信息(Mutual Information)是信息论里一种有用的信息度量，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性  。  \n",
    "  \n",
    "它度量两个对象之间的相互性。  \n",
    "  \n",
    " 是两个随机变量统计相关性的测度，使用互信息理论进行特征抽取是基于如下假设：在某个特定类别出现频率高,但在其他类别出现频率比较低的词条与该类的互信息比较大。通常用互信息作为特征词和类别之间的测度，如果特征词属于该类的话，它们的互信息量最大。由于该方法不需要对特征词和类别之间关系的性质作任何假设，因此非常适合于文本分类的特征和类别的配准工作 [2]  。\n",
    "参考资料  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
