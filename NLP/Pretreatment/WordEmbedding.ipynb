{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac60e547",
   "metadata": {},
   "source": [
    "# 词的向量表示\n",
    "   将词项用数学形式表示，便于使用数学方法进行分析。称为词的向量表示。（以下有的是词向量有的是文档向量）  \n",
    "   1、词袋向量表示（词频向量）2、n-gram 词袋向量表示  3、TF-IDF向量（词项频率 * 逆文档频率）4、主题向量 5、词向量（Word2vec）\n",
    "   \n",
    "  \n",
    "#### 词频向量（词袋向量）\n",
    "        用词的one-hot向量对词进行表示得到的文本数据会非常之大，并不适合计算机进行NLP处理。  \n",
    "        假设：一个句子的大部分意义都可以从词的本身获取，可以忽略词的顺序和语法，并将他们混合在一个‘袋子’中，每个句子或者每个文档对应一个袋。这个袋子里的词向量可以用来在忽略词序和语法的情况下仍能概括文档的本质内容。\n",
    "        词频向量：使用上述的词袋向量将文档信息压缩。即：将所有词向量表示的one-hot向量相加，得到一个词袋向量表示（只有一个向量，而不是n个向量）这个词袋向量也被称作词频向量，因为他只计算了词的频率，而不是词的顺序。  \n",
    "        （两种词袋向量：1、所有one-hot向量相加的词频向量 2、所有one-hot向量进行or运算的二值词袋向量）\n",
    "        \n",
    "        \n",
    "#### TF-IDF向量（词项频率 * 逆文档频率）\n",
    "        TF-IDF :能够更好地表示单个词项的重要度得分。\n",
    "        假设：一个词在文档中出翔的次数越多，那么该词对文档的意义贡献越大。\n",
    "        某个词的TF-IDF向量表示：词项频率 * 逆文档频率。其中词项频率指的是某个词在谋篇文章中出现的总次数（TF），逆文档频率指的是文档集合中总文档数除以某个词出现的文档总数(IDF)。\n",
    "        归一化词项频率：某个词出现的频率除以文档中词项总数。即归一化词项频率是经过文档长度调和后的词频。但是为什么要调和呢？（dog在文档A中出现3次，在B中出现100次，那么该词对B更重要吗？不一定，因为A总共就30个词，但是B却有580000个词，如此该词对文档A更重要）\n",
    "        TF计算方式：TF('dog',documentA)=3/30=0.1  ; TF('dog',documentB)=100/580000=0.00017\n",
    "        IDF:逆文档频率，即某个词条在该文档的稀缺性，如果某个词条在谋篇文档中出现多次，但是很少出现在语料库中其他的文档中，那么就可以假设该词条对当前文档非常重要。\n",
    "        TF-IDF=TF*IDF(词项频率 * 逆文档频率)          \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\text{tf}(t,d)=\\frac{\\text{单词t在文档d中出现的次数}}{\\text{文档d的长度}}$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\text{idf}(t,D)=log\\frac{\\text{文档总数}}{\\text{包含单词t的文档数}}$  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\text{tf-idf}(t,d,D)=\\text{tf}(t,d) \\times \\text{idf}(t,D)$\n",
    " &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\color{#F00}{某个词条在该文档的稀缺性，如果某个词条在谋篇文档中出现多次，但是很少出现在语料库中其他的文档中，那么就可以假设该词条对当前文档非常重要。例如：dog 只在A中出现，在其他文档中没有出现，那么dog对文档A很重要（文档总数/包含词的文档数；包含该词的文档数越少，那么逆文档值越大，对该文档越重要。），当然还要看dog在文档A中出现的归一化频率。故此就得出了tf-idf公式词频*逆文档频率。} $       \n",
    "        \n",
    "\n",
    "#### n-gram 词袋向量表示\n",
    "        pass\n",
    "        \n",
    "#### 主题向量   \n",
    "  \n",
    "#### 词向量（Word2Vec）  \n",
    "\n",
    "&emsp;&emsp; 一种考虑词的上下文语义的词的向量表示方法。两种：连续词袋模型（continuous bag-of-words,CBOW）和跳词模型（Skip-gram）。一种基于词与词之间的共现信息实现词向量的学习。  \n",
    "  \n",
    "&emsp;&emsp;  1、连续词袋模型（continuous bag-of-words,CBOW）  \n",
    "&emsp;&emsp;  &emsp;&emsp;  给定一段文本，根据上下文的信息对目标词进行预测。\n",
    " \n",
    "### 齐普洛夫定律\n",
    "    齐普洛夫定律：在给定的自然语言语料库中，任何一个词的频率与它在频率表中的排名成反比。\n",
    "    \n",
    "### 主题建模（基于TF-IDF）\n",
    "    仅仅对词计数的向量（词袋的二值向量、词频向量）并不能像它们的TF-IDF那样具有可描述性。因此我们在每篇文档中用TF-IDF代替词的TF形成新的向量表示。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a013de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
