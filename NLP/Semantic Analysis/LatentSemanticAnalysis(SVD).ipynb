{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2953aac3",
   "metadata": {},
   "source": [
    "# 隐性语义分析（Latent Semantic Analysis，LSA）\n",
    "&emsp;&emsp; LSA和PCA的数学计算方法一样。  \n",
    "     \n",
    "&emsp;&emsp; LSA基于最古老和最常用的降维技术——奇异值分解(SVD)  \n",
    "  \n",
    "### 奇异值分解（Singular Value Decomposition, SVD）  \n",
    "#### 特征值和特征向量\n",
    "$$Ax=\\lambda x$$  \n",
    "\n",
    "&emsp;&emsp; $A$是$n\\times n $的矩阵，$x$是向量。  \n",
    "  \n",
    "&emsp;&emsp;用特征值和特征向量对矩阵A进行分解，$n$个特征值$\\lambda_1\\leq \\lambda_2 \\leq \\cdots \\leq \\lambda_n$ 对应$n$个特征向量$\\{w_1,w_2,\\cdots,w_n\\}$如果这$n$个特征向量线性无关，那么矩阵$A$可以表示为($\\Sigma$为特征值组成的对角矩阵，$W$为特征向量矩阵)：  \n",
    "$$A=W\\Sigma W^{-1}$$  \n",
    "  \n",
    "&emsp;&emsp; 一般把特征向量向量标准化$||w_i||_2$,或者说$w_i^Tw_i = 1$,此时$W$的$n$个特征向量为标准正交基，满足$W^TW = I$即$W^T = W^{-1}$则\n",
    "$$A=W\\Sigma W^T$$   \n",
    "\n",
    "$\\color{#F00}{矩阵A必须为方阵，才能进行特征值分解。}$\n",
    "  \n",
    "#### 奇异值分解（SVD）  \n",
    "&emsp;&emsp; SVD也是对矩阵进行分解，但是和特征分解不同，SVD并不要求要分解的矩阵为方阵。假设的矩阵$A$是一个$m\\times n$的矩阵，那么我们定义矩阵A的SVD为： \n",
    "$$A=U\\Sigma V^T$$   \n",
    "  \n",
    "&emsp;&emsp;  其中$U$是一个$m\\times m$的矩阵，$\\Sigma$是一个$m\\times n$的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，$V$是一个$n\\times n$的矩阵。$U$和$V$都是酉矩阵，即满足$U^TU=I,V^TV=I , U^T=U^{-1},V^T=V^{-1}$。$U$ 是左奇异矩阵，$V$是右奇异矩阵。  \n",
    "  \n",
    " &emsp;&emsp;求解矩阵:$U、\\Sigma 、V$   \n",
    "   \n",
    " &emsp;&emsp;如果我们将$A$的转置和$A$做矩阵乘法，那么会得到$n\\times n$的一个方阵$A^TA$。既然$A^TA$是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式：    \n",
    " $$(A^TA)v_i = \\lambda_i v_i$$  \n",
    "   \n",
    " &emsp;&emsp;这样我们就可以得到矩阵$A^TA$的$n$个特征值和对应的$n$个特征向量$v$了。将$A^TA$的所有特征向量张成一个$n\\times n$的矩阵$V$，就是我们SVD公式里面的$V$矩阵了。一般我们将$V$中的每个特征向量叫做$A$的右奇异向量。  \n",
    "  \n",
    "&emsp;&emsp;如果我们将$A$和$A$的转置做矩阵乘法，那么会得到$m\\times m$的一个方阵$AA^T$。既然$AA^T$是方阵，那么我们就可以进行特征分解，得到的特征值和特征向量满足下式：   \n",
    " $$(AA^T)u_i = \\lambda_i u_i$$    \n",
    "  \n",
    "&emsp;&emsp;这样我们就可以得到矩阵$AA^T$的$m$个特征值和对应的$m$个特征向量$u$了。将$AA^T$的所有特征向量张成一个$m\\times m$的矩阵$U$，就是我们SVD公式里面的U矩阵了。一般我们将U中的每个特征向量叫做A的左奇异向量。\n",
    "\n",
    "&emsp;&emsp;$U$和$V$我们都求出来了，现在就剩下奇异值矩阵$\\Sigma$没有求出了。由于$\\Sigma$除了对角线上是奇异值其他位置都是0，那我们只需要求出每个奇异值$\\sigma$就可以了。  \n",
    " 我们注意到: \n",
    "$$A= U\\Sigma V^T  \\implies AV= U\\Sigma V^TV  \\implies AV= U\\Sigma \\implies Av_i= \\sigma_i u_i \\implies \\sigma_i= Av_i/u_i$$\n",
    "   \n",
    " 这样我们可以求出我们的每个奇异值，进而求出奇异值矩阵$\\Sigma$。\n",
    "\n",
    "上面还有一个问题没有讲，就是我们说$A^TA$的特征向量组成的就是我们SVD中的$V$矩阵，而$AA^T$的特征向量组成的就是我们SVD中的$U$矩阵，这有什么根据吗？这个其实很容易证明，我们以V矩阵的证明为例。  \n",
    "  \n",
    "  $$A= U\\Sigma V^T   $$  \n",
    "  \n",
    "  $$A^T= V\\Sigma^T U^T   $$  \n",
    "  \n",
    "  $$A^TA= V\\Sigma^T U^TU\\Sigma V^T  = V\\Sigma^2 V^T $$\n",
    "  \n",
    "\n",
    "     \n",
    "上式证明使用了:$U^TU=I,\\Sigma^T\\Sigma=\\Sigma^2$。可以看出$A^TA$的特征向量$V$组成的的确就是我们SVD中$A$的$V$矩阵。类似的方法可以得到$AA^T$的特征向量组成的就是我们SVD中的U矩阵。\n",
    "\n",
    "进一步我们还可以看出$A^TA$的特征值矩阵$\\lambda$等于奇异值矩阵$\\Sigma$的平方，也就是说特征值$\\lambda$和奇异值$\\Sigma$满足如下关系：  \n",
    "  \n",
    " $$\\lambda = \\Sigma ^2$$   \n",
    "   \n",
    "这里$A^TA$是已知的所以很好求其特征值$\\lambda$,在已知$\\lambda$的情况下，用上述的公式可以方便地球的奇异矩阵的特征值$\\Sigma$  \n",
    "  \n",
    " $$\\sigma_i = \\sqrt{\\lambda_I}$$  \n",
    "   \n",
    " 这样也就是说，我们可以不用$\\sigma_i= Av_i/u_i$来计算奇异值，也可以通过求出$A^TA$的特征值取平方根来求奇异值。   \n",
    "   \n",
    "     \n",
    "###  利用SVD 完成隐性语义分析 LSA  \n",
    "&emsp;&emsp; 利用SVD,LSA可以将TF-IDF矩阵分解成三个更简单的矩阵。这三个更简单矩阵揭示了原始TF-IDF的一些性质。利用这些性质简化原始矩阵，如将这些矩阵相乘之前，对其进行截断处理（忽略一些行和列），以减少向量空间模型中需要处理的维数。这些截断矩阵相乘并不等于原始矩阵，然而他们却给出了一个更好的矩阵。$\\color{#F00}{文档新的表示包含了这些文档的本质，即隐性语义}$。这就是SVD被用于数据压缩的原因，他能捕捉数据集的本质，并且忽略噪声。  \n",
    "  \n",
    " &emsp;&emsp; 在自然语言中以这种方式使用SVD时，我们将其称为隐性语义分析（LSA）。  \n",
    "   \n",
    " &emsp;&emsp;这种使用SVD的方法，被称为截断奇异值分解。  \n",
    "    \n",
    " &emsp;&emsp; LSA使用SVD查找导致数据种最大方差的词的组合。我们可以旋转TF-IDF向量，使旋转后的向量的新维度（基向量）于这些最大方差的词组合方向一致。这样每个轴都变成了词频的组合，而不是单个词频，每个轴都是一种主题（主题由各个主题词加权组合）。  \n",
    "   \n",
    " &emsp;&emsp;  每个轴是一个主题，机器实质上并不理解这个轴所代表的含义（可以由我们给这些主题起个名字）。实质上我们也不需要为他们起名字，就可以使用他们进行数学运算（如用这些主题语义估算文档之间的相似性，用语义完成搜索等，都不必需要知道其语义的名字）。  \n",
    "  \n",
    "&emsp;&emsp; 左奇异矩阵U：包含词项-主题矩阵，它给出每个词所具有的上下文信息。（词语义）（这是NLP中最重要的语义分析矩阵。）   \n",
    "   \n",
    "&emsp;&emsp; 奇异值矩阵S: 奇异值给出了在新的语义（主题）向量空间中灭个维度（轴）所代表的信息量。  \n",
    "    \n",
    "&emsp;&emsp; 右奇异矩阵$V^T$：是一个文档-文档矩阵，该矩阵将在文档之间提供共享的语义。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe845d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf6cb80f",
   "metadata": {},
   "source": [
    "刘建平SVD： https://www.cnblogs.com/pinard/p/6251584.html\n",
    "![](img/SVD.jpeg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af52829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
