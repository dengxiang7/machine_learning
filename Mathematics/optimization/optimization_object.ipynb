{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 常用的优化目标\n",
    "真实的数据分布：$P_{data}(x)\\quad$是自然界中数据的真实分布情况。  \n",
    "  \n",
    "模型分布：$P_{model}(x|\\theta)\\quad$是人类创建一些分布模型用来靠近数据的真实分布，如均匀分布，高斯分布，指数分布等。  \n",
    "  \n",
    "经验分布：$\\hat{P}_{data}(x)\\quad$在训练数据集上对真实分布$P_{data}(x)\\;\\;$的估计  \n",
    "  \n",
    "三者的关系：在训练数据上借助模型分布$P_{model}(x|\\theta)\\quad$求得分布的参数$\\theta\\;\\;$求得$\\hat{P}_{data}(x)$借此逼近真实分布$P_{data}(x)\\quad$。  \n",
    "         \n",
    "        \n",
    "  <br>  \n",
    "  \n",
    "### 信息熵  \n",
    "信息熵：人们常常说信息很多，或者信息较少，但却很难说清楚信息到底有多少，而信息熵就是用来描述信息量的多少。热力学中的热熵是表示分子状态混乱程度的物理量。香农用信息熵的概念来描述信源的不确定度 。  一个信源发送出什么符号是不确定的，衡量它可以根据其出现的概率来度量。概率大，出现机会多，不确定性小；反之不确定性就大。  \n",
    "不确定性函数f是概率P的减函数，两个独立符号所产生的不确定性应等于各自不确定性之和故不确定性函数为对数函数：$f(p)=log(\\frac{1}{p})=-lpg(p)$  \n",
    "  \n",
    "在信源中，考虑的不是某一单个符号发生的不确定性，而是要考虑这个信源所有可能发生情况的平均不确定性。若信源符号有n种取值：U1…Ui…Un，对应概率为：P1…Pi…Pn，且各种符号的出现彼此独立。这时，信源的平均不确定性应当为单个符号不确定性-logPi的统计平均值（E），可称为信息熵，即  \n",
    "$$H(u)=E(-log_2p_i)=-\\sum_{i=1}^np_ilog_2p_i$$  \n",
    "  \n",
    "    \n",
    " <br>  \n",
    "   \n",
    "     \n",
    " ### 最大化似然函数\n",
    "$arg \\;\\;\\underset{\\theta}{max}p(x_1,x_2,\\cdots,x_n|\\theta)\\;=\\;arg \\;\\;\\underset{\\theta}{max}\\;p_{model}(x_1,x_2,\\cdots,x_n|\\theta)\\;=\\;arg \\;\\;\\underset{\\theta}{max}\\;\\sum_{i=1}^n log_2p_{model}(x_i|\\theta)\\;=\\;arg \\;\\;\\underset{\\theta}{max}\\;\\frac{1}{n}\\sum_{i=1}^n log_2p_{model}(x_i|\\theta)$  \n",
    "  \n",
    "  在训练集上假设样本都服从的模型分布，求得模型得最大参数，让更多数据都服从这个模型分布，逼近可能得真实分布\n",
    "  \n",
    "    \n",
    "### 交叉熵  \n",
    "主要用于度量两个概率分布间的差异性信息  \n",
    "  \n",
    "  经验分布作为交叉熵的训练集的先验信息，模型分布作为后验信息。  \n",
    "    \n",
    " 目标是最小化经验分布于模型分布得交叉熵：  \n",
    "   \n",
    "   $\\underset{\\theta}{min}=E_{x： \\hat{p}_{data}}\\;\\;[-log_2P_{model}(x|\\theta)]\\;=\\;\\sum_{i=1}^n- \\hat{p}_{data}(i)log_2p_{model}(x|\\theta)$  \n",
    "     \n",
    "       \n",
    "### KL距离(相对熵)\n",
    "又被称为Kullback-Leibler散度（Kullback-Leibler divergence）或信息散度（information divergence），是两个概率分布（probability distribution）间差异的非对称性度量 。在在信息理论中，相对熵等价于两个概率分布的信息熵（Shannon entropy）的差值  \n",
    "  \n",
    "  \n",
    "设 $P(x)和Q(x)$是随机变量$X$ 上的两个概率分布，则在离散和连续随机变量的情形下，相对熵的定义分别为 \n",
    "$$KL(P|Q|)=\\sum P(x)log_2\\frac{P(x)}{Q(x)}$$\n",
    "$$KL(P|Q|)=\\int P(x)log_2\\frac{P(x)}{Q(x)} dx$$  \n",
    "  \n",
    "    \n",
    "  优化目标最小化经验分布和模型分布的KL距离：  \n",
    "  $$\\underset{\\theta}{min} \\sum_{x:\\hat{p}_{data}}\\hat{p}_{data}(x)log_2 \\frac{\\hat{p}_{data}(x)}{p_{model}(x)}$$   \n",
    "    \n",
    "      \n",
    "        \n",
    "  <br>\n",
    "  优化目标 最大化似然函数 和  交叉熵  和 KL距离(相对熵) 等价   \n",
    "    \n",
    "      \n",
    "  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
