{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 机器学习中求解目标函数的最优解     \n",
    "  \n",
    "<br>  \n",
    "\n",
    "* 求解最优解的两种方法：  \n",
    "  \n",
    "  1、解析解求出最优值得精确解  \n",
    "    \n",
    "  2、数值解逼近精确解\n",
    "\n",
    "<br>\n",
    "\n",
    "* 前置知识：解析解，数值解  \n",
    "  \n",
    "  解析解：从公式推导出的解的形式  \n",
    "    \n",
    "  数值解：用某种计算方法，让数值迭代逼近最优解。（常用的两种方法：梯度下降法，牛顿法）\n",
    "  \n",
    "<br>\n",
    "  \n",
    "在机器学习中最终我们建模得到一个损失函数，而我们的目标就是获得这个损失函数的最优解。来得到这个最优解时的参数。  \n",
    "\n",
    "<br>\n",
    "    \n",
    "问题是：有时候我们的目标函数非常复杂，并不一定都能得到其解析解。所以面对这些得不到解析解的函数，我们只能去求其数值解。  \n",
    "  \n",
    "  \n",
    "<br>  \n",
    "  \n",
    "### 用梯度下降法求解最优的数值解  \n",
    "  \n",
    "* 梯度下降法 ：(计算方法看数学知识中梯度部分)  \n",
    "  \n",
    "  **$\\color{red}{计算梯度：就是求函数f在点x_i时往哪个方向变化最快}$**    \n",
    "  \n",
    "  <br>\n",
    "    \n",
    "  **$\\color{red}{梯度下降法：就是求函数f在点x_i时往哪个方向变化最快后，沿着这个变化最快的方向走一小步，不走大步是防止走过头了。}$**  \n",
    "    \n",
    "  **$\\color{red}{ 这样对这个过程经过多次迭代后就会逼近其最优的解析解x^*}$**  \n",
    "  \n",
    "  <br>  \n",
    "    \n",
    "* 缺陷：  \n",
    "    1、步伐大小的设置：太大走过头，会在最优值附近震荡且偏离最优值较远。 步伐太小导致学习速度过慢。   \n",
    "    \n",
    "    2、对于凸函数理论上可以逼近全局最优解的值。但是对于非凸函数上：理论上我们可能会陷入局部最优解（想象连续的高低不同的山峰）。这时候我们是否会陷入局部最优解很大程度上取决于迭代初始点随机选取的位置。（解决方法：模拟退火法等）。  \n",
    "      \n",
    "        \n",
    "<br>  \n",
    "  \n",
    "### 在神经网络中用梯度下降法求解数值解的过程     \n",
    "    \n",
    "<br>  \n",
    "\n",
    "![](../../img/math/numerical-process/optimaVal.PNG)\n",
    "      \n",
    "<br>  \n",
    "   \n",
    "**注意：最终损失函数模型，这个函数的参数是神经网络的参数，是要求的梯度的参数，不是我们输入的样本，我们输入样本的值的作用：在这个损失模型函数里样本值就相当常数项，和系数**     \n",
    "     \n",
    "<br>  \n",
    "  \n",
    "   1、首先建立一个损失函数的数学模型。（这个数学模型函数的参数：就分布在神经网络中各个层的参数。（这些参数是损失模型函数里的参数））  \n",
    "     \n",
    "   2、为这些参数初始化一个值。（也就是求梯度时，是从函数哪个点，开始求其梯度，（也就是最开始从哪点开始往最优解的位置行走））  \n",
    "     \n",
    "   3、 最后只需要对这个损失函数进行多次迭代，要么设置迭代轮数，要么设置迭代停止条件。让最后伦次的数值解逼近最优解。  \n",
    "     \n",
    "   4、最后获得的最优数值解值就是模型的参数应该有的值。也就获得一个具体的模型  \n",
    "     \n",
    "<br>  \n",
    "  \n",
    "* 参数走向和参数与样本值的关系：   \n",
    "  \n",
    "  参数走向:  \n",
    "  \n",
    "  数学模型函数里参数：这个模型的输入是样本值，输出是我们想要的数值。所以这个模型的数学函数：$f$：的变量X是输入样本值。另外还要一些固定的变量值$\\theta$。  \n",
    "    \n",
    "  损失函数里参数：我们把模型函数里固定的变量值$\\theta$，和损失函数里原本就有的变量$\\eta$，共同组成损失函数的变量。$Loss(\\theta,\\eta)$  \n",
    "    \n",
    "  参数走向：模型里是固定值$\\theta_m$，损失函数里是变量$\\theta_l$。\n",
    "     \n",
    " <br>  \n",
    "  \n",
    "* 损失函数与样本值得关系：  \n",
    "  \n",
    "  损失函数调整模型里固定变量值$\\theta_m$。让模型函数对样本值处理得到我们想要的值。这里模型函数的变量空间是所有样本的取值域。损失函数的变量空间是$\\theta_m$的所有取值域。  \n",
    "    \n",
    "  损失函数求$\\theta$的最优点$\\rightarrow\\;$ $\\theta$作为模型函数的固定变量值，让模型对输入样本值处理后输出的值，是能够让损失函数最小的值  \n",
    "    \n",
    "  <br>   \n",
    "  \n",
    "  这里模型输出的数值是某一个函数的值域变化。所以损失函数调整模型的固定变量值，也就是调整，整个模型的函数。（或者说改变模型函数，让改变后的模型函数，的输出值能让损失函数最小。）\n",
    "   \n",
    " <br>  \n",
    "   \n",
    " <br>\n",
    " \n",
    "   \n",
    "### 总结模型、损失函数，还有优化之间的关系   \n",
    "\n",
    " \n",
    "模型：是一个函数：变量$X$是输入样本值。网络的固定参数$\\theta$决定了这是一个什么样函数。(参数$\\theta$可以让这个模型是任意的函数。) \n",
    "      \n",
    "$\\;\\;\\;;\\qquad$这个函数的功能：是实现我们的想法，把输入样本值处理成我们想要的值。（我们知道有这样一个函数，但我们不知道这个函数的具体形 \n",
    "      \n",
    "$\\;\\;\\;\\qquad$ 式，所以我们用网络固定参数$\\theta$，来决定这是一个什么样的函数，通过改变$\\theta$值，能逼近任意的函数，也就是逼近能实现我们想法的函数）  \n",
    "  \n",
    "<br>  \n",
    "\n",
    "模型的总结：利用改变模型的固定参数$\\theta$值，能逼近任意的函数，这一定理。来通过改变模型的固定参数$\\theta$的值，来逼近能实现我们想法的函数。  \n",
    "  \n",
    "<br>  \n",
    "  \n",
    "损失函数：就是计算模型的输出值与我们理想值之间的差距。这里这个损失函数的变量：是模型的固定参数$\\theta$，和一些自身需要的变量$\\eta$。  \n",
    "  \n",
    "<br>  \n",
    "  \n",
    "优化：目的就是想通过改变参数$\\theta$的值，来改变模型的函数，让改变后的模型函数能够实现我们的想法。  \n",
    "  \n",
    "<br>  \n",
    "  \n",
    "<br>  \n",
    "  \n",
    "总结：模型是能够实现我们想法的函数，这个函数的形式由固定参数$\\theta$决定。（函数的变量是输入样本）。损失函数就是：这个能实现我们想法的函数的输出值与理想值的差距。(变量是$\\theta，\\eta$)，我们通过优化这个损失函数，来改变模型的函数，逼近能实现我们想法的函数。（优化是通过梯度下降法）  \n",
    "  \n",
    "<br>  \n",
    "\n",
    "### 优化的流程  \n",
    "  \n",
    "  1、建立好模型  \n",
    "    \n",
    " 2、得到损失函数  \n",
    "  \n",
    " 3、只对损失函数进行迭代优化（迭代梯度下降法），对决定函数形式的变量$\\theta$进行求解最优数值解。求出数值解，也就求出了能实现我们想法的函数是什么样的形式。  \n",
    "   \n",
    " 4、最终得数值解就是我们最后要得到的能实现我们想法的模型  \n",
    "   \n",
    "     \n",
    " <br>  \n",
    "   \n",
    "<br>  \n",
    "  \n",
    "我们建立的各种神经网络（各种CNN，RNN,），就是为了能够产生更接近我们想法的函数。建立这个模型的最总想法就是为了得到损失函数。 所以我们最后优化的时候是直接对损失函数进行优化。而不是直接对模型进行优化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
