{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 变分自编码器  \n",
    " \n",
    "###  引言  \n",
    "  \n",
    "  生成模型是机器学习中的一个广泛领域，它是处理数据概率分布$P(x)$的模型。概率分布$P(x)$是定义在高维数据空间$X$中的数据点$x$的分布。  \n",
    "    \n",
    "  例如图片问题：每个样本都有成千上万的维度，生成模型的工作就是以某种方法来捕捉维度之间的依赖关系。比如相近的像素点之间的颜色应该相近。我们模型要做的就是捕捉维度之间的依赖。  \n",
    "    \n",
    "  生成模型一个直接的方法就是计算$P(x)$。在图片问题中，样本X要是看起来像真实的图片，那么$P(x)$的概率应该高。如果图片看起来是随机的噪声，那么概率应该较低。  \n",
    "    \n",
    "<br>  \n",
    "\n",
    "### 准备工作：隐变量模型   \n",
    "  \n",
    "  生成模型的维度之间依赖关系比较复杂时，这个模型训练过程是很困难的。  \n",
    "    \n",
    "  隐变量模型：由一些背后关键的隐藏变量，决定了前面的变量。（隐马尔可夫模型）。在VAE中，$Z$就是隐变量。  \n",
    "    \n",
    "  概率密度函数（PDF）:$P(z)$  \n",
    "    \n",
    "  编码函数：$f(z;\\theta) \\qquad\\quad  f：z \\times \\theta \\rightarrow X $   \n",
    "    \n",
    "$\\qquad\\qquad\\qquad$ 如果$z$随机，$\\theta$固定，那么$f(z;\\theta)$是X空间中随机样本。我们的期望是优化$\\theta$,目的是从$P(z)$中抽样，让$f(z;\\theta)$得到的X在真实数据中。  \n",
    "  \n",
    " 抽象为数学公式：最大化 $\\quad p(X)=\\int p(x|z,\\theta)p(z)dz$   \n",
    "   \n",
    " 在VAE中真实数据分布选择的是高斯分布。 $ p(x|z,\\theta)=N(x|f(z;\\theta),\\sigma^2 * I)) \\qquad$ 均值，方差，$f(z;\\theta)\\quad \\sigma^2$   \n",
    "   \n",
    "    \n",
    "<br>  \n",
    "\n",
    "<br>  \n",
    "\n",
    "### 变分自编码器  \n",
    "\n",
    "   变分自编码器的数学基础，和典型的自编码器，关系并不大。  \n",
    "     \n",
    "   变分自编码器，被称为自编码器仅仅是因为最终的训练目标源于一个‘编码器’，一个‘解码器’，而它们类似于传统的自编码器。  \n",
    "   \n",
    "   \n",
    "    \n",
    "<br>  \n",
    "\n",
    "<br>   \n",
    "   \n",
    "   \n",
    "<br>  \n",
    "\n",
    "<br>  \n",
    "\n",
    "   \n",
    "<br>  \n",
    "\n",
    "<br> \n",
    "\n",
    "### 准备工作：隐变量模型  \n",
    "\n",
    "  在训练生成模型时，维度之间的依赖关系越复杂，模型的训练就越困难。例如，生成手写字符图像的问题。为了简单起见，我们只关心数字0-9的建模。如果字符的左半部分包含5的左半部分，则右半部分不能包含0的左半部分，否则字符将很明显看起来不像任何真正的数字。直观地说，如果模型在给任何特定像素赋值之前先决定要生成哪个字符，这会很有帮助。这种决定在形式上被称为潜在变量。也就是说，在我们的模型绘制任何东西之前，它首先从集合[0，…，9]中随机抽取一个数字值z，然后确保所有笔划都与该字符匹配。z之所以被称为“潜在”，是因为只要给定模型产生的一个字符，我们不一定知道是哪些潜在变量的设置生成了这个角色。我们需要用计算机来推断。\n",
    "  \n",
    "    \n",
    " 在我们可以说我们的模型代表我们的数据集之前，我们需要确保对于数据集中的每个数据点X，都有一个（或多个）潜在变量的设置，这会导致模型生成与X非常相似的东西。形式上，假设我们在高维空间z中有一个潜变量向量z，我们可以根据z上定义的概率密度函数（PDF）P（z）对其进行采样。然后，假设我们有一系列确定性函数f（z；θ），由某个空间Θ中的向量θ参数化，其中f:z×Θ→X.f是确定性的，但是如果z是随机的，θ是固定的，那么f（z；θ）是空间X中的一个随机变量。我们希望优化θ，这样我们就可以从P（z）中采样z，并且，在很高的概率下，f（z；θ）将类似于我们数据集中的X  \n",
    "   \n",
    "  为了使这个概念在数学上更精确，我们的目标是在整个生成过程中使训练集中每个X的概率最大化  \n",
    "  \n",
    "  <br>  \n",
    "  \n",
    "  在这里，f（z；θ）被一个分布P（X | z；θ）代替，这使得我们可以利用全概率定律明确地表示X对z的依赖性.这个框架背后的直觉被称为“最大似然”，即如果模型很可能产生训练集样本，那么它也很可能产生相似的样本，而不太可能产生不同的样本。在V AEs中，这种输出分布的选择通常是高斯分布，即。  \n",
    "    \n",
    "   也就是说，它的平均值f（z；θ）和协方差等于单位矩阵I乘以某个标量σ（这是一个超参数）。这个替换是必要的，以形式化的直觉，一些z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
