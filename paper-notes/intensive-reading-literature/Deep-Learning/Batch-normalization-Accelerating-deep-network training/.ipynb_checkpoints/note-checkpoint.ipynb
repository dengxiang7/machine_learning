{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 标量标准化：通过降低内部协变量偏移加速神经网络训练 \n",
    "\n",
    "对照批量标准化标记：https://machine_learning/Deep-Learning/basic-theory/Batch-normalization.ipynb   \n",
    "\n",
    "<br>  \n",
    "  \n",
    "每层的数据分布：最原始数据是来自一个分布。经过一层后得到一个数据分布（作为下一层的输入）。而随着优化周期的进行会导致各个不同周期里同一层的参数也会不同。而对于某个固定层，如果每次参数不同，那么对于同一个数据分布的输入，这个层的不同参数也会导致得到不同的数据分布。  \n",
    "  \n",
    "<br>  \n",
    "  \n",
    "深层网络训练的难度：从输入数据分布中得到一个目标数据的分布。网络就是要从无数个分布中，确定中间分布。这需要较小的学习率，且初始化需要更精心的安排，所以深度网络训练很难。    \n",
    "\n",
    "  \n",
    "把以上的现象称为 Internal Covariate Shift\n",
    "\n",
    "<br>  \n",
    "\n",
    "#### 摘要  \n",
    "  \n",
    "  深度神经网络训练的复杂性：随着训练的进行每层的参数会随着周期的优化而改变，这导致每层输入的分布也随着优化周期的进行而改变。  \n",
    "    \n",
    " 这降低了训练的速度，因为每层的分布随着优化周期改变，（遍历所有中间层的分布）这就需要较低的学习率，且初始化需要更精心的安排，而且想要训练得到非线性的饱和区也非常困难。    \n",
    " （每层输入的分布随着优化周期的改变而改变，这就导致要得到最终数据的分布，中间层的分布都要符合要求，那么就需要遍历所有中间层的分布，这也就需要较低的学习率，而且随着层数的增加要得到非线性饱和区也非常困难，这些都导致了网络训练的困难性。）  \n",
    "   \n",
    " 把以上的现象称为 Internal Covariate Shift  \n",
    "     \n",
    " 解决方法：把每个小批量训练的输入层数据进行标准化  \n",
    "   \n",
    " 批量标准化：可以让我们使用较大的学习率，并且在初始化时可以更随意。  \n",
    "   \n",
    "<br>  \n",
    "  \n",
    "#### 引言  \n",
    "  \n",
    "  深度学习极大的提高了计算机视觉，语音识别，其他领域的水平。  \n",
    "    \n",
    "  随机梯度下降法（SGD）,其变体动量法和AdG ,用来训练神经网络。  \n",
    "  \n",
    " <br>\n",
    " \n",
    " * 随机梯度法（SGD）:\n",
    "    \n",
    " 随机梯度法（SGD）:求使总损失最小的参数$\\theta^*$  \n",
    "   \n",
    "   $\\theta^*=\\underset{\\theta}{arg\\;min}\\;\\frac{1}{N} \\; \\sum_{i=1}^N l(x_i,\\theta)$   \n",
    "     \n",
    " SGD过程：  \n",
    "   \n",
    "   $x_1,x_2,……,x_N$是训数据集，随机梯度下降法分步骤进行。每一步只考虑小批量数据$x_1,x_2,……,x_m$损失近似总损失 $\\;\\;\\frac{1}{m} \\; \\frac{\\partial\\; l(x_i,\\theta)}{\\partial \\theta}$  \n",
    "     \n",
    "   一次使用小批量样本比一次只使用一个样本是效果更好。  \n",
    "     \n",
    "   首先：小批量上的梯度是对整个数据集上梯度的近似估计，其近似程度随着批量数目的增加而增加。   \n",
    "     \n",
    "   其次：由于现代平台的并行性性能，批处理计算效率高  \n",
    "     \n",
    "   尽管随机梯度法简单有效，但是他要求超参数需要精心设计，调整，尤其是其学习率，和初始值。  而且每层输入的分布会不断改变，随着网络网络的加深参数的微小改变，会在网络中不断的引起变化，这些都导致了训练的困难性。  \n",
    "     \n",
    "<br>  \n",
    "\n",
    "* 批标准化：  \n",
    "  \n",
    "<br>  \n",
    "  \n",
    "<br>  \n",
    " \n",
    "#### 降低Internal Covariate Shift   \n",
    "  \n",
    "  我们将内协变量移位定义为在训练过程中由于网络参数的变化而引起的网络层分布的变化。  \n",
    "    \n",
    "  为了加速训练，我们要降低内部协变量的偏移。随着训练的进行，通过确定每层输入x的分布，我们期望提高训练速度。  \n",
    "    \n",
    " * 白化：  \n",
    "   \n",
    "   经过白化处理后，新的数据X'满足两个性质：(1)特征之间相关性较低；(2)所有特征具有相同的方差。  \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
