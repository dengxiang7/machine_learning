{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InfoGAN :通过信息最大化对抗生成网络学习可解释表征  \n",
    "  \n",
    "  本篇文章基于信息论的基础扩展GAN网络，它能够以完全无监督的方式学习有效特征。InfoGAN是去最大化，潜在变量的子集和样本之间的互信息。我们的目标是互信息的一个下界。这个下界可以有效的优化。  \n",
    "  **实验表明InfoGAN学到的可解释特征和有监督方式下学到的特征有的一比。**  \n",
    "    \n",
    " <br>  \n",
    " \n",
    "## 引言  \n",
    "  \n",
    "  无监督学习可以描述为：从巨大的无标签数据中学习有效的数据。现下最流行的无监督特征学习框架是表示学习。它的目标是用无监督数据去学习特征，这个特征能够揭示重要的语义。  \n",
    "  无监督学习的一个重要组成部分是生成模型。它的动机就是能够生成数据，好的生成模型能够自动的学习有效的特征。   \n",
    "    \n",
    "  **InfoGAN修改了GAN网络使其能够学习可解释的有意义的特征。首先我们先从GAN的噪声变量Z中固定一个小的子集a，然后最大化a和观测样本的互信息。这能够证明a和观测样本是直接相关的。尽管这很朴素，但是结果却惊人的有效。**    \n",
    "    \n",
    "  <br>  \n",
    "  \n",
    "  **有用的分离表示可以为以下每个属性分配一组单独的维度：面部表情、眼睛颜色、发型、是否有眼镜以及对应的人的身份。这能有效的帮助那些需要了解数据显著属性的自然任务。它能够在许多图像数据集上发现高度语义和有意义的隐藏表示**\n",
    "        \n",
    " <br>  \n",
    " \n",
    "## 引导作用的潜在编码的互信息 \n",
    "  \n",
    "  **在GAN的生成器中，噪声变量z的所有维度是一种高度纠缠的方式利用的。导致z的各个维度与数据的语义特征不对应。**  \n",
    "    \n",
    "  **然而很多领域中，是要把变量从语义上划分为各个有意义元素子集的。**   \n",
    "    \n",
    "  **比如在生成手写数字的时候。一个想法是：模型能够自动的分配一个离散的随机变量去代表生成0-9中哪个数字c，再选择两个额外的连续变量去代表数字的角度，和数字笔画的胖瘦z1,z2。**    \n",
    "    \n",
    "  **在这篇文章中：相较于使用单独的无结构向量z,我们提出把噪声向量z划分为两部分。1、把z作为不可压缩噪声的来源。2、加入c,把c叫做潜在编码和数据分布的突出语义结构**    \n",
    "  \n",
    "  **在本文中，我们不使用单一的非结构化噪声向量，而是将输入噪声向量分解为两部分：（i）z，它被视为不可压缩噪声的源；（ii）c，我们将其称为潜在代码，并针对数据分布的显著结构化语义特征**\n",
    "    \n",
    "   在数学上，我们把所有**潜在变量的结构集合**统一标注为：$c_1,c_2,……，c_L$，那么分布$P(c_1,c_2,……，c_L)=\\prod_{i=1}^L p(c_i)$  \n",
    "     \n",
    "   现在我们提出了**一种无监督的方式来发现这些潜在变量的结构集合**。生成网络的输入是c和z.形式变为G(z,c)。I(c,G(z,c))应该很高，即 c与G(z,c)互信息应该很高。   \n",
    "     \n",
    "   在信息论中I(X;Y)测量了X和Y之间的信息总量。  \n",
    "     \n",
    "   $$I(X;Y)=H(X)-H(X|Y)=H(Y)-H(Y|X)$$     \n",
    "   \n",
    "   假如我们知道了$X\\text{~}P_G(X)$，我们希望$P_G(C|X)$有一个微乎其微的信息熵。\n",
    "     \n",
    "   **直观的解释是：当Y给定时，X的不确定性的减少量。如果X和Y是相互独立的那么I(X;Y)=0。相反如果X和Y确实有某种确定性的联系，最大化互信息是可以得到的。**    \n",
    "     \n",
    "   **$\\color{red}{\\text{关键思想：给定任意一个}x\\text{~}P_G(x)\\text{我们都希望},P_G(c|x)\\text{都有一个极其微小的信息熵。}}$**   \n",
    "   **$\\color{red}{\\text{ 也就是说在损失函数，潜在变量c是不能丢失的。互信息表示我们应该去考虑变量c的影响。}}$**    \n",
    "     \n",
    "  $$\\underset{G}{min }\\;\\;\\underset{D}{max }\\;\\; V_I(D,G)=V(D,G)-\\lambda I(c,G(z,c))$$     \n",
    "    \n",
    " <br>  \n",
    " \n",
    " \n",
    "## 最大化互信息的变分  \n",
    "  \n",
    " $I(c,G(z,c))=H(c)-H(c|G(z,c))$  \n",
    "    \n",
    " $\\qquad\\qquad=\\sum_{i=1}^n p(c|G(z,c))log\\;p(c|G(z,c)) +H(c) $    \n",
    "   \n",
    " $\\qquad\\qquad=E_{p(c|G(z,c)}log \\;p(c|G(z,c)) +H(c) $   \n",
    "   \n",
    " $\\qquad\\qquad=E_{x\\text{~}G(z,c)}[E_{c'\\text{~}p(c|x)}log \\;p(c'|x)] +H(c) $    \n",
    "   \n",
    "设计一个辅助分布Q(c|x)让他接近P(c|x)   \n",
    "  \n",
    " $\\qquad\\qquad=E_{x\\text{~}G(z,c)}[E_{c'\\text{~}p(c|x)}[log \\;p(c'|x)- log \\; q(c|x) +log \\; q(c|x)]] +H(c) $   \n",
    "   \n",
    "$\\qquad\\qquad=E_{x\\text{~}G(z,c)}[\\underbrace{KL(p(c|x)||q(c|x))}_{\\geq 0}+E_{c'\\text{~}p(c|x)}[log \\; q(c|x)]] +H(c) $   \n",
    "  \n",
    "$\\qquad\\qquad \\geq E_{x\\text{~}G(z,c)}[E_{c'\\text{~}p(c'|x)}[log \\; q(c'|x)]] +H(c) $    \n",
    "  \n",
    "此时找到了互信息的下界。  \n",
    "  \n",
    "<br>  \n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "**引理：**  \n",
    "对于随机变量X，Y和函数f(x,y)，在适当的条件下：$E_{x\\text{~}X,y\\text{~}Y|x}[f(x,y)]=E_{x\\text{~}X，y\\text{~}Y|x，x'\\text{~}X|y}\\;\\;[f(x',y)]$  \n",
    "  \n",
    "<br>  \n",
    "\n",
    "<br>  \n",
    "\n",
    "则对上面的下界使用引理：  \n",
    "$I(c,G(z,c)) \\geq E_{x\\text{~}G(z,c)}[E_{c'\\text{~}p(c'|x)}[log \\; q(c'|x)]] +H(c) $   \n",
    "  \n",
    "$\\qquad\\qquad = E_{c\\text{~}p(c)}[E_{x\\text{~}G(z,c)}[E_{c'\\text{~}p(c'|x)}[log \\; q(c'|x)]] ]+H(c) $   \n",
    "  \n",
    "$\\qquad\\qquad = E_{c\\text{~}p(c)，x\\text{~}G(z,c)}\\;[log \\; q(c|x)]+H(c) $   \n",
    "  \n",
    "$\\qquad\\qquad=L_I(G,Q)$    \n",
    "  \n",
    "**$L_I(G,Q)$的显示表达式：可以MCMC采样模拟*    \n",
    "  \n",
    "<br> \n",
    "  \n",
    "**最总的目标函数为：**  \n",
    "  \n",
    "  \n",
    "$\\underset{G,Q}{min}\\;\\underset{D}{max}\\; V_{InfoGAN}(D,G,Q) \\;=\\; V(D,G)-\\lambda L_I(G,Q)$   \n",
    "\n",
    "\n",
    "### Q的形式  \n",
    "  \n",
    "  在实际中，通常把辅助分布Q设为一个神经网络。在InfoGAN中，我们让Q和D共享卷积层，但是有各自不同的输出层。D的顶层输出真假。Q的顶层为全连接层。  \n",
    "   这样做的好处是：InfoGAN只给GAN增加了微不足道的计算成本。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
