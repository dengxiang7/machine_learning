{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 高斯混合变分自编码器的深度无监督聚类  \n",
    "  \n",
    "## 摘要  \n",
    "本文研究了先验分布是高斯混合模型的变分自编码器，目的是想通过深度生成模型来完成无监督聚类。而且我们发现在常规的VAE中出现的over-regularisation  \n",
    "（过正则化），在我们的模型中也出现了。而且over-regularisation（过正则化）会导致cluster degeneracy(聚类衰退).  \n",
    "  \n",
    "我们发现了一种启发式的方法， minimum information constraint（最小化信息约束），它可以用在VAE中用来缓解 over-regularisation（过正则化）现象。  \n",
    "也可以用在我们的模型中来提升聚类性能。  \n",
    "   \n",
    "实验结果证明了模型的可解释性和聚类性能比最先进的方法还要好。    \n",
    "  \n",
    "## 引言    \n",
    "  \n",
    "  无监督聚类任然是机器学习领域中一项重要的基础性研究内容。**像K-means,GMM任然是很多聚类应用的核心。然而这些方法的相似性测量，被限制于数据空间的局部关系，不能捕捉潜在空间隐藏的分级依赖性关系。然而，深度生成模型有丰富的潜在结构。** 但是它确不能直接应用于无监督聚类问题，常常用于降维。  \n",
    "    \n",
    "   \n",
    "生成模型在做出潜在空间的一些假设的情况下评估数据的密度。   \n",
    "  \n",
    "然而在模型中用复杂的潜在结构去做推断是一件困难的事情。但是变分推断是一种很好的方法，现在流行的方法都是基于变分自编码器的。  \n",
    "  \n",
    "  \n",
    "### 相关工作    \n",
    "  \n",
    "  最近工作都是在用生成模型提取有语义意义的分离特征。例如InfoGAN  \n",
    "    \n",
    "### VAE    \n",
    "  \n",
    "  介绍VAE\n",
    "  \n",
    "### GMMVAE    \n",
    "  \n",
    " **isotropic：各方向同性。（性质不会因为方向的变化而出现改变。）** \n",
    "  \n",
    "  **在常规的变分自编码器中，潜在变量的先验分布通常是同性的高斯分布。选择这种先验分布会造成（样本的）多元高斯分布的每一维，都是从数据中学得的一个独立的连续变量因子。这就造成最终的生成样本的特征是结构化的，相互分离的。**  \n",
    "    \n",
    "  **常规VAE,的噪声变量z都是从同一个高斯分布中抽样的，（各方向同性），（真实样本是服从多维正态分布的）导致最终生成样本的每一维度都是相互独立，相互分离的。（生成样本模糊）**    \n",
    "    \n",
    " 虽然可解释性更好，但是每个特征都是单峰的，不能表示更复杂的特征。（学到特征不一定都是单峰的）\n",
    " \n",
    " \n",
    "    \n",
    "  本文选择GMM作为先验分布，以期能表示更复杂的信息。  \n",
    "    \n",
    " 如果我们假设数据是从GMMM中产生的。**那么推断数据属于哪一个类别等价于推断数据是由隐空间哪一个态生成的。**  \n",
    "   \n",
    " 虽然这使我们有可能将我们的潜在空间划分为不同的类，但在这个模型中的推断是非常重要的。**众所周知，通常用于V-AEs的重参数化技巧不能直接应用于离散变量。** 我们证明通过调整标准V-AE的结构，**我们的高斯混合变分自动编码器（GMV-AE）的变分下界的估计可以通过重新参数化技巧用标准反向传播来优化，从而使推理模型保持简单。**  \n",
    "   \n",
    "   \n",
    "### GMVAE的生成识别模型    \n",
    "  \n",
    "生成模型的结构：  \n",
    "![](img/1.PNG)  \n",
    "在这个生成模型里有：{y},{wz}条件独立于x，w与z相互独立。所以生成模型的概率公式为：  \n",
    "$P_{\\beta,\\theta}(y,x,w,z)=P_{\\beta,\\theta}(y,w,z|x)P(x)=P_{\\theta}(y|x)P_{\\beta}(w,z|x)P(x)=P_{\\theta}(y|x)P_{\\beta}(x,w,z)=P_{\\theta}(y|x)P_{\\beta}(x|w,z)P(w,z)=P_{\\theta}(y|x)P_{\\beta}(x|w,z)P(w)P(z)$  \n",
    "  \n",
    "最终可得：  \n",
    "$P_{\\beta,\\theta}(y,x,w,z)=P_{\\theta}(y|x)P_{\\beta}(x|w,z)P(w)P(z)$  \n",
    "\n",
    "<br>\n",
    "  \n",
    " 其中设定变量：  \n",
    "    \n",
    "  $W\\text{~}N(0,1) \\qquad\\qquad$为重参数技巧中得标准正态分布N(0,1)  \n",
    "    \n",
    "  $Z\\text{~}Mult(\\pi) \\qquad\\qquad$为多项分布，其中K为类别数，$\\pi_k=K^{-1}$,即多项分布为选择每个类别的概率相同（均匀分布）  \n",
    "    \n",
    "  $x|w  \\qquad\\qquad\\qquad$为通过参数为$\\beta$的神经网络，将输入的w产生，K个均值向量和K个方差对角阵，即产生了K个多维正态分布。组成高斯混合模型，  \n",
    "  $\\qquad\\qquad\\qquad\\qquad$只是没有每个组件对应的系数  \n",
    "    \n",
    "  $x|w,z \\text{~} \\prod_{k=1}^K \\; N(u_{zk}(w;\\beta),diag(\\sigma^2_{zk}(w;\\beta)\\;)\\;)^{zk} \\qquad$每个k对应一个多维正态分布。这个决定了x是自哪个模态，（zk取值是0or1）（即来自哪个组件）  \n",
    "    \n",
    "  $y|x \\text{~} \\; N(u(x;\\theta),diag(\\sigma^2(x;\\theta)\\;)\\;) \\qquad $通过参数为$\\theta$的神经网络,将从某个潜在模态中采样的潜在变量x，生成样本y. （真实样本y服从的也是多维正态分布。）    \n",
    "    \n",
    "  <br>  \n",
    "  \n",
    "  $N(0,1)\\rightarrow N(U,\\sum) \\text{多维正态分布的潜在抽样变量} \\rightarrow N(U,\\sum)\\text{样本服从的多维正态分布}$  \n",
    "    \n",
    "<br>   \n",
    "  \n",
    "### 用识别模型来进行推断    \n",
    "    \n",
    "    \n",
    "  按上述描述进行变分自编码器的设计之后，优化过程的需要使用变分推断，所以现在我们为变分推断，假设一个简单的带参数分布，也就是平均场变分族：\n",
    "   ![](img/2.PNG)  \n",
    "   \n",
    "  简单带参变分族为：$q_{\\phi x}(x|y)q_{\\phi w}(w|y)p_{\\beta}(z|x,w)$   \n",
    "    \n",
    "   通过调整这个变分族，让它靠近真实后验分布$q_{true}(x,w,z|y)$  \n",
    "   \n",
    "  \n",
    "  则有下界(ELBO)公式为：  \n",
    "    \n",
    "  $$log p(y)=E_{q_{mvf}\\;(x,w,z|y)\\;}[log \\frac{P_{\\beta,\\theta}(y,x,w,z)}{q_{mvf}\\;(x,w,z|y)}] - KL(q_{mvf}\\;(x,w,z|y)||q_{true}(x,w,z|y))$$    \n",
    "    \n",
    "  其中 ：  \n",
    "  $$q_{mvf}\\;(x,w,z|y)=q_{\\phi x}(x|y)q_{\\phi w}(w|y)p_{\\beta}(z|x,w) \\quad\\text{变分族}$$   \n",
    "    \n",
    "  $$L_{ELBO}=E_{q_{mvf}\\;(x,w,z|y)\\;}[log \\frac{P_{\\beta,\\theta}(y,x,w,z)}{q_{mvf}\\;(x,w,z|y)}]$$ \n",
    "    \n",
    "  \n",
    "   \n",
    "变分族中的$p_{\\beta}(z|x,w)$可以具体的推导为（这里x每一次为一个样本）：  \n",
    "  \n",
    "  $$p_{\\beta}(z_j =1|x,w)=\\frac{P(z_j=1)P(x|z_j=1,w)}{\\sum_{k=1}^K P(z_k=1)P(x|z_k=1,w)}$$  \n",
    "    \n",
    "  $$\\qquad\\qquad\\qquad= \\frac{\\pi_j N(x|u_j(w;\\beta),\\sigma_j(w;\\beta))}{\\sum_{k=1}^K \\pi_k N(x|u_k(w;\\beta),\\sigma_k(w;\\beta)}$$  \n",
    "    \n",
    "<br>  \n",
    "\n",
    "所以具体的变分下界为（ELBO）:   \n",
    "  \n",
    "  $L_{ELBO}=E_{q_{mvf}\\;(x,w,z|y)\\;}[log \\frac{P_{\\beta,\\theta}(y,x,w,z)}{q_{mvf}\\;(x,w,z|y)}]$ \n",
    "  \n",
    "  $\\qquad\\;\\;=E_{q_{\\phi x}(x|y)q_{\\phi w}(w|y)p_{\\beta}(z|x,w)\\;}[log \\frac{P_{\\theta}(y|x)P_{\\beta}(x|w,z)P(w)P(z)}{q_{\\phi x}(x|y)q_{\\phi w}(w|y)p_{\\beta}(z|x,w)}] $  \n",
    "    \n",
    "  $\\qquad\\;\\;=E_{q_{\\phi x}(x|y)q_{\\phi w}(w|y)p_{\\beta}(z|x,w)\\;}[log P_{\\theta}(y|x) + log P_{\\beta}(x|w,z)+log P(w)+log P(z)-log q_{\\phi x}(x|y)-log q_{\\phi w}(w|y)- logp_{\\beta}(z|x,w)]$   \n",
    "    \n",
    "   $\\qquad\\;\\;=E_{q_{\\phi x}(x|y)q_{\\phi w}\\;(w|y)p_{\\beta}(z|x,w)\\;}[log P_{\\theta}(y|x) -log \\frac{q_{\\phi x}\\;(x|y)}{P_{\\beta}(x|w,z)}-log\\frac{q_{\\phi w}\\;(w|y)}{ P(w)}-log \\frac{p_{\\beta}(z|x,w)}{P(z)}]$  \n",
    "     \n",
    "   $\\quad\\;\\;=E_{q_{\\phi x}(x|y)\\;}[log P_{\\theta}(y|x)] -E_{q_{\\phi w}\\;(w|y)p_{\\beta}(z|x,w)\\;}[KL(q_{\\phi x}\\;(x|y)||P_{\\beta}(x|w,z)]-KL(q_{\\phi w}\\;(w|y)|| P(w))-E_{q_{\\phi x}(x|y)q_{\\phi w}\\;(w|y)\\;}[KL(p_{\\beta}(z|x,w) || P(z))]$  \n",
    "     \n",
    " 这些就是最终的变分下界（ELBO）   \n",
    "   \n",
    " 各项分别为：重构项、条件先验项、w-先验项、z-先验项  \n",
    "   \n",
    "<br>  \n",
    "\n",
    "### 条件先验项  \n",
    "  \n",
    "  重建项可以通过从q(x|y)提取蒙特卡罗样本来估计，其中梯度可以用标准的重参数化技巧反向传播。w-先验项可以解析计算。  \n",
    "    \n",
    " **而条件先验项的计算方式为：**  \n",
    "   \n",
    "$$E_{q_{\\phi w}\\;(w|y)p_{\\beta}(z|x,w)\\;}[KL(q_{\\phi x}\\;(x|y)||P_{\\beta}(x|w,z)] \\approx \\frac{1}{M}\\sum_{j=1}^M \\sum_{k=1}^K p_{\\beta}(z_k=1|x^j,w^j) KL(q_{\\phi x}\\;(x|y)||P_{\\beta}(x|w^j,z_k=1)$$   \n",
    "  \n",
    "即 $q_{\\phi w}\\;(w|y)$使用蒙特卡洛采样，其梯度可以用标准的重参数化技巧反向传播。而$p_{\\beta}(z|x,w)$可以通过计算有z指，来计算其期望，并使用标准BP计算梯度。  \n",
    "\n",
    "<br>  \n",
    "\n",
    "### 离散潜变量Z的KL代价 （分析z-先验项所造成的影响）  \n",
    "  \n",
    " $-E_{q_{\\phi x}(x|y)q_{\\phi w}\\;(w|y)\\;}[KL(p_{\\beta}(z|x,w) || P(z))]$  \n",
    " \n",
    " 用来衡量不同集群重叠程度的度量。（即一个样本如果来自每个类别的概率相同，说明所有类别完全重叠。最理想的情况来自一个类别的开率为1，来自其他类别的概率为0。）。所以这个z-先验项越小，说明重叠程度越小，越接近0说明重叠程度越大。\n",
    " \n",
    " <br> \n",
    "  \n",
    "ELBO中最重要的项是z-先验项。（z-后验分布P(z|x,w)通过询问现在正态x与通过w生成的每个聚类的组件正态分布位置的距离（由$z_k=1$结果为x与w生成k组件的距离），直接从x和w的值计算聚类分配概率。因此，这个z-先验项可以通过同时操纵函数输出的簇w和编码点x的位置来减小z-后验和均匀分布z之间的KL散度。） 直观地说，它会通过最大化簇之间的重叠并将平均值移近来尝试合并簇。**也就是说这个z-先验项具有反聚类效果**  。 这种现象和标准VAE中存在的过渡正则化项很相似。  \n",
    "  \n",
    "<br>  \n",
    "\n",
    "### 过渡正则化项（over-regularisation）  \n",
    "\n",
    "**V-AE文献中多次描述了正则化项对V-AE训练的过度影响。由于受先验信息的强烈影响，所得到的潜在表示往往过于简化，不能很好地反映数据的底层结构。** 到目前为止，有两种主要的方法来克服这种影响：一种解决方案是在训练期间退火KL项，允许重建项训练自动编码器网络，然后缓慢地纳入KL项的正则化。 另一种主要方法是通过设置一个临界值来修改目标函数，当KL项低于某个阈值时，该值会消除KL项的影响（Kingma等人，2016）。  \n",
    "\n",
    " 正如我们在下面的实验部分所展示的，这种过度正则化的问题在GMVAE簇的分配中也很普遍，并且表现在大的退化簇中。我们证明了Kingma等人提出的第二种方法。（2016）确实缓解了这种合并现象，找到解决过度正则化问题的方法仍然是一个具有挑战性的开放性问题。  \n",
    "   \n",
    "  \n",
    "<br>  \n",
    "\n",
    "### 人工数据  \n",
    "  \n",
    "  我们通过绘制z先验项的大小来量化聚类性能。在训练期间。这个数量可以被认为是衡量不同集群重叠程度的度量。因为我们的目标是在潜在空间中实现有意义的聚类，所以我们希望随着模型学习独立的簇，这个数量会下降  \n",
    "    \n",
    "$$-E_{q_{\\phi x}(x|y)q_{\\phi w}\\;(w|y)\\;}[KL(p_{\\beta}(z|x,w) || P(z))]$$  \n",
    "    \n",
    "  然而，根据经验，我们发现情况并非如此。我们的模型收敛到的潜在表示将所有类合并到同一个大集群中，而不是表示关于不同集群的信息，如图所示。因此，每个数据点都有可能属于任何一个簇，这使得我们的潜在表示对于类结构来说完全不具信息性。  \n",
    "    \n",
    " **我们认为这种现象可以解释为z-先验项过度正则化的结果。考虑到这个数量是由下界中KL项的优化所驱动的，它达到了其最大可能值0，而不是通过训练来减少，以确保关于类的信息的编码。** $\\color{red}{\\text{（根据VAE的推导，可以知道我们这个下界是没有问题，z-先验项也没有问题。但是这个z-先验项，只是用来保持样本的类别的编码信息。}}$  \n",
    " $\\color{red}{\\text{不能让它过度正则化，过度正则化，达到完美的0，达到完美的反聚类效果（类别完全重合），聚类性能反而变差了。）}}$ 我们怀疑先验在初始训练阶段的影响太大，导致模型参数陷入一个很差的局部最优值，以后重建项很难将其赶走  \n",
    "    \n",
    "  这个观察家在概念上与遇到不规则值的过度正则化问题非常相似，因此我们假设应用类似的启发式方法应该有助于缓解问题。我们在图2f中显示，**通过使用前面提到的对Kingma等人提出的下界的修改。（2016年），我们可以避免z-先验导致的过度正则化。这是通过将z先验的代价保持在一个常量值λ，直到它超过该阈值。形式上，修改后的z-先验项写为**  \n",
    "    \n",
    " $$ = -max(\\lambda, E_{q_{\\phi x}(x|y)q_{\\phi w}\\;(w|y)\\;}[KL(p_{\\beta}(z|x,w) || P(z))])$$ \n",
    "    \n",
    "  **这种修改抑制了z-prior合并所有簇的初始效果，从而允许它们分散开来，直到z-prior代价足够高。在这一点上，它的影响被大大降低，并且主要局限于合并足够重叠的单个集群。** 在无花果中可以清楚地看到。2e和2f。前者显示了在考虑z-先验成本之前的集群，因此集群能够扩展。一旦z-先验被激活，非常接近的簇将被合并，如图所示。  \n",
    "    \n",
    "  最后，为了说明使用神经网络转换分布的好处，我们比较了我们的模型（图2c）和数据空间中规则GMM（图2c）观测到的密度。如图所示，GMVAE允许比常规GMM更丰富、更准确的表示，因此在建模非高斯数据方面更为成功。  \n",
    "    \n",
    "  图2：合成数据集的可视化：（a）数据在二维数据空间上以5种模式分布。（b） GMV-AE学习了密度模型，它可以使用数据空间中非高斯分布的混合来建模数据。（c） 由于高斯假设的限制性，GMM不能很好地表示数据。（d） 然而，GMV-AE存在着过度正则化的问题，当观察潜在空间时，会导致很差的极小值。（e） 使用对ELBO的修改（Kingma等人，2016年）允许集群扩展。（f） 当模型收敛时，z-先验项被激活，并在最后阶段通过合并过多的簇来对簇进行正则化。  \n",
    "    \n",
    "  图3:z-先验项图：（a）在没有信息约束的情况下，GMV-AE由于收敛到一个差的最优值而遭受过度正则化的困扰，该最优解将所有簇合并在一起以避免KL成本。（b） 在达到阈值（虚线）之前，可以关闭z-先验项的渐变，以避免簇被拉到一起（有关详细信息，请参见文本）。当达到阈值时，簇被充分分离。此时，来自z-先验项的激活梯度只会将非常重叠的簇合并在一起。即使在激活了它的梯度之后，z-先验的值也会继续下降，因为它被导致有意义的簇和更好的优化的其他项所过度支持  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
