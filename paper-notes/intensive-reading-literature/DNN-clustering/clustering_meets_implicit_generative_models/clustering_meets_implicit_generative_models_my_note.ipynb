{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聚类满足隐式生成模型  \n",
    "  \n",
    "### 摘要  \n",
    "  \n",
    "  聚类是无监督学习的基石，它可以被认为是分离数据背后的多重生成机制。本文介绍了一个算法框架来训练隐式生成模型的混合体，这个模型是我们为变分自编码器特别设计的。   \n",
    "     \n",
    " \n",
    " **思想：基于一组额外的鉴别器，本文提出了一种竞争过程，其中每个模型只需要近似数据分布的一部分，从中可以产生真实的样本。聚类解释自然地产生于模型之间训练样本的划分。实验表明，我们的方法合理地分割了训练分布，提高了样本的质量**  \n",
    "   \n",
    " <br> \n",
    "  \n",
    "### 聚类思想   \n",
    "  \n",
    "GAN和VAE这两种方法的目的都是尽量减少真实数据分布与模型学习数据之间的差异。模型分布通常用神经网络参数化，该神经网络将随机向量转换为训练数据（如图像）空间中的样本。  \n",
    "  \n",
    "**VAE会产生模糊的样本。**     \n",
    "  \n",
    "**GAN训练是有代价的。GAN难训练，并遭受模式崩溃的问题。解决这个问题的一种方法是顺序或并行训练多个生成模型。**   \n",
    "  \n",
    "**思想框架：  \n",
    "构建一种通用的方法来并行训练多个生成模型（每个模型采用一个VAE），这些模型集中在训练分布的不同部分（不同模型训练分布的不同分布）**    \n",
    "  \n",
    "**结果：尽管每个VAE可能会在某些模式下坍塌，但是整个模型的混合任然能够近似数据的分布。**   \n",
    "\n",
    "<br>\n",
    "  \n",
    "### 算法描述    \n",
    "  \n",
    " 网络结构：  \n",
    " \n",
    " ![](img/1.PNG)  \n",
    "   \n",
    " <br>  \n",
    " \n",
    " 数据定义：      \n",
    "   \n",
    " 单个组件模型分布：$P_{gj} \\qquad\\qquad$    \n",
    "   \n",
    " 单个组件真实分布：$P_{dj} \\qquad\\qquad$     \n",
    " \n",
    " $\\color{red}{\\text{这两个分布组成了变分自编码器组件}\\quad \\underset{p_{gj}}{min}D_f(P_{gj}||P_{dj})}$ \n",
    "\n",
    "   \n",
    " 数据模型分布： $P_{model}=\\sum_j^k \\alpha_jP_{gj} $    \n",
    " \n",
    " 数据真实分布: $P_d$       \n",
    "\n",
    "设个一个识别器D:$D_{gj}(x_i) \\qquad$用于识别输入样本是真实样本还是生成样本。  \n",
    "\n",
    "<br>\n",
    " \n",
    " 分配函数：$c_j(x_i) \\qquad 0 \\;\\;or\\;\\; 1 \\qquad$0代表样本$x_i$不属于类别j, 1代表样本$x_i$不属于类别j。\n",
    " \n",
    " <br>\n",
    " \n",
    " 则每个样本的微分布为：$d P_{gj}(x_i) \\approx d P_{d}(x_i)\\frac{1-D_{gj}(x_i)}{D_{gj}(x_i)}$  \n",
    "   \n",
    "$\\color{red}{\\text{这个公式中样本}x_i\\text{都是真实样本，且这里的真实分布}P_{d}\\text{也是我们事先假设的带参数的真实分布，不是来自每一轮迭代中每个组件VAE中求出的}P_{dj}}$   \n",
    " \n",
    "<br>\n",
    " \n",
    " 我们总的目标函数：$\\underset{P_{gj}}{min}\\sum_j \\alpha_jD_f(P_{gj}||P_{dj})$  \n",
    "   \n",
    "<br>  \n",
    "\n",
    "<br>  \n",
    "\n",
    "\n",
    "#### 算法流程  \n",
    "  \n",
    "1、初始化K个变分自编码器，样本分配$C_j^0$  \n",
    "  \n",
    "2、循环T次  \n",
    "  \n",
    "$\\qquad\\qquad$3、用上次迭代样本分配结果$C_j^{t-1}$平行的这K个变分自编码器$\\underset{P_{gj}}{min}D_f(P_{gj}||P_{dj})$  \n",
    "  \n",
    "$\\qquad\\qquad$4、训练各个VAE所对应的变分自编码器$D_{gj}$    \n",
    "  \n",
    "$\\qquad\\qquad$5、用极大似然估计，计算 $d P_{gj}(x_i) \\approx d P_{d}(x_i)\\frac{1-D_{gj}(x_i)}{D_{gj}(x_i)} \\quad$ （这里$d P_{d}$是带参的真实分布，不是来自自编码器）   \n",
    "  \n",
    "$\\qquad\\qquad$5、根据 $C_{j}^{t}(x_i)=1 \\quad  if\\quad j=arg \\; max _l\\;d P_{gl}^t(x_i) \\qquad C_{j}^{t}(x_i)=0 $ 更新样本分配。    \n",
    "\n",
    "<br>  \n",
    "\n",
    "<br> \n",
    "\n",
    "\n",
    "**算法的核心思想类似于K-means算法。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
