{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用于聚类分析的无监督深度嵌入\n",
    "#### 摘要\n",
    "聚类是许多数据驱动的应用程序域的核心，并且在距离函数和分组算法方面进行了广泛的研究。 相对较少的工作集中在学习表示的聚类上。 在本文中，我们提出了深度嵌入式群集（DEC），这是一种使用深度神经网络同时学习功能表示和群集分配的方法。  DEC学习从数据空间到低维特征空间的映射，在该映射中迭代地优化聚类目标。 我们对图像和文本语料库的实验评估表明，与最新方法相比，已有显着改进\n",
    "\n",
    "#### 引言\n",
    "聚类是一种基本的数据分析和可视化工具，已从不同的角度在无人监督的机器学习中进行了广泛的研究：定义了什么？ 什么是正确的距离指标？ 如何有效地将实例分组到群集中？ 如何验证集群？\n",
    "   等等。 文献中已经探索了许多不同的距离函数和嵌入方法。 相对较少的工作集中在执行聚类的特征空间的无监督学习上。\n",
    "   距离或相异性的概念对于数据集群算法至关重要。 距离又取决于在特征空间中表示数据。 例如，k均值聚类算法（MacQueen等，1967）使用给定特征空间中点之间的欧几里得距离，对于图像而言，该距离可能是原始像素或梯度方向直方图。 通常将特征空间的选择留作最终用户确定的特定于应用程序的细节。 但是很明显，特征空间的选择至关重要。 对于除最简单的图像数据集以外的所有图像，在原始像素上用欧几里得距离进行聚类完全无效。 在本文中，我们将重新审视聚类分析并提出一个问题：我们是否可以使用数据驱动的方法来共同解决特征空间和聚类成员关系？\n",
    "   我们从计算机深度学习的最新工作中汲取了灵感（Krizhevsky等人，2012; Girshick等人，2014; Zeiler＆Fergus，2014; Long等人，2014），其中基准任务的明显收益来自 学习更好的功能。 但是，这些改进是通过有监督的学习获得的，而我们的目标是无监督的数据聚类。 为此，我们定义了从数据空间X到低维特征空间Z的参数化非线性映射，在此我们优化了聚类目标。 与先前的工作是在数据空间或浅线性嵌入式空间上进行操作的工作不同，我们通过对聚类目标进行反向传播使用随机梯度下降（SGD）来学习由深度神经网络参数化的映射。 我们将此群集算法称为“深度嵌入式群集”或DEC。\n",
    "   优化DEC具有挑战性。 我们要同时解决群集分配和基本功能表示。 但是，与监督学习不同，我们无法使用标记数据来训练我们的深度网络。\n",
    "   相反，我们建议使用从当前软集群分配派生的辅助目标分布来迭代地优化集群。 该过程逐渐改善了聚类以及特征表示。\n",
    "   我们的实验表明，在图像和文本数据集的准确性和运行时间方面，都比最新的聚类方法有了显着改进。 我们在MNIST（LeCun等，1998），STL（Coates等，arXiv：1511.06335v2 [cs.LG] 2016年5月24日无监督深度嵌入进行聚类分析2011）和REUTERS（Lewis等，  （2004年），将其与标准和最新的聚类方法进行了比较（Nie等，2011； Yang等，2010）。 此外，我们的实验表明，与最新方法相比，DEC对超参数的选择不那么敏感。\n",
    "   这种鲁棒性是我们的聚类算法的重要属性，因为当将其应用于实际数据时，监督不适用于超参数交叉验证。\n",
    "   我们的贡献是：（a）联合优化深埋层和聚类；  （b）通过软分配进行新颖的迭代改进；  （c）就聚类准确性和速度而言，最新的聚类结果。 我们基于DEC的Caffe（Jia等人，2014）的实现可从https://github.com/piiswrong/dec获得。  \n",
    "   \n",
    "     \n",
    " ##### 相关工作\n",
    " 聚类已经在机器学习中根据特征选择（Boutsidis等，2009; Liu＆Yu，2005; Alelyani等，2013），距离函数（Xing等，2002; Xiang等）进行了广泛的研究。  （2008年），分组方法（Mac Queen等，1967年； Von Luxburg，2007年； Li等，2004年）和聚类验证（Halkidi等，2001年）。 空间不允许进行全面的文献研究，我们将读者引向（Aggarwal＆Reddy，2013）进行调查。  \n",
    " 已经提出了k均值的几种变体来解决高维输入空间的问题。  De la Torre＆Kanade（2006）;  Ye等。  （2008年）执行联合维数减少和聚类，方法是首先将数据与k均值聚类，然后将数据投影到较低维，其中聚类间方差最大。\n",
    "   在EM样式的迭代中重复此过程，直到收敛为止。 但是，此框架仅限于线性嵌入。 我们的方法采用深度神经网络来执行非线性嵌入，这对于更复杂的数据是必需的。\n",
    "   光谱聚类及其变体最近获得了流行（Von Luxburg，2007）。 它们允许更灵活的距离度量，并且通常比k均值表现更好。  Yang等人已经探索了将光谱聚类和嵌入相结合的方法。  （2010）；  Nie等。  （2011）。 田等。  （2014年）提出了一种基于频谱聚类的算法，但用深度自动编码器代替了特征值分解，从而提高了性能，但进一步增加了内存消耗。\n",
    "   大多数谱聚类算法需要计算完整的图拉普拉斯矩阵，因此在数据点数量上具有二次或超级二次复杂度。 这意味着他们需要具有大内存的专用机器来存储大于几万个点的任何数据集。 为了将频谱聚类扩展到大型数据集，发明了近似算法来权衡性能与速度（Yan等，2009）。 但是，我们的方法在数据点数量上是线性的，并且可以适当地扩展到大型数据集。\n",
    "   在数据分布和嵌入式分布之间最小化Kullback-Leibler（KL）差异已用于数据可视化和降维（van der Maaten＆Hinton，2008）。 例如，T-SNE是该学校的非参数算法，并且t-SNE的参数变体（van der Maaten，2009）使用深度神经网络对嵌入进行参数化。  t-SNE的复杂度为O（n2），其中n是数据点的数量，但可以近似为O（n log n）（van Der Maaten，2014）。\n",
    "   我们从参数化t-SNE中获得启发。 我们没有最小化KL散度以产生忠实于原始数据空间中距离的嵌入，而是定义了基于质心的概率分布并将其KL散度最小化为辅助目标分布以同时改善聚类分配和特征表示 观点。 基于质心的方法还具有将复杂度降低到O（nk）的好处，其中k是质心的数量。\n",
    "     \n",
    "       \n",
    "##### 深度嵌入式集群\n",
    "考虑将一组n个点{xi∈X} ni = 1聚类为k个聚类的问题，每个聚类由质心µj表示，j = 1。  。  。  ，k。 我们建议先使用非线性映射fθ：X→Z变换数据，而不是直接在数据空间X中进行聚类，其中θ是可学习的参数，Z是潜在特征空间。  Z的维数通常比X小得多，以避免“维数的诅咒”（Bellman，1961）。 为了参数化fθ，深层神经网络（DNN）由于其理论功能近似特性（Hornik，1991）和已证明的特征学习能力（Bengio et al。，2013）而成为自然选择。所提出的算法（DEC）通过在特征空间Z中同时学习一组k个聚类中心{μj∈Z} kj = 1以及将数据点映射到Z的DNN参数θ来对数据进行聚类。DEC具有两个阶段 ：（1）使用深层自动编码器初始化参数（Vincent等，2010）和（2）参数优化（即，聚类），其中我们在计算辅助目标分布和最小化Kullback-Leibler（KL）之间进行迭代 分歧。 我们从描述阶段（2）参数优化/聚类开始，假设初始估计为θ并且{μj} kj = 1。  \n",
    "    \n",
    "#### KL聚类\n",
    "步骤：  \n",
    "给定非线性映射fθ的初始估计和初始聚类质心{μj} kj = 1，我们建议使用一种在两步之间交替的无监督算法来证明聚类。 第一步，我们在嵌入点和聚类质心之间进行软分配。 在第二步中，我们通过使用辅助目标分布从当前的高可信度分配中学习，从而更新深度映射fθ并优化聚类质心。 重复该过程，直到满足收敛标准为止。  \n",
    "  \n",
    "1、软分配  \n",
    "根据van der Maaten＆Hinton（2008），我们使用Student的t分布作为核来衡量嵌入点zi和质心µj之间的相似性：其中zi =fθ（xi）∈Z对应于嵌入后的xi∈X，其中α是学生t分布的自由度，而qij可解释为将样本i分配给聚类j的概率（即软分配）。 由于我们无法在无监督的环境中对验证集上的α进行交叉验证，并且得知它是多余的（van der Maaten，2009），因此对于所有实验，我们让α= 1。  \n",
    "![](../../extensive-reading-literature/img/DL0091.PNG)  \n",
    "  \n",
    "qij 为把样本i分配给聚类j的概率   \n",
    "\n",
    "2、KL分流最小化  \n",
    "我们建议在辅助目标分布的帮助下，通过从群集的高可信度分配中学习来迭代地优化群集。 具体来说，通过将软分配与目标分布匹配来训练我们的模型。 为此，我们将目标定义为软分配qi和辅助分布pi之间的KL散度损失，如下所示  \n",
    "![](../../extensive-reading-literature/img/DL0092.PNG)  \n",
    "目标分布P的选择对于DEC的性能至关重要。 天真的方法是将每个pi设置为高于置信度阈值的数据点的delta分布（到最接近的质心），而忽略其余的点。 但是，由于qi是软分配，因此使用较软的概率目标更加自然和灵活。 具体来说，我们希望目标分布具有以下属性：（1）加强预测（即提高簇纯度），（2）更加注重以高置信度分配的数据点，以及（3）归一化每个损失的贡献 重心以防止大型簇扭曲隐藏的特征空间。在我们的实验中，我们通过先将qi升至第二次幂，然后按每个簇的频率进行归一化来计算pi\n",
    "![](../../extensive-reading-literature/img/DL0093.PNG)  \n",
    "我们的培训策略可以看作是自我培训的一种形式（Nigam＆Ghani，2000）。 与自训练中一样，我们采用初始分类器和未标记的数据集，然后使用分类器标记数据集，以便对其自身的高置信度预测进行训练。 确实，在实验中，我们观察到DEC通过学习高置信度预测来提高每次迭代中的初始估计，从而有助于改善低置信度预测。  \n",
    "  \n",
    "    \n",
    "<br>  \n",
    "<br>  \n",
    "\n",
    "\n",
    "#### 优化\n",
    "我们使用带有动量的随机梯度下降（SGD）联合优化聚类中心{µj}和DNN参数θ。 关于每个数据点zi和每个簇质心µj的特征空间嵌入的L梯度计算如下：\n",
    "  ![](../../extensive-reading-literature/img/DL0094.PNG) \n",
    "  然后将梯度∂L/∂zi向下传递到DNN，并在标准反向传播中用于计算DNN的参数梯度∂L/∂θ。 出于显示集群分配的目的，当少于两次的点间更改集群分配的点数少于总点数时，我们停止执行该过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
