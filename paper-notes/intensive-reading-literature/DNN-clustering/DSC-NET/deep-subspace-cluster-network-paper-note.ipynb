{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 深度子空间聚类网络  \n",
    "  \n",
    "### 摘要  \n",
    "  \n",
    "  作者提出了一种新的用于无监督子空间聚类的深层神经网络结构。该体系结构建立在深度自动编码器的基础上，它将输入数据非线性映射到一个潜在空间。我们的核心思想是在编码器和解码器之间引入一个新的自表达层，以模拟在传统子空间聚类中被证明是有效的“自表达”特性。作为可微的，我们新的自我表达层提供了一种简单而有效的方法，通过标准的反向传播过程来学习所有数据点之间的成对亲和力。由于非线性，我们基于神经网络的方法能够对具有复杂（通常是非线性）结构的数据点进行聚类。我们进一步提出了预训练和微调策略，使我们能够有效地学习子空间聚类网络的参数。实验表明，我们的方法明显优于目前最先进的无监督子空间聚类技术。  \n",
    "    \n",
    "### 引言  \n",
    "  \n",
    "  在这篇论文中，我们讨论了子空间聚类问题，这是一个无监督学习的子领域，其目的是以无监督的方式对从低维子空间集合中提取的数据点进行聚类。子空间聚类在计算机视觉中有着广泛的应用，如图像分割、运动分割和图像聚类，已经成为一个重要的问题。例如，在朗伯反射下，一个物体在固定姿势和不同光照条件下获得的面部图像位于接近9维的低维子空间中。因此，我们可以利用子空间聚类的方法将多个被摄体的图像按其各自的主题进行分组。  \n",
    "    \n",
    "  <br>  \n",
    "    \n",
    "  最近关于子空间聚类的研究集中于线性子空间的聚类。然而，在实际应用中，数据并不一定符合线性子空间模型。例如，在面部图像聚类的示例中，反射率通常是非朗伯的，并且对象的姿势经常变化。在这些条件下，一个被摄体的人脸图像往往位于一个非线性子空间（或子流形）中。一些著作提出利用核技巧来解决非线性子空间的情况。然而，不同核类型的选择在很大程度上是经验的，并且没有明确的理由相信对应于预定义核的隐式特征空间确实适合于子空间聚类。\n",
    "  \n",
    "  <br>  \n",
    "    \n",
    "  相比之下，在本文中，我们引入了一种新的深度神经网络结构来学习（以无监督的方式）数据明确的非线性映射，该映射很好地适应子空间聚类。为此，我们在深度自动编码器的基础上建立了深度子空间聚类网络（DSC网），通过一系列的编码器层将数据点非线性映射到潜在空间。我们的主要贡献是在编码器和解码器之间的连接处引入一个新的自我表达层，即一个没有偏差和非线性激活的完全连接层。该层对从子空间的并集中提取的数据的“自表达性”属性进行编码，即每个数据样本可以表示为同一子空间中其他样本的线性组合。就我们所知，我们的方法是第一次尝试直接学习一个神经网络中所有数据点之间的亲和力（通过组合系数）。此外，我们提出了有效的预训练和微调策略，以无监督的方式在有限的数据量下学习DSC网络的参数。  \n",
    "    \n",
    "  <br>  \n",
    "    \n",
    "  我们使用扩展的Y ale B和ORL数据集对我们的方法进行了广泛的评估，并使用COIL20和COIL100对一般的对象聚类进行了评估。我们的实验表明，我们的DSC网络明显优于最先进的子空间聚类方法。  \n",
    "    \n",
    "  <br>  \n",
    "    \n",
    "### 相关工作  \n",
    "  \n",
    "#### 子空间聚类  \n",
    "  \n",
    "  多年来，人们发展了许多线性子空间聚类方法。一般来说，这些方法包括两个步骤：第一步也是最关键的一步是估计每对数据点的亲和力，以形成亲和力矩阵；第二步使用该亲和力矩阵应用归一化割集或谱聚类。由此产生的方法可以大致分为三类：因子分解方法、基于高阶模型的方法和基于自表达的方法。\n",
    "本质上，因子分解方法通过分解数据矩阵来建立亲和力矩阵，基于高阶模型的方法则利用局部子空间模型拟合的残差来估计亲和力。近年来，基于自表达的方法已成为最流行的方法，它将数据点表示为同一子空间中其他点的线性组合。这些方法利用组合系数矩阵建立亲和力矩阵。与因子分解技术相比，当依赖正则化项来解释数据损坏时，基于自表达的方法通常对噪声和异常值更具鲁棒性。与基于高阶模型的方法相比，它们还具有优势，即考虑所有数据点之间的连接，而不是利用局部模型，后者通常是次优的。为了处理数据点不完全位于线性子空间的并集中，而是在非线性子空间中的情况，一些工作提出用预定义的核矩阵（例如多项式核和高斯RBF核）来代替数据矩阵的内积。然而，没有明确的理由说明为什么这些核应该对应于非常适合于子空间聚类的特征空间。相比之下，在这里，我们提出的方法更明确说明了这一点。  \n",
    "  \n",
    "<br>  \n",
    "  \n",
    "#### 自编码器  \n",
    "  \n",
    "  自编码器能够非线性的把数据点转换到子空间中。当这个潜在空间的维数低于原始空间，这可以看作是一种非线性PCA。自动编码器通常由编码器和解码器组成，以定义数据重建成本。随着深度学习的成功，深度（或堆叠）AEs已成为无监督学习的热门。例如，deep AEs已经被证明对降维和图像去噪有用。最近，deep-AEs也被用来初始化无监督聚类的深度嵌入网络。deep-AEs的卷积形式也被用于提取层次特征和初始化卷积神经网络（CNNs）  \n",
    "    \n",
    "  <br>  \n",
    "   \n",
    " 将深度学习与子空间聚类相结合的研究很少。据我们所知，唯一的例外是[36]，它首先从图像中提取SIFT或HOG特征，并将它们输入给一个完全连接的深度自动编码器，该编码器具有稀疏子空间聚类（SSC）。通过对学习到的自动编码器特征应用k-means或SSC得到最终的聚类结果。从本质上讲，[36]可以看作是一种使用深度自动编码特征的基于k-means或SSC的子空间聚类方法。我们的方法与[36]的显著不同在于，我们的网络被设计成直接学习亲和力，这要归功于我们新的自我表达层。  \n",
    "   \n",
    " <br>  \n",
    "   \n",
    " ### 深度子空间聚类网络（DSC-Nets）  \n",
    "     \n",
    "       \n",
    "   我们的深层子空间聚类网络利用深度自动编码器和自表达特性。在介绍我们的网络之前，我们首先详细讨论这个属性。  \n",
    "     \n",
    "  ![](img/1.PNG)   \n",
    "     \n",
    "   <br>  \n",
    "     \n",
    "#### Self-Expressiveness  \n",
    "  \n",
    "   给定数据点：来自多个线性子空间$\\{S_i\\}\\quad i=1,2,……，k$ 的数据点$\\{x_i\\}\\quad i=1,2,……，N$ 。  \n",
    "     \n",
    "   可以将子空间中的一个点表示为同一子空间中其他点的线性组合。这种性质被称为Self-Expressiveness 。  \n",
    "     \n",
    "   把所有数据点按列排列成一个数据矩阵。那么  Self-Expressiveness  性质，可以表示成一个简单的等式。$X=XC$  \n",
    "     \n",
    "   $\n",
    "        \\begin{pmatrix}\n",
    "        x_1^1 & x_1^2 &  \\cdots & x_1^n \\\\\n",
    "        \\vdots  & \\vdots & \\ddots & \\vdots \\\\\n",
    "        x_n^1 &  x_n^2 & \\cdots &  x_n^n \\\\\n",
    "        \\end{pmatrix}     \n",
    "$  =     $\n",
    "        \\begin{pmatrix}\n",
    "        x_1^1 & x_1^2 &  \\cdots & x_1^n \\\\\n",
    "        \\vdots  & \\vdots & \\ddots & \\vdots \\\\\n",
    "        x_n^1 &  x_n^2 & \\cdots &  x_n^n \\\\\n",
    "        \\end{pmatrix}     \n",
    "$ $\\times$  $\n",
    "        \\begin{pmatrix}\n",
    "        c_1^1 & c_1^2 &  \\cdots & c_1^n \\\\\n",
    "        \\vdots  & \\vdots & \\ddots & \\vdots \\\\\n",
    "        c_n^1 &  c_n^2 & \\cdots &  c_n^n \\\\\n",
    "        \\end{pmatrix}     \n",
    "$    \n",
    "    \n",
    "由上式可得（X=XC ）（C式线性相关的系数）：Self-Expressiveness性质， 即子空间中的一个点表示为同一子空间中其他点的线性组合。   \n",
    "  \n",
    "<br>  \n",
    "  \n",
    "文献[15]指出，在子空间相互独立的假设下，通过最小化C的某些范数，保证C具有块对角结构（直到一定的置换），即在同一子空间中，cij$\\neq$0 当且仅当点Xi和点xj 位于同一个子空间。  所以我们可以利用矩阵C来构造光谱聚类的亲和矩阵。  \n",
    "  \n",
    "数学上，这个问题可以概念化这个公式的优化问题：$\\underset{C}{min} \\;\\; ||c||_p ，X=XC ,(diag(C)=0)$    \n",
    "  \n",
    "C上的可选对角线约束阻止稀疏诱导范数的平凡解，例如'1范数。  \n",
    "  \n",
    "文献中提出了C的各种范数，如稀疏子空间聚类（SSC）中的“1范数”[9,10]、低秩表示（LRR）[24,23]和低秩子空间聚类（LRSC）[11,43]中的核范数，以及最小二乘回归（LSR）[26]和有效稠密子空间聚类（EDSC）[15]中的Frobenius范数。 \n",
    "        \n",
    "  \n",
    "为了解释数据的损坏，上诉的优化公式中约束项常常被放宽为一个正则化项。即：  \n",
    "  \n",
    "$\\underset{C}{min} \\;\\; ||c||_p +\\frac{C}{2}||X-XC||_F^2  \\quad s.t.(diag(C)=0)$   \n",
    "  \n",
    "不幸的是，Self-Expressiveness性质只适用于线性子空间。  \n",
    "  \n",
    "虽然基于核的方法[34，35，51，47]的目标是解决非线性的情况，但还不清楚预定义的核是否会产生适合子空间聚类的隐式特征空间。  \n",
    "\n",
    "我们的目标是学习一个显式映射，使子空间更可分离。  \n",
    "  \n",
    "<br>  \n",
    "  \n",
    "#### 深度自动编码器中Self-Expressiveness层  \n",
    "  \n",
    "  我们的目标是训练一个深度的自动编码器，如上图所示。因为它的潜在表示非常适合于子空间聚类。为此，我们引入了一个新的层来编码Self-Expressiveness的概念。  \n",
    "    \n",
    "  具体而言，让Θ表示自动编码器参数，其可分解为编码器参数$Θ_e$和解码器参数$Θ_d$。此外，让$Z_{Θ_e}$注释编码器的输出，即数据矩阵X的潜在表示。  \n",
    "    \n",
    "  为了编码Self-Expressiveness，我们引入一个新的损失函数。  \n",
    "  \n",
    "  $L(\\theta,C)=\\frac{1}{2}||x-\\hat{X_\\theta}||_F^2 + \\lambda_1||C||_p  + \\frac{\\lambda_2}{2} ||Z_{\\theta_e}-Z_{\\theta_e}C||_F^2 \\quad s.t.(diag(C)=0)$  \n",
    "    \n",
    " 式中，ˆXΘ表示由自动编码器重建的数据。  \n",
    "   \n",
    " 为了最小化（3），我们建议利用这样一个事实：如下文所述，C可以被视为附加网络层的参数，这使得我们可以使用反向传播来联合求解Θ和C。  \n",
    "   \n",
    " 具体来说，考虑到Self-Expressiveness性质：$||Z_{\\theta_e}-Z_{\\theta_e}C||_F^2$  ，由于每个数据点zi（在潜在空间中）是由其他点{zj}j=1，···，N（可选地，j$\\neq$i）与加权cij的加权线性组合来近似，因此这种线性运算正好对应于一组没有非线性激活的线性神经元。  \n",
    "   \n",
    " 因此，如果我们把每个zi作为网络中的一个节点，我们就可以用一个完全连通的线性层来表示自表达项，我们称之为Self-Expressiveness层。   \n",
    "   \n",
    " 自表达层的权重对应于新损失函数中的矩阵C 。矩阵C进一步用来构造所有数据点之间的亲和力。\n",
    "\n",
    "因此，我们的自我表达层本质上允许我们通过网络直接学习亲和矩阵。 此外，最小化$||C||_p$简单地转化为向自表达层的权重添加正则化器。  \n",
    "  \n",
    "在这项工作中，我们考虑了C上的两种正则化：（i）1范数，产生由DSC-Net-L1表示的网络；（ii）“2范数”，得到由DSC-Net-L2表示的网络。  \n",
    "  \n",
    "为了符号的一致性，让我们将自我表现层的参数（只是C的元素）表示为Θs。如图2所示，我们将网络的解码器部分的输入作为变换后的潜在表示ZΘeΘs。这让我们将损失函数重写为  \n",
    "   \n",
    "   $L(\\theta)=\\frac{1}{2}||x-\\hat{X_\\theta}||_F^2 + \\lambda_1||\\theta_s||_p  + \\frac{\\lambda_2}{2} ||Z_{\\theta_e}-Z_{\\theta_e}\\theta_s||_F^2 \\quad s.t.(diag(\\theta_s)=0)$   \n",
    "     \n",
    "   其中，网络参数现在由编码器参数Θe、自我表现层参数Θs和解码器参数Θd组成，其中重构数据ˆX现在是{Θe，Θs，Θd}的函数，而不仅仅是{Θe，Θd}.  \n",
    "     \n",
    "<br>  \n",
    "  \n",
    "#### 网络结构  \n",
    "  \n",
    "  ![](img/2.PNG)  \n",
    "    \n",
    " 我们的网络由三部分组成，编码器，Self-Expressiveness层  ，解码器层。在本文中，由于我们关注的是图像聚类问题，我们提倡使用卷积自动编码器，它比完全连接的编码器参数少，因此更容易训练。但是请注意，全连接的自动编码器也与我们的自我表达层兼容。在卷积层中，我们在水平和垂直方向使用步长为2的核，非线性激活使用校正线性单位（ReLU）。  \n",
    "   \n",
    "   <br>  \n",
    "   \n",
    "  给定N个要聚类的图像，我们在一个批中使用所有的图像。每个输入图像由卷积编码器层映射到一个潜在向量（或节点）zi，在图2中用阴影圆圈表示。在自表达层中，使用线性权值完全连接节点，而不存在偏差和非线性激活。然后通过反褶积解码层将潜在向量映射回原始图像空间。  \n",
    "    \n",
    "  对于内核大小为ki×ki的ni通道的第i个编码器层，权重参数的数量为k2 ini−1ni，n0=1。由于编码器和解码器具有对称结构，它们的参数总数isP i2k2 ini−1ni加上偏置参数sp i2ni−n1+1的数量。对于N个输入图像，自我表现层的参数数目为N2   \n",
    "    \n",
    " <br>  \n",
    "   \n",
    "#### 训练策略  \n",
    "  \n",
    "  由于用于无监督子空间聚类的数据集的大小通常是有限的（例如，在数千张图像的顺序上），我们的网络仍然是可控制的大小。然而，出于同样的原因，从零开始直接训练一个具有数百万参数的网络仍然是困难的。为了解决这个问题，我们设计了下面描述的预培训和微调策略。请注意，这也允许我们在最小化损失的同时避免琐碎的全零解。  \n",
    "    \n",
    "  如图2所示，我们首先对deep auto-encoder进行预训练，而不需要对所有数据进行自我表达层。然后我们使用训练好的参数初始化网络的编码器和解码器层。在此之后，在微调阶段，我们使用所有数据构建一个大批量，用梯度下降法将（4）中定义的损失ΘL（Θ）最小化。  \n",
    "    \n",
    "  具体地说，我们使用Adam，一种基于动量的自适应梯度下降方法来最小化损失，在我们的所有实验中，我们将学习速率设置为1.0×10−3。  \n",
    "    \n",
    "由于我们在每个训练阶段都使用同一批，所以我们的优化策略是一种基于确定性动量的梯度法，而不是随机梯度法。还请注意，由于我们只能访问用于培训的图像，而不能访问群集标签，所以我们的培训策略是无监督的（或自我监督的）。  \n",
    "  \n",
    "#### 聚类策略  \n",
    "  \n",
    "  一旦网络被训练，我们就可以使用自表达层的参数来构造谱聚类的亲和矩阵。虽然这种亲和力矩阵原则上可以计算为| C |+| CT |，但多年来，该领域的研究人员开发了许多启发式方法来改进得到的矩阵。由于文献中没有这一步骤的全球公认的解决方案，我们使用SSC[10]和EDSC[15]所采用的启发式方法。由于篇幅不足，我们建议读者参考SSC的公开实现和[15]的第5节，以及我们算法2的TensorFlow实现，以获得更详细的信息。\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
