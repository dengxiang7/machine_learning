{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 原型聚类\n",
    "原型：一个类型的典型模型或原形象。  \n",
    "  \n",
    "原型聚类：就是通过一组原型，来对数据集进行聚类。方法为随机化一组原型，通过对数据集迭代来学得这组原型。  \n",
    "  \n",
    "#### K均值算法\n",
    "样本集$D=\\{x_1,x_2,\\cdots,x_n\\}\\;\\;\\;$（原型聚类）K均值算法就是假设K个聚类结构，$C=\\{c_1,c2,\\cdots,c_n\\}$,   \n",
    "  \n",
    "  目标是最小化:$$E=\\sum_{i=1}^k\\sum_{x\\in C_i}||x-u_i||^2 \\;\\qquad u_i=\\frac{1}{|c_i|}\\sum_{x \\in c_i}x$$  \n",
    "    \n",
    " 从这个式子可以看出：我们最小化目标就是在求样本围绕簇$c_i$的均值向量的紧密程度。E值越小簇内样本相似程度越高。  \n",
    "     \n",
    "  <br>  \n",
    "  \n",
    "     \n",
    "   然而要找到此式的最优解，需要考察样本集样本的所有可能划分，样本集多时NP难，所以直接求此式的最优解很难。  \n",
    "     \n",
    "   所以采用贪心策略求此式的最优解：即贪心算法认为计算每个样本与当前簇的均值向量的距离，并把此样本归到最小距离的簇中。（虽然实际中可能有  \n",
    "   $\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad$其他的样本组合的簇的均值向量 $\\;\\;$与此样本的距离更近$\\;\\;$）；然后通过多次迭代更新均值向量，来求得簇结构。  \n",
    "     \n",
    "       \n",
    "   算法结构截图周老师203页：  \n",
    "   ![](../../img/Pattern_recognition/clustering/kmeans.PNG)  \n",
    "       \n",
    "         \n",
    "<br>     \n",
    "<br>\n",
    "       \n",
    " #### 学习向量量化 (LVQ) \n",
    " 学习向量量化：与K均值相似（但是更新原型向量的策略不同）；但是LVQ是假设数据是有标记的，事先我们假设要聚成几类（k）就把无标记的数据集样本 \n",
    "   \n",
    "   $\\qquad\\qquad\\quad$做假设标记。然后初始化一组原型 向量（q个）原性向量的个数不一定要等与标签个数（$q\\geq k$）;为了确保每个真实标签都有原型向量，  \n",
    "     \n",
    "   $\\qquad\\qquad\\quad$我们从每个假设的类别中选取几个原型向量;使用学习率来学习原型向量。  \n",
    "     \n",
    " <br>      \n",
    "       \n",
    "   给定样本集$D=\\{x_1,x_2,\\cdots,x_n\\}$;先对样本集进行标记假设： $D=\\{（x_1，y_1),（x_2，y_2),\\cdots,（x_n，y_n)\\}$,从每个标记类中抽取若干个初始化原型向量得到n个原型行了$\\{p_1,p_2,\\cdots,p_n\\}$ ;计算样本$x_j$到原型向量的距离$d_{ji}=||x_j-p_i||^2$ ;选取样本到原型向量最小距离的原型向量并检查原型向量的标签与该样本的标签是否一致。  \n",
    "     \n",
    "  更新原型向量（也叫原型向量学习）：一致减少原型向量与样本之间的距离：\n",
    "  $$ ||p^{'}-x_j||_2=(1-\\eta)*||P_i^*-x_j||_2$$  \n",
    "  \n",
    "   $$ ||p^{'}-x_j||_2\\;=\\;||\\;\\;(1-\\eta)（P_i^*-x_j）\\;\\;||_2\\;=\\;||\\;P_i^*+\\eta(x_j-p_i^*)-x_j\\;||_2$$   \n",
    "     \n",
    "       \n",
    "   $$p^{'}=P_i^*+\\eta(x_j-p_i^*)$$    \n",
    "       \n",
    "         \n",
    "   不一致增大原型向量与样本之间的距离：  \n",
    "   $$ ||p^{'}-x_j||_2=(1+\\eta)*||P_i^*-x_j||_2$$  \n",
    "  \n",
    "   $$ ||p^{'}-x_j||_2\\;=\\;||\\;\\;(1+\\eta)（P_i^*-x_j）\\;\\;||_2\\;=\\;||\\;P_i^*-\\eta(x_j-p_i^*)-x_j\\;||_2$$   \n",
    "     \n",
    "       \n",
    "   $$p^{'}=P_i^*-\\eta(x_j-p_i^*)$$   \n",
    "     \n",
    "       \n",
    "  学的一组原型向量后，对任意样本x，它将被划入与其距离最近的原型向量所代表的簇中。  \n",
    "    \n",
    "  算法结构截图周老师205页：  \n",
    "   ![](../../img/Pattern_recognition/clustering/lvq.PNG)  \n",
    "       \n",
    "  <br>\n",
    "  <br>\n",
    "       \n",
    "  #### 高斯混合聚类（GMM）\n",
    " 高斯混合模型：任何事件都可以有多个高斯分布组成。\n",
    " $$p(x|\\theta)=\\sum_{i=1}^n\\alpha_iN(x|u_i,\\Sigma_i)$$  \n",
    "   \n",
    "   $$\\sum_{i=1}^n\\alpha_i=1$$   \n",
    "     \n",
    "       \n",
    " <br> \n",
    " \n",
    " 最大化期望：  \n",
    " $$E_{\\alpha}\\;[lnp(X,\\alpha|\\theta)]$$  \n",
    "    \n",
    "     \n",
    "     \n",
    "  $$E_{\\alpha}\\;[\\sum_{i=1}^n ln\\;p(x_i,\\alpha_i|\\theta)]$$   \n",
    "    \n",
    "      \n",
    "   $$=\\sum_{i=1}^n E_{\\alpha}ln\\;p(x_i,\\alpha_i|\\theta)$$    \n",
    "     \n",
    "   $$=\\sum_{i=1}^n \\sum_{l=1}^k p(\\alpha_i=l|x_i,\\theta)\\;\\;ln\\;p(x_i,\\alpha_i=l|\\theta)$$   \n",
    "     \n",
    "       \n",
    "  <br>       \n",
    "   \n",
    "   其中$\\alpha_i$在$x_i和\\theta^t$下的概率分布：\n",
    "   $$p(\\alpha_i=l|x_i,\\theta)=\\frac{p(\\alpha_i=l,x_i,\\theta)}{p(x_i,\\theta)}=\\frac{N(x_i|U_l^t,\\Sigma_l^t)*\\alpha_i=l}{\\sum_{l=1}^k N(x_i|U_l^t,\\Sigma_l^t)*\\alpha_i=l}\\;\\;\\;\\;@\\gamma_{nl}^t$$  \n",
    "     \n",
    "       \n",
    "  其中$\\alpha_i$在$x_i和\\theta^t$下的取值：  \n",
    "  $$ln\\;p(x_i,\\alpha_i=l|\\theta)=ln\\;(\\;N(x_i|U_l^t,\\Sigma_l^t)*\\alpha_i=l\\;)$$  \n",
    "    \n",
    "      \n",
    "  所以期望是 \n",
    "  $$Q(\\theta)=E_{\\alpha}\\;[lnp(X,\\alpha|\\theta)]=\\sum_{i=1}^n \\sum_{l=1}^k\\;\\;\\gamma_{nl}^t\\;*\\;ln\\;(\\;N(x_i|U_l^t,\\Sigma_l^t)*\\alpha_i=l\\;)$$   \n",
    "    \n",
    "  求使期望最大参数  \n",
    "  \n",
    "    \n",
    " $$\\theta^{t+1}=\\underset{\\theta}{arg\\;max\\;}Q(\\theta^t)=\\underset{\\theta}{arg\\;max\\;}\\sum_{i=1}^n \\sum_{l=1}^k\\;\\;\\gamma_{nl}^t\\;*\\;ln\\;(\\;N(x_i|U_l^t,\\Sigma_l^t)*\\alpha_i=l\\;)$$  \n",
    "   \n",
    "     \n",
    "<br>     \n",
    "对此式求导解出最值：  \n",
    "  \n",
    "  $$\\frac{N(x_i|U_l^t,\\Sigma_l^t)*\\alpha_i=l}{\\sum_{l=1}^k N(x_i|U_l^t,\\Sigma_l^t)*\\alpha_i=l}\\;\\;\\;\\;@\\gamma_{nl}^t$$  \n",
    "    \n",
    "  <br>\n",
    "  \n",
    "  $$U^{t+1}=\\frac{\\sum_{i=1}^n\\gamma_{nl}^tx_i}{\\sum_{i=1}^n\\gamma_{nl}^t}$$  \n",
    "    \n",
    "      \n",
    "   <br>  \n",
    "   \n",
    "   $$\\Sigma^{t+1}=\\frac{\\sum_{i=1}^n\\gamma_{nl}^t(x_i-U_l^{t+1})(x_i-U_l^{t+1})^T}{\\sum_{i=1}^n\\gamma_{nl}^t}$$  \n",
    "     \n",
    "   <br>  \n",
    "   \n",
    "   $$\\alpha^{t+1}=\\frac{\\sum_{i=1}^n\\gamma_{nl}^t}{N}$$   \n",
    "     \n",
    "       \n",
    "  对上述过程迭代直至收敛。   \n",
    "    \n",
    "  <br>  \n",
    "   ##### 理解\n",
    "   虽然根据高斯混合模型知道，事件可以由多个高斯分布组合而成；但是在无标签的样本的集合中我们并不知道样本属于哪个高斯分布；如果知道样本属于哪个高斯分布；我们可以直接根据极大似然函数求出各个高斯分布的均值和方差；$\\;\\;\\;\\;$虽然现在我们不知道样本属于哪个高斯分布，（假设事件由K个高斯分布组成）；但是我们可以知道单个样本x在这K个高斯分布下的真实概率分布；那么我们就可以从样本真实概率的极大似然函数，变成求真实分布期望的极大似然函数。  \n",
    "     \n",
    "       \n",
    "  <br>   \n",
    "  算法结构截图周老师210页：  \n",
    "   ![](../../img/Pattern_recognition/clustering/gmm.PNG)  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
