{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax回归\n",
    "logstic回归是用线性回归处理二分类的回归任务，那如果用线性回归处理多分类的任务呢？这时使用softmax回归。  \n",
    "\n",
    "###  思路\n",
    "  \n",
    "**1.** 观看二分类任务的logstic回归  \n",
    "\n",
    "Sigmoid函数：<img src=\"../../img/sigmoid.png\" width=\"300\" height=\"300\">  \n",
    "  \n",
    "  \n",
    "  \n",
    "$Y=W_1*X$用来当做预测X为l类的可能值 \n",
    "\n",
    "Sigmoid函数：$y=\\frac {1}{1+e^{-(W_1X)}}\\quad$ 把这个式子看成求y是1类的后验概率 $\\quad  p(y=1|x)=\\frac {e^{(W_1X)}}{1+e^{(W_1X)}} \\quad p(y!=1|x)=\\frac {1}{1+e^{(W_1X)}} \\qquad $  \n",
    "\n",
    "\n",
    "在二分类任务时，我们处理单个样本时是把>0.5的处理成1类，<=0.5处理成二类；其从后验概率的角度看是样本为1的后验概率大于0.5则认为样本为1类，否则是样本为1类的后验概率小于0.5，认为其不大可能是1类，则归为2类。  \n",
    "  \n",
    "**2.** 现在我们把其推广到多分类任务[1,2,...,k]时，我们再来看Sigmoid的函数:  \n",
    "二分类时Sigmoid函数把求y预测值转为求y为某类的可能的后验概率，现在类推多类时我们也用线性方式把求y为某类的预测值，转为求y为某类的可能后验概率：  \n",
    "\n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\; \\qquad\\; \\qquad\\; Y_1=W_1*X\\qquad $  \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\; \\qquad\\; \\qquad\\; Y_2=W_2*X\\qquad $  \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\; \\qquad\\;\\;\\;\\qquad\\;{\\cdots}{\\cdots}{\\cdots}{\\cdots}$  \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\;\\qquad\\;\\;\\;\\qquad\\;{\\cdots}{\\cdots}{\\cdots}{\\cdots}$     \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\; \\qquad\\; \\qquad\\; Y_i=W_i*X\\qquad $  \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\;\\qquad\\;\\;\\;\\qquad\\;{\\cdots}{\\cdots}{\\cdots}{\\cdots}$  \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\;\\qquad\\;\\;\\;\\qquad\\;{\\cdots}{\\cdots}{\\cdots}{\\cdots}$    \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\; \\qquad\\; \\qquad\\; Y_k=W_k*X\\qquad $    \n",
    "  \n",
    "\n",
    "我们处理单个样本时,要把求y的预测值转为求y为某类的可能概率：y可能为[1,2,...,k]的某一个:  \n",
    "$\\color{red}{\\bf利用正则化(概率为正，所以先正则化)和归一化就可以把线性模型预测的Y的值，转化为预测y属于某类的后验概率  }$  \n",
    "    \n",
    "进行正则化，也就是进行广义的线性模型，不会影响预测（一一对应）主要是Y的预测值映射为正值：  \n",
    "  \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\; \\qquad\\; \\qquad\\; Y_1=e^{W_1*X}\\qquad $  \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\; \\qquad\\; \\qquad\\; Y_2=e^{W_2*X}\\qquad $  \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\; \\qquad\\;\\;\\;\\qquad\\;{\\cdots}{\\cdots}{\\cdots}{\\cdots}$  \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\;\\qquad\\;\\;\\;\\qquad\\;{\\cdots}{\\cdots}{\\cdots}{\\cdots}$     \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\; \\qquad\\; \\qquad\\; Y_i=e^{W_i*X}\\qquad $  \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\;\\qquad\\;\\;\\;\\qquad\\;{\\cdots}{\\cdots}{\\cdots}{\\cdots}$  \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\;\\qquad\\;\\;\\;\\qquad\\;{\\cdots}{\\cdots}{\\cdots}{\\cdots}$    \n",
    "$\\qquad \\qquad\\; \\qquad\\; \\qquad\\;  \\qquad\\; \\qquad\\; \\qquad\\; \\qquad\\; Y_k=e^{W_k*X}\\qquad $    \n",
    "  \n",
    "    \n",
    "  \n",
    "  \n",
    "  \n",
    "归一化：把预测的Y值转变为Y为某类的概率分布(预测为某类的概率)：  \n",
    "  \n",
    "  \n",
    "  \n",
    "$ \\qquad \\qquad\\; \\qquad\\; \\qquad \\qquad \\qquad  \\begin{bmatrix} p(y=1|x) \\\\ p(y=2|x)\\\\{\\vdots}\\\\p(y=i|x)\\\\{\\vdots} \\\\p(y=k|x) \\end{bmatrix} \\;= \\;\\; \\begin{bmatrix} \\frac {e^{(W_1X)}}{\\sum_{i=1}^j e^{(W_iX)}} \\\\ \\frac {e^{(W_2X)}}{\\sum_{i=1}^j e^{(W_iX)}}\\\\{\\vdots}\\\\\\frac {e^{(W_iX)}}{\\sum_{i=1}^j e^{(W_iX)}} \\\\{\\vdots}\\\\\\frac {e^{(W_kX)}}{\\sum_{i=1}^j e^{(W_iX)}} \\end{bmatrix}\\;=\\;  \\frac {1}{\\sum_{i=1}^j e^{(W_iX)}} * \\begin{bmatrix} e^{(W_1X)} \\\\ e^{(W_2X)}\\\\{\\vdots}\\\\e^{(W_iX)} \\\\{\\vdots}\\\\e^{(W_kX)} \\end{bmatrix}$\n",
    "  \n",
    "既然现在已经得到了预测的y为各个类的概率，则可以开始求正确概率的极大似然函数：  \n",
    "  \n",
    "样本X的正确预测：$p(y=i|x)={\\sum_{i=1}^k} \\{y =\\,= k\\}p(y=i|x) \\;=\\; \\begin{bmatrix} 0 &0 &{\\cdots}&\\underset{y真实标记位置}{1} &\\;{\\cdots} &0 \\end{bmatrix} * \\begin{bmatrix} \\frac {e^{(W_1X)}}{\\sum_{i=1}^j e^{(W_iX)}} \\\\ \\frac {e^{(W_2X)}}{\\sum_{i=1}^j e^{(W_iX)}}\\\\{\\vdots}\\\\\\frac {e^{(W_iX)}}{\\sum_{i=1}^j e^{(W_iX)}} \\\\{\\vdots}\\\\\\frac {e^{(W_kX)}}{\\sum_{i=1}^j e^{(W_iX)}} \\end{bmatrix}\\;=\\;t_{ik}*P(X_i)$  \n",
    "  \n",
    "参数$W$的极大似然值：$lnp( \\prod_{i=1}^n  p(y=i|x)) \\;=\\;{\\sum_{i=1}^n} lnp(p(y=i|x)\\;={\\underset{对列,行求和} {sum}(Tik.*ln(P))}\\;\\;（点乘）$  \n",
    "  \n",
    "$$\n",
    "       Tik= \\begin{pmatrix}\n",
    "        0 & 0 & 1 & \\cdots & 0 \\\\\n",
    "        1 & 0 & 0& \\cdots & 1 \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        0 & 1 & 0 & \\cdots & 0\\\\\n",
    "        \\end{pmatrix}_{k\\times n}\n",
    "$$\n",
    "  \n",
    "<br>   \n",
    "<br> \n",
    "\n",
    "$$\n",
    "       P= \\begin{pmatrix}\n",
    "        \\frac {e^{(W_1X_1)}}{\\sum_{i=1}^j e^{(W_iX_1)}} & \\frac {e^{(W_1X_2)}}{\\sum_{i=1}^j e^{(W_iX_2)}} & \\cdots & \\frac {e^{(W_1X_n)}}{\\sum_{i=1}^j e^{(W_iX_n)}} \\\\\n",
    "       \\frac {e^{(W_2X_1)}}{\\sum_{i=1}^j e^{(W_iX_1)}} & \\frac {e^{(W_2X_2)}}{\\sum_{i=1}^j e^{(W_iX_2)}} & \\cdots & \\frac {e^{(W_2X_n)}}{\\sum_{i=1}^j e^{(W_iX_n)}} \\\\\n",
    "        \\vdots &  \\vdots & \\ddots & \\vdots \\\\\n",
    "       \\frac {e^{(W_kX_1)}}{\\sum_{i=1}^j e^{(W_iX)_1}} & \\frac {e^{(W_kX_2)}}{\\sum_{i=1}^j e^{(W_iX_2)}} & \\cdots & \\frac {e^{(W_kX_n)}}{\\sum_{i=1}^j e^{(W_iX_n)}}\\\\\n",
    "        \\end{pmatrix}_{k\\times n}\n",
    "$$\n",
    "  \n",
    "<br>  \n",
    "<br> \n",
    "\n",
    "\n",
    "$$\n",
    "       W= \\begin{pmatrix}\n",
    "        w1_1& w2_1 & \\cdots & wk_1 \\\\\n",
    "        w1_2 & w2_2  & \\cdots &wk_2 \\\\\n",
    "        \\vdots &  \\vdots & \\ddots & \\vdots \\\\\n",
    "       w1_j & w2_j & \\cdots & wk_j\\\\\n",
    "        \\end{pmatrix}_{j\\times k}\n",
    "$$  \n",
    "  \n",
    "<br>  \n",
    "<br> \n",
    "      \n",
    "$$\n",
    "       X= \\begin{pmatrix}\n",
    "        x1_1& x2_1 & \\cdots & xn_1 \\\\\n",
    "        x1_2 & x2_2  & \\cdots &xn_2 \\\\\n",
    "        \\vdots &  \\vdots & \\ddots & \\vdots \\\\\n",
    "       x1_j & x2_j & \\cdots & xn_j\\\\\n",
    "        \\end{pmatrix}_{j\\times n}\n",
    "$$  \n",
    "  \n",
    "<br> \n",
    "<br>  \n",
    "  \n",
    "  \n",
    "  \n",
    "所以最终的极大似然函数值为：  ${\\underset{W}{maxarg}}=\\underset{对列,行求和}{sum}(Tik.*ln(P))=\\underset{对列，行求和}{sum}(Tik.*ln((\\frac {e^{(W^TX)}}{\\underset{对列求和}{sum (e^{(W^TX))})}}) ))\\quad(这里是点乘)$  \n",
    "  \n",
    "  \n",
    "$\\qquad\\qquad\\qquad\\qquad\\;\\;\\;\\;E(W)=-{\\underset{W}{minarg}}=-\\underset{对列,行求和}{sum}(Tik.*ln(P))=-\\underset{对列，行求和}{sum}(Tik.*ln((\\frac {e^{(W^TX)}}{\\underset{对列求和}{sum} (\\;e^{(W^TX)}\\;)}))\\;)$  \n",
    "   \n",
    "  \n",
    "$\\qquad\\qquad\\qquad\\qquad\\;\\;\\;\\;E(W)=-{\\underset{W}{minarg}}=-\\underset{对列,行求和}{sum}(Tik.*ln(P))=-\\underset{对列，行求和}{sum}(Tik.*[（ln( {e^{(W^TX)}})-ln({\\underset{对列求和}{sum} (\\;e^{(W^TX)}\\;)})]\\;)$    \n",
    "  \n",
    "$\\qquad\\qquad\\qquad\\qquad\\;\\;\\;\\;\\frac {\\partial E(W)}{\\partial W}=-(Tik.*(X^T-\\frac{e^{(W^TX)}X^T}{\\underset{对列求和}{sum} (\\;e^{(W^TX)}\\;)})\\;)\n",
    "\\quad$  \n",
    "$\\qquad\\qquad\\qquad\\qquad\\;\\;\\;\\;（因为是对整个W求导不是对分量w_1{\\cdots}w_k 求偏导，所以sum去掉，想当于求分量的偏导后，再和起来。）$\n",
    "  \n",
    "$\\qquad\\qquad\\qquad\\qquad\\;\\;\\;\\;\\frac {\\partial E(W)}{\\partial W}=-((Tik-\\frac{e^{(W^TX)}}{ \\underset{对列求和}{sum}(\\;e^{(W^TX)}\\;)})X^T\\;) $    \n",
    "\n",
    "使用梯度下降法求最优解：  \n",
    "$\\qquad\\qquad\\qquad\\qquad\\;\\;\\;\\;W^{t+1}=W^{t}-\\eta(\\;-((Tik-\\frac{e^{(W^TX)}}{\\underset{对列求和}{sum} (\\;e^{(W^TX)}\\;)})X^T\\;)^T \\;)$  \n",
    "  \n",
    "用训练的模型预测时找其中最大值：  \n",
    "  \n",
    "$\\qquad\\qquad\\qquad\\qquad\\;\\;\\;\\; Y_i=W_i*X\\qquad $   \n",
    "  \n",
    "    \n",
    "      \n",
    "        \n",
    "### 总结\n",
    "从上述的计算过程可以看出，最要的就是计算每个样本的各个可能类别所占的概率，这个过程可以抽象成softmax函数：  \n",
    "  \n",
    "    \n",
    "    \n",
    "softmax函数（softmax operator）解决了输出值的范围不确定，我们难以直观上判断这些值的意义，将输出值变换成值为正且和为1的概率分布：\n",
    "\n",
    "$$\\hat{y}_1, \\hat{y}_2, \\hat{y}_3 = \\text{softmax}(o_1, o_2, o_3),$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$\n",
    "\\hat{y}_1 = \\frac{ \\exp(o_1)}{\\sum_{i=1}^3 \\exp(o_i)},\\quad\n",
    "\\hat{y}_2 = \\frac{ \\exp(o_2)}{\\sum_{i=1}^3 \\exp(o_i)},\\quad\n",
    "\\hat{y}_3 = \\frac{ \\exp(o_3)}{\\sum_{i=1}^3 \\exp(o_i)}.\n",
    "$$  \n",
    "  \n",
    "<br>\n",
    "<br>\n",
    "      \n",
    "softmax函数有三个重要的性质：  \n",
    "softmax函数将任意n维实值向量转换为取值范围为（0，1）之间的n维实值向量，并且和为1。  \n",
    "\n",
    "1.单调递增函数，因此不改变原始数据的大小顺序   \n",
    "\n",
    "2.讲原始数据映射到（0，1）之间，和为1，表征概率  \n",
    "\n",
    "3.$Softmax(x)=Softmax(x+c)$ 这个性质重要，涉及到数据的稳定性。在python中由于浮点数范围限制，当超出限制会出错，一般$C=-max(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(785, 10)\n",
      "准确率\n",
      "0.915\n",
      "用时：\n",
      "806.5617842674255\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#实例识别图片中0到10\n",
    "%matplotlib inline\n",
    "import time\n",
    "from mxnet.gluon import data as gdata\n",
    "import numpy as np\n",
    "import struct   #处理数据流\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import tools.readimg as tl\n",
    "import bigfloat\n",
    "import math\n",
    "from numpy import matlib\n",
    "\n",
    "\n",
    "#生成训练数据和测试数据\n",
    "f_train=\"../../img/train-images.idx3-ubyte\"\n",
    "ff_train=\"../../img/train-labels.idx1-ubyte\"\n",
    "f_test=\"../../img/t10k-images.idx3-ubyte\"\n",
    "ff_test=\"../../img/t10k-labels.idx1-ubyte\"\n",
    "imgs_train=tl.readallimg(f_train)\n",
    "labs_train=tl.readalllabels(ff_train)\n",
    "img_test=tl.readallimg(f_train)\n",
    "lab_test=tl.readalllabels(ff_train)\n",
    "\n",
    "#图片转为样本矩阵\n",
    "def tranexample(imgs_train):\n",
    "    Tr_x=[]    \n",
    "    for i in imgs_train:\n",
    "        Tr_x.append(((np.row_stack(( i.reshape(784,1),1))).reshape(1,785))[0])\n",
    "    return Tr_x\n",
    "#print((np.array(Tr_x)).shape)  \n",
    "Tr_x=tranexample(imgs_train)\n",
    "\n",
    "#初始化参数\n",
    "W=(np.random.normal(scale=0.1,size=(10,785))).T\n",
    "print(W.shape)\n",
    "\n",
    "#定义训练算法\n",
    "def train(W,X,Y,eta,ecoh):\n",
    "    for i in range(ecoh):\n",
    "        #print(i)\n",
    "        maxW=np.amax(np.dot(W.T,X),0)\n",
    "        a=np.exp(np.dot(W.T,X)-maxW)\n",
    "        c=a/a.sum(0)\n",
    "        Tik=[]\n",
    "        Lk=np.linspace(0,9,10)\n",
    "        for i in range(len(Y)):    \n",
    "            for j in range(len(Lk)):\n",
    "                \n",
    "                if Y[i]==Lk[j]:\n",
    "                    Tik.append(1)\n",
    "                else :\n",
    "                    Tik.append(0)         \n",
    "        Tik=np.array(Tik)\n",
    "        Tik=np.mat(Tik.reshape(len(Y),len(Lk))).T\n",
    "        d=-np.dot(Tik-c , X.T)\n",
    "        W=W-eta*d.T \n",
    "    return W\n",
    "\n",
    "#预测函数：\n",
    "def predict(W,img_test):\n",
    "    img_test=(np.array(img_test)).reshape((np.array(img_test).shape[0],1))\n",
    "    #Y=np.exp(np.dot(W.T,img_test))\n",
    "    #Y=np.nan_to_num(Y)\n",
    "    Y=np.dot(W.T,img_test)\n",
    "    k=((Y.reshape(1,len(Y))).argmax(1))\n",
    "    return k\n",
    "\n",
    "#准确率\n",
    "def accure(W,img_test,lab_test):\n",
    "    pre_lab=[]\n",
    "    num=0\n",
    "    for  i in range(len(lab_test)):\n",
    "        k=predict(W,img_test[:,i])\n",
    "        k=np.array(k)\n",
    "        k=(k[0])[0]\n",
    "        pre_lab.append(k)\n",
    "        if k == lab_test[i] :\n",
    "            num+=1\n",
    "    return num/len(lab_test) ,pre_lab\n",
    "\n",
    "\n",
    "#开始训练\n",
    "eta=0.01\n",
    "ecoh=300\n",
    "labs_train1=np.array(labs_train)\n",
    "Tr_x1=np.array(Tr_x)\n",
    "#labs_train1=labs_train1.reshape(labs_train1.shape[0],1)\n",
    "start=time.time()\n",
    "W1=train(W,Tr_x1.T,labs_train1,eta,ecoh)\n",
    "\n",
    "img_test1=np.array(tranexample(img_test))\n",
    "preaccure,prelab=accure(W1,img_test1.T,lab_test)\n",
    "print('准确率')\n",
    "print(preaccure)\n",
    "print('用时：')\n",
    "print(time.time()-start)\n",
    "tl.showimg(img_test,prelab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
