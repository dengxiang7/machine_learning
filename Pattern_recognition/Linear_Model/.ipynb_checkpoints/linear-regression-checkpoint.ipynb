{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 线性回归\n",
    "学的模型：$ f(x_1,x_2,{\\cdots} ,x_n)= a_1x_1+a_2x_2+{\\cdots}+a_nx_n+b$进行预测。\n",
    "![线性模型](../../img/linear-flow-img.png)  \n",
    "由此框架：    \n",
    "判别函数选择(单个样本)：  $ \\quad f(X)= WX $ &emsp;&ensp;&ensp; $W=[w_1,w_2,{\\cdots},w_n,b]$  &emsp;&ensp;&ensp;$X=[x_1,x_2,{\\cdots},x_n]^T$  \n",
    "  \n",
    "  \n",
    "优化准则(所有样本)： 最小二乘法（均方误差最小） $ (W^*,b^*)={\\underset{(W,b)} {arg\\,min}}{\\sum_{i=1}^n}(f(X^i)-y^i)^2$  \n",
    "\n",
    "\n",
    "优化算法(样本矩阵)：就是求$E( W)=({\\bf X^T}W^T- Y)^T({\\bf X^T}W^T-Y)$的最优解（样本用矩阵表示，每列一个样本）  \n",
    "  \n",
    "求解$E(W)$也就是损失函数的最优解时，通常有两种情况，一种是有解析解，一种是数值解（通常求不出解析解），本例两种都可以  \n",
    "解析解：  \n",
    "\n",
    "  $$\\frac{\\partial E(W)}{\\partial W}=2{\\bf X}（{\\bf X}^TW^T-Y）$$  \n",
    "  \n",
    "  $$令\\frac{\\partial E(W)}{\\partial W}=0 $$  \n",
    "  $$W^T=({\\bf XX}^T)^{-1}{\\bf X}Y$$  \n",
    "  现在这一步是否能经行下去是不确定的：$XX^T$  是否可逆是关键  \n",
    "  \n",
    "  $XX^T$ 可逆(唯一解): $ \\qquad\\qquad\\qquad \\qquad \\qquad\\qquad W^T=({\\bf XX}^T)^{-1}{\\bf X}Y$  \n",
    "    \n",
    "  $XX^T$ 不可逆（多解）: $ \\qquad\\qquad\\qquad \\quad\\qquad\\qquad \\frac{\\partial E(W)}{\\partial W}=0 \\quad即\\quad {\\bf XX}^TW^T={\\bf X}Y $  \n",
    "  \n",
    "  此时样本个数小于属性数，或样本相关性强（行向量不是线性无关的）此时$R(A)<n$有多组解，选择那组解，可以设置正则项来选择偏好\n",
    "  $$E( W)=({\\bf X^T}W^T- Y)^T({\\bf X^T}W^T-Y)+\\alpha \\psi(W)$$  \n",
    "    \n",
    "   ### 广义线性回归\n",
    "   1.寻常回归预测值$W{\\bf X}$和其真实值$Y$是线性对应的;$\\quad$现在如果把$Y$值与非线性值$g(Y)$一一对应,则现在$W{\\bf X}$与$Y$一一对应，$Y$与$g(Y)$一一对应（非线性），故$W{\\bf X}$与$g(Y)一一一对应 \\qquad$有广义线性回归：$Y=g^{-1}({\\bf X}^TW^T)$  \n",
    "   2.寻常回归变量值${\\bf X}$和非线性值$\\phi({\\bf X}^T)$一一对应;则有$\\qquad$有广义线性回归：$Y=\\phi ({\\bf X}^T)W^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真实的模型值\n",
      "[2, 3, 5, 4]\n",
      "训练的模型值\n",
      "\n",
      "[1.9999998 2.9999998 4.9999995 3.9999998]\n",
      "<NDArray 4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "#示例线性回归\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "from mxnet import autograd, nd\n",
    "import random\n",
    "\n",
    "#定义真实值\n",
    "W=[2,3,5,4]\n",
    "\n",
    "#生成样本\n",
    "feature=nd.random.normal(scale=1,shape=(1000,4))\n",
    "feature[:,3]=1\n",
    "lables=W[0]*feature[:,0]+W[1]*feature[:,1]+W[2]*feature[:,2]+W[3]*feature[:,3]\n",
    "\n",
    "#定义判别函数\n",
    "def linear_reg(X,W):\n",
    "    Y=nd.dot(X,W.T)\n",
    "\n",
    "#定义优化算法\n",
    "def optimiza(X,Y):\n",
    "    return nd.dot(nd.dot(nd.linalg.inverse(nd.dot(X,X.T)),X),Y)\n",
    "    \n",
    "#初始化\n",
    "b=optimiza(feature.T,lables.T)\n",
    "print('真实的模型值')\n",
    "print(W)\n",
    "print('训练的模型值')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(nd.linalg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
