{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 降维    \n",
    "  高维空间带来的问题：  \n",
    "  * 1、样本稀疏：  \n",
    "  在KNN中我们假设了如果样本足够密，就能有很好的预测效果。然而在现实中我们不可能取得如此多得训练样本。  \n",
    "  $\\delta=0.001;$在一维的情况下：假设属性为x:那么把属性的取值归一化后，也就是让x的取值范围维\\[0-1\\];那么在\\[0-1\\]就需要有1000个   样本才能满足要求；在二维情况下：就需要有$1000 \\times 1000 =（10^3）^2$个样本；在10维的时候就需要$1000 \\times 1000 =（10^3）     ^{10}$个样本，而实际中我们属性数目远多于10个，所以我们实际不可能找到那么多的样本。  \n",
    "    \n",
    "   (维度增加时，空间的体积增加得很快，使得可用的数据变得稀疏。稀疏性对于任何要求有统计学意义的方法而言都是一个问题，为了获得在统计学上正确并且有可靠的结果，用来支撑这一结果所需要的数据量通常随着维数的提高而呈指数级增长。而且，在组织和搜索数据时也有赖于检测对象区域，这些区域中的对象通过相似度属性而形成分组。然而在高维空间中，所有的数据都很稀疏，从很多角度看都不相似，因而平常使用的数据组织策略变得极其低效)  \n",
    "     \n",
    "   <br>  \n",
    "    \n",
    "  * 2、距离计算困难：   \n",
    "  距离函数：超球体的半径r ,维数为d ，超立方体边长为2r   \n",
    "    \n",
    "    $\\Gamma(x)$函数：$\\Gamma(x)=\\int_0^{+\\infty}t^{x-1}e^{-t}dt(x >0)$\n",
    "  \n",
    "  3维欧几里球体体积：$\\frac{4\\pi r^3 2^3}{3}\\quad$  ;多维超球体：欧几里得距离：$\\frac{2r^d\\pi^{\\frac{d}{2}}}{d\\Gamma(d/2)}\\qquad$超立方体欧几里得距离：$(2r)^d$  \n",
    "    \n",
    "  当维数过高时，相对立方体的体积，其内接球的体积几乎可以忽略不记；这也说明了距离在高维空间不再有辨识度。  \n",
    "    \n",
    "  从超立体的超内接球体积极为小；可以得出一般的结论，，几乎所有的高维空间都远离其中心。  \n",
    "    \n",
    "  当维数越高，几乎空间中的所有点到中心距离的最大值和最小值几乎相同；这也说明了距离在高维空间不再有辨识度  \n",
    "    \n",
    "  $\\underset{d \\rightarrow +\\infty}{\\lim} \\frac{d_{max}\\;-\\;d_{min}}{d_{min}}\\rightarrow 0$  \n",
    "    \n",
    "  <br>  \n",
    "  \n",
    "  (因此，在某种意义上，几乎所有的高维空间都远离其中心，或者从另一个角度来看，高维单元空间可以说是几乎完全由超立方体的“边角”所组成的，没有“中部”，这对于理解卡方分布是很重要的直觉理解。 给定一个单一分布，由于其最小值和最大值与最小值相比收敛于0，因此，其最小值和最大值的距离变得不可辨别。)  \n",
    "    \n",
    "  (因此，在高维空间用距离来衡量样本相似性的方法已经渐渐失效。所以以距离为标准的分类算法（欧氏距离，曼哈顿距离，马氏距离）在低维空间会有更好的表现。类似的，高斯分布在高维空间会变得更加平坦，而且尾巴也会更长。\n",
    "对于使用距离测度的机器学习算法的影响：\n",
    "由于维度灾难的影响，正如前面所说的在高维空间中，欧式距离的测度会失去意义，当维度趋于无穷时，数据集中任意两点的距离会趋向收敛，意思是任意两点的最大距离和最小距离会变为相同。\n",
    "因此基于欧式距离的k-means算法，会无法进行聚类（因为距离会趋于收敛）。而K-NN会的临近K个点中，会出现更多非同类的点（远多于低维度的情况）  \n",
    "  \n",
    "    \n",
    "基于以上两种情况得出再高维空间带来的各种问题称为维数灾难；正是由于维数灾难我们才要降维处理。    \n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    " <br>\n",
    " <br>  \n",
    " \n",
    "    \n",
    "    \n",
    "### 低维嵌入\n",
    "\n",
    "一方面找不到满足密度采样要求的数据集；另一方面在高维的情况下，计算样本距离是困难的。所以使用降维来缓解维数灾难。  \n",
    "  \n",
    "<br>  \n",
    "\n",
    "降维：通过某种数学变换，将高维属性空间，变为低维子空间；在这个子空间中样本密度大幅度提高，距离计算也变得更容易。  \n",
    "  \n",
    "$\\qquad$虽然人们观察到样本属性是高维的，但是与我们学习任务相关的可能只是某一些属性而不是全部属性。也就是高维空间的一个低维嵌入。    \n",
    "\n",
    " <br>  \n",
    " \n",
    "    \n",
    "*  多维缩放（MDS）：  \n",
    "一种经典的降维方法，能够使在高维空间中样本之间的距离在低维空间中得以保持。  \n",
    "    \n",
    "  假设m个样本在原始空间中距离矩阵为为：$D \\in R^{m \\times m}$  \n",
    "     \n",
    "  目标是：是获得m个样本在低维子空间的表示$Z \\in R^{d^{'} \\times m}$ 且在子空间的距离矩阵不变也为$D \\in R^{m \\times m}$  \n",
    "    \n",
    "  $B=Z^TZ \\quad$B为内积矩阵 $b_{ij}=z_i^Tz_j  \\qquad$为了后面方便讨论，对降维样本中心化即$\\sum_{i=1}^mz_i=0 \\quad$那么对B有$\\sum_{i=1}^mb_{ij}=\\sum_{i=1}^mb_{ji}=0$   \n",
    "  \n",
    "   两样本之间的距离为：$dist_{ij}^2=||z_i||^2+||z_j||^2-2z_i^Tz_j  \\;=\\; b_{ii}+b_{jj}-2b_{ij}$   \n",
    "     \n",
    "   所以根据$\\sum_{i=1}^mz_i=0 \\quad tr(B)=\\sum_{i=1}^mb_{ii}$有：  \n",
    "     \n",
    "    $\\sum_{i=1}^mdist_{ij}^2=tr(B)+mb_{jj}$  \n",
    "      \n",
    "    $\\sum_{j=1}^mdist_{ij}^2=tr(B)+mb_{ii}$  \n",
    "      \n",
    "    $\\sum_{i=1}^m\\sum_{j=1}^mdist_{ij}^2=tr(B)+\\cdots+tr(B)=2mtr(B)$  \n",
    "          \n",
    "   重新令：$dist_{i\\cdot}^2$ ：  \n",
    "   $dist_{i\\cdot}^2=\\frac{1}{m}\\sum_{j=1}^m dist_{ij}^2$  \n",
    "     \n",
    "   $dist_{\\cdot j}^2=\\frac{1}{m}\\sum_{j=1}^m dist_{ij}^2$  \n",
    "     \n",
    "   $dist_{\\cdot \\cdot}^2=\\frac{1}{m^2}\\sum_{i=1}^m \\sum_{j=1}^m dist_{ij}^2$  \n",
    "     \n",
    "   结合以上公式可以推导出:$b_{ij}=-\\frac{1}{2}(dist_{ij}^2-dist_{\\cdot j}^2-dist_{i\\cdot}^2+dist_{\\cdot\\cdot}^2)$  \n",
    "     \n",
    "   得到这个式子，就可以明显看出，在原始空间样本距离矩阵不变的情况下，可以求出降维后的样本表示矩阵的内积矩阵B。（降维矩阵Z的距离矩阵也是D）  \n",
    "     \n",
    "   此时已经求出了B;而$B=Z^TZ \\qquad $ ；求出B之后，只要对B做特征分解，就可以求出Z;$\\;\\; B=\\bigvee\\bigwedge\\bigvee^T$那么$z=\\bigwedge^{\\frac{1}{2}}\\bigvee^T$     \n",
    "     \n",
    "   其中：$\\bigwedge$是特征值，组成的对角矩阵，$\\bigvee$是特征向量组成的矩阵。  \n",
    "     \n",
    "   多维缩放：主要是根据原始空间样本距离矩阵，求低维子空间的样本的内积矩阵，在通过特征值分解这个内积矩阵，来求出低维样本矩阵。 \n",
    "     \n",
    "    \n",
    "      \n",
    " <br>  \n",
    " <br>  \n",
    " ### 降维的种类  \n",
    "* 线性降维：  \n",
    "获得低维子空间，最简单就是对原始高维空间在低维子空间做线性映射（也就是线性变换）。  \n",
    "给定高维空间样本$X=\\{x_1,x_2,\\cdots,x_m\\}$,进行线性变换$Z_{d^{'}\\times m}=W^T_{d^{'}\\times d}X_{{d\\times m}}$就把d个属性的m个样本，变成了d'个属性，m个样本的集合.  \n",
    "其中若：$w_i\\;\\;and \\;\\;w_j\\;\\;i\\neq j$正交，那么W是正交坐标系，此时是正交变换。  \n",
    "如图通过某种线性变换，把3维空间的点降维到1维空间；对W施加不同的约束，我们会得到不同的子空间，例如对是W的1范数为1,就会得到不同子空间。\n",
    "![](../../img/Pattern_recognition/dr-ml/drml4.PNG)\n",
    "     \n",
    "       \n",
    " <br>  \n",
    " <br>\n",
    " \n",
    " ### 主成分分析（PCA）  \n",
    " * 什么是数据的主成分：  \n",
    " 假设有空间中，有3个点其中一个过原点，且这3个点在同一平面内。我们用x,y,坐标系表示这3个点，但是事实上这3个点是分布在2维平面上的，但是我们却需要一个3维坐标系x,y,z，来表示这3个点。，然而我们通过旋转x,y,z坐标系使数据平面和x,y平面重合，就可以用二维坐标系x,y来表示这3个数据。  \n",
    "![](../../img/Pattern_recognition/dr-ml/drml5.PNG)   \n",
    " 本质：  \n",
    " 把这3个数据用矩阵表示A，那么这个矩阵的秩就是2，也就是这个矩阵的最大线性无关向量组是2；注意这里为什么要有一个在原点的数据，因为在原点，则数据矩阵的秩必然<=n-1;就可以降维；这也就是必须要数据中心化的缘故。将坐标原点平移到数据中心，这样原本不相关的数据在这个新坐标系中就有相关性了！  \n",
    "   \n",
    " 使用这种降维方式，数据没有丢失任何信息，因为这些数据在第3维分量为0；然而如果第3维分量上有一个很小的波动，那么我们仍然用上述的二维表示这些数据，理由是我们可以认为这两个轴的信息是数据的主成分，而这些信息对于我们的分析已经足够了，z'轴上的抖动很有可能是噪声，也就是说本来这组数据是有相关性的，噪声的引入，导致了数据不完全相关，但是，这些数据在z'轴上的分布与原点构成的夹角非常小，也就是说在z'轴上有很大的相关性，综合这些考虑，就可以认为数据在x',y' 轴上的投影构成了数据的主成分！  \n",
    "   \n",
    "   PCA的思想是将n维特征映射到k维上（k<n），这k维是全新的正交特征。这k维特征称为主成分，是重新构造出来的k维特征，而不是简单地从n维特征中去除其余n-k维特征。  \n",
    " \n",
    "   \n",
    "     \n",
    "<br>  \n",
    "<br>  \n",
    "\n",
    "* PCA的推导：  \n",
    "在一个正交属性空间中的样本点，要做到上面讲的那样降维。首先要在这个空间中找到一个超平面，来表示这些数据，然后用在这个超平面上的数据来分析主成分。  \n",
    "那么按上面的要求这个空间中的样本：  \n",
    "最近重构性：样本点到这个超平面的距离都足够近；（非主成分）  \n",
    "最大可分性：样本点在这个超平面上的投影尽可能分开  \n",
    "  \n",
    " 1、对数据样本进行中心化即：$\\sum_{i}x_i=0$  \n",
    "   \n",
    " 2、假定投影变换后的坐标系为正交坐标系$w_i$为标准正交基：$W=\\{w_1,w_2,\\cdots,w_d\\} \\quad ||w_i||_2=1 \\quad w_i^Tw_j=0$  \n",
    "   \n",
    " 3、样本点降维d到d' $\\quad Z_{d^{'}\\times m}=(W_{d\\times d^{'}})^TX_{{d\\times m}}$  \n",
    "   \n",
    "   例如从8个样本从d=7维降到d'=3维  \n",
    "   $$\n",
    "        Z_{3\\times8}=\\begin{pmatrix}\n",
    "        z_{11} & z_{12} & \\cdots &  z_{18} \\\\\n",
    "        z_{21} & z_{22} & \\cdots & z_{28} \\\\\n",
    "        z_{31} & z_{32} & \\cdots & z_{38} \\\\\n",
    "        \\end{pmatrix}\n",
    "   $$  \n",
    "   \n",
    "    $$\n",
    "        W_{7\\times3}=\\begin{pmatrix}\n",
    "        w_{11} & w_{12}  &  w_{13} \\\\\n",
    "        w_{21} & w_{22} & w_{23} \\\\\n",
    "          \\vdots & \\vdots & \\vdots & \\\\\n",
    "        w_{71} & w_{72}  & w_{73} \\\\\n",
    "        \\end{pmatrix}\n",
    "   $$    \n",
    "   \n",
    "    $$\n",
    "        X_{7\\times8}=\\begin{pmatrix}\n",
    "        x_{11} & x_{12} & \\cdots &  x_{18} \\\\\n",
    "        x_{21} & x_{22}& \\cdots & x_{28} \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\ddots  \\\\\n",
    "        x_{71} & x_{72} & \\cdots & x_{78} \\\\\n",
    "        \\end{pmatrix}\n",
    "   $$   \n",
    "     \n",
    "   $Z=W^TX$;可以看出超平面的选择是3维的超平面这3维是主成分；那么其他4维就是非主成分。$w_1,w_2,w_3$就是正交变换系。其中每个$w_i$就是该系的方向向量。$z_{ij}=w_j^Tx_i$是$x_i$在低维坐标下第j维的坐标。  \n",
    "     \n",
    "   考虑整个数据集上，原样本X与基于投影重构的样本$\\hat{X}$之间的距离：  \n",
    "   $||WZ-X||_2^2 \\;\\;=\\;\\;(WZ-X)^T(WZ-X)=(Z^TW^T-X^T)(WZ-X)$  \n",
    "   \n",
    "   $\\qquad\\qquad\\qquad=Z^TZ-Z^TW^TX-X^TWZ+X^TX=Z^TZ-2Z^TW^TX+const$  \n",
    "     \n",
    "   $\\qquad\\qquad\\qquad\\approx -tr(W^TXX^TW)$   \n",
    "     \n",
    "   重构后的距离应该足够小：  \n",
    "     \n",
    "     $\\qquad\\qquad\\qquad \\underset{W}{min}\\;\\;-tr(W^TXX^TW)$  \n",
    "       \n",
    "     $\\qquad\\qquad\\qquad s.t. W^TW=I$   \n",
    "       \n",
    "         \n",
    "   上面是从最大重构性方面推导PCA.  \n",
    "       \n",
    "   <br>  \n",
    "   <br>\n",
    "       \n",
    "   从最大可分性推导PCA：  \n",
    "     \n",
    "   协方差:  \n",
    "   是衡量两个变量同时变化的变化程度。协方差大于0表示x和y若一个增，另一个也增；小于0表示一个增，一个减。如果ｘ和ｙ是统计独立的，那么二者之间的协方差就是０；但是协方差是０，并不能说明ｘ和ｙ是独立的。协方差绝对值越大，两者对彼此的影响越大，反之越小。协方差是没有单位的量，因此，如果同样的两个变量所采用的量纲发生变化，它们的协方差也会产生树枝上的变化。  \n",
    "     \n",
    "  选取波动最大的k个维度，作为主成分（最好的k维特征是将n维样本点转换为k维后，每一维上的样本方差都很大。按照方差大的目标求解）。  \n",
    "    \n",
    "  \n",
    "  给定一组样本$X=\\{x_1,x_2,\\cdots,x_m\\}$  \n",
    "    \n",
    "  数据中心为：$u=\\sum_{i=1}^m x_i$  \n",
    "    \n",
    "  样本去中心化:$x_i \\leftarrow x_i -u$  \n",
    "    \n",
    "  样本在主成分方向上的投影的绝对值之和越大，代表样本在这个主成分方向的方差越大；方差越大主成分方向就是我们要求的主成分；  \n",
    "    \n",
    "  目标是最大化投影之和：$max \\;\\; \\frac{1}{m}\\sum_{i=1}^m |x_i\\cdot m_j|\\;\\;\\Leftrightarrow\\;\\;max \\;\\;\\frac{1}{m}\\sum_{i=1}^m (x_i\\cdot m_j)^2$  \n",
    "    \n",
    "  向量投影;$x_i=(x^1,x^2,\\cdots,x^d) \\;\\;在 \\quad w_j=(w^1,w^2,\\cdots,w^d)上的投影即：x_i与w_j的点积\\quad x_i\\cdot w_j=x_i^Tw_j$    \n",
    "    \n",
    "  所以最大化的目标是：$max \\;\\;\\frac{1}{m}\\sum_{i=1}^m (x_i^T m_j)^2$  \n",
    "    \n",
    "    \n",
    "  即为：$max \\;\\;\\frac{1}{m}\\sum_{i=1}^m (m_j^Tx_i x_i^T m_j)\\;\\;=\\;\\;\\frac{1}{m} m_j^T(\\sum_{i=1}^mx_i x_i^T) m_j\\;\\;=\\;\\;\\frac{1}{m} m_j^TX^TX m_j$  \n",
    "    \n",
    "  最后求就是 $\\underset{m_j}{max}\\;\\; \\frac{1}{m} m_j^TX^TX m_j$ 且受约束的条件是标准基向量$\\;\\; s.t. m_j^Tm_j=1 \\;\\;(|m_j|=1)$  \n",
    "    \n",
    "  使用拉格朗日乘子法：$2X^TXm_j+\\lambda m_j=0 \\qquad\\;\\; X^TXm_j=\\lambda_j m_j$  \n",
    "    \n",
    "  所以只需要对$X^TX$进行特征值分解，将求得特征值排序$\\lambda_1 \\geq \\lambda_1 \\geq \\cdots \\geq \\lambda_d$   \n",
    "    \n",
    "  我们只要选取我们指定前k个特征值对应的特征向量，就是我们的正交坐标转换系。\n",
    "  \n",
    "    \n",
    " <br>  \n",
    " <br>  \n",
    "   \n",
    "#### 核化线性降维  \n",
    "线性降维方法假设从高维空间到低维空间的函数映射是线性的。然而在不少现实任务中，可能需要非线性映射才能找到恰当的低维嵌入。  \n",
    "如周老师书上提供的例子：线性降维失去原有结构  \n",
    "![](../../img/Pattern_recognition/dr-ml/drml7.PNG)  \n",
    "这时候要使用非线性降维：  \n",
    "\n",
    "非线性降维的一种常用方法是：基于核技巧对线性降维方法进行核化。  \n",
    "  \n",
    "核主成分分析（KPCA）:  \n",
    "  \n",
    "   **从图a使用线性降维到图c失去数据点的有用结构；此时用线性降维显然不行。使用非线性降维可以保持数据结构降到低维**。  \n",
    "       \n",
    "  <br>    \n",
    "     \n",
    "  **采用核思想：把图a数据升到某个更高维的空间；在这个更高维的空间进行线性降维，能保持原有的数据结构**。\n",
    "      \n",
    "   <br>\n",
    "      \n",
    " 1、首先将样本点投到更高维的空间;$z_i$为原始空间$x_i$在高维的观察点；  \n",
    "   \n",
    " 2、将在这个更高维空间的数据投影到低维的$W_{d\\times d'}$的超平面上。（在这个更高维空间上做PCA）  则超平面上的主成分方向：$(\\sum_{i=1}^mz_iz_i^T)w_j=\\lambda_jw_j$  \n",
    "   \n",
    "3、$w_j=\\sum_{i=1}^m z_i\\frac{z_i^Tw_j}{\\lambda_j}\\;\\;=\\;\\;\\sum_{i=1}^m z_i\\alpha_i^j \\qquad \\alpha_i^j$为样本zi在主成分方向的投影 \n",
    "  \n",
    "4、利用核思想$x_i$在更高维的投影$z_i=\\phi(x_i)$;核函数为:$\\kappa(x_i,x_j)=\\phi(x_i)^T\\phi(x_j)$;那么主成分方向$w_j$为：$w_j=\\sum_{i=1}^m \\phi(x_i)\\alpha_i^j $  \n",
    "  \n",
    "5、$w_j=\\sum_{i=1}^m \\phi(x_i)\\alpha_i^j \\quad$ 代入：$\\quad (\\sum_{i=1}^mz_iz_i^T)w_j=\\lambda_jw_j$  \n",
    "  \n",
    "6、$\\sum_{i=1}^m\\phi(x_i)\\phi(x_i)^T\\sum_{i=1}^m \\phi(x_i)\\alpha_i^j = \\lambda_j\\sum_{i=1}^m \\phi(x_i)\\alpha_i^j $  \n",
    "  \n",
    "    \n",
    "7、$\\sum_{i=1}^m\\phi(x_i)\\sum_{i=1}^m\\phi(x_i)^T\\phi(x_i)\\alpha_i^j=\\lambda_j\\sum_{i=1}^m \\phi(x_i)\\alpha_i^j $  \n",
    "  \n",
    "8、$\\sum_{i=1}^m\\phi(x_i)K\\alpha^j=\\lambda_j \\alpha^j \\sum_{i=1}^m \\phi(x_i)$  \n",
    "  \n",
    "9、$K\\alpha^j=\\lambda_j \\alpha^j $  \n",
    "  \n",
    "对核矩阵做特征值分解；取前d’个最大特征值对应的特征向量就可以了($\\alpha_j$)。  \n",
    "  \n",
    "10、解出$\\alpha_j \\quad $；就可以求出样本在低维j上的坐标$\\Rightarrow \\quad x'_j=w_j^T\\phi(x_i)=(\\sum_{i=1}^m \\phi(x_i)\\alpha_i^j)^T \\phi(x_i)=\\sum_{i=1}^m \\alpha_i^j\\phi(x_i)^T \\phi(x_i)=\\sum_{i=1}^m\\alpha_i^j \\kappa(x_i,x)$   \n",
    "  \n",
    "    \n",
    "      \n",
    " <br>  \n",
    " <br>\n",
    " <br>\n",
    "   \n",
    "####  流行学习（流行降维）\n",
    "流行：一般维数越高欧式距离计算越不可靠；而流行就是指在高维空间中数据局部属性与欧式距离同坯的空间。换言之：在这个局部空间具有欧式空间的性质，能用欧式距离来进行计算。   \n",
    "  \n",
    "<br>  \n",
    "\n",
    "流行在降维中用法：  \n",
    "  \n",
    "* 等度量映射（Isomap）：  \n",
    "    测地线距离：不脱离面的曲面的最近距离。\n",
    "    ![](../../img/Pattern_recognition/dr-ml/drml9.PNG)\n",
    "\n",
    "    计算测地线的距离：利用流行在局部空间和欧式距离同坯。对每个点基于欧式距离找到邻近点，建立一个近邻连接图。  \n",
    "\n",
    "    算法流程：  \n",
    "    1、对每个样本在原始空间中找到K个近邻样本（此时距离设置的欧式距离）；那么在原始空间中所有样本之间就存在拓扑关系图  \n",
    "      \n",
    "    2、每个样本都有K个近邻；此时在原始空间中对任意两个样本之间$(x_i,x_j)$，根据求得的近邻关系图；调用Dijstra或Floyd算法，就可以求得，这两个样本点之间的最短路径；（路径上的欧式距离之和就是这两个点在原始高维空间之中的测地线距离）  \n",
    "      \n",
    "     3、利用以上两步就可以求出在原始空间中；样本之间的测地线距离矩阵$dist(x_i,x_j) \\rightarrow D$ ;利用多维缩放算法能够保持原有距离不变，进行降维；就可以得到降维后的样本了  \n",
    "       \n",
    "    使用到流行的地方就是求k个近邻的时候；近邻局部使用同坯欧式空间的性质；直接使用欧式距离公式。       \n",
    "    ![](../../img/Pattern_recognition/dr-ml/drml8.PNG)  \n",
    "      \n",
    "     问题：1、Isomap仅能得到训练样本的降维坐标；对于一个新的样本如何得到降维坐标。 2、k个近邻样本，指定K个样本可能包含较远样本，指定距离阈值可能造成区域不连接  \n",
    "       \n",
    "         \n",
    " <br>  \n",
    " \n",
    " * 局部线性嵌入(LLE):\n",
    "   \n",
    "   不再保持样本领域的测地线距离；而是想保持样本领域的线性关系。\n",
    "     \n",
    "\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
