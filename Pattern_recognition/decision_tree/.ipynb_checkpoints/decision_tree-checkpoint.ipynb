{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树  \n",
    "#### 数值特征与非数值特征  \n",
    "特征是数值特征可以计算距离。特征为非数值的特征难以比较大小。还有一些区间特征。虽然可以把非数值特征，数值化但这样会带来一些其他问题，如信息损失，增加维数等问题，并不一定好。而决策树可以处理非数值特征，也可以处理特征数值。  \n",
    "  \n",
    "    \n",
    " 树的每一个节点都是一个特征：   \n",
    "![](../../img/Pattern_recognition/decision_tree/dt1.PNG)  \n",
    "  \n",
    "   \n",
    "    \n",
    "##  划分选择  \n",
    "在建树的过程中，每个节点该选择什么的属性，才是最优的呢？  \n",
    "  \n",
    "#### 1、信息增益   \n",
    "  \n",
    "    \n",
    "  信息熵：在信源中，考虑的不是某一单个符号发生的不确定性，而是要考虑这个信源所有可能发生情况的平均不确定性。若信源符号有n种取值：U1…Ui…Un，对应概率为：P1…Pi…Pn，且各种符号的出现彼此独立。这时，信源的平均不确定性应当为单个符号不确定性-logPi的统计平均值（E），可称为信息熵，即（信息熵越小则表明概率越大，概率越大可能发生，则信息量越多）  \n",
    "$$H(u)=E(-log_2p_i)=-\\sum_{i=1}^np_ilog_2p_i$$   \n",
    "  \n",
    "    \n",
    " <br>  \n",
    "   \n",
    "所以假设一个训练样本集合$D$中,第$k$类样本所占比例为$p_k\\;\\;\\;(k=1,2,\\cdots,|n|)$ ,则D的信息熵为：  \n",
    "$$Ent(D)=E(-log_2p_i)=-\\sum_{i=1}^n p_ilog_2p_i$$  \n",
    "  \n",
    "    \n",
    "      \n",
    "<br>  \n",
    " \n",
    " 有了样本集合的信息熵后，在对当前样本集合进行划分节点的属性选择时，可以根据选择属性后的信息熵之和，与原信息熵的差值的大小来选择划分的属性。  \n",
    "   <br>\n",
    "     \n",
    "   如样本集合D,的信息熵为$Ent(D)$,假如某一个离散属性为a,a取值为${a_1,a_2,\\cdots,a^v}$ 则划分后的信息熵之和为($|D^i|$为i类样本集合的数量)$$\\sum_{i=1}^v\\frac{|D^i|}{|D|}Ent(D^i)$$   \n",
    "     \n",
    "   <br>  \n",
    " 如此集合D，在选择属性a后的信息增益为：  \n",
    " $$Gain(D,a)=Ent(D)-\\sum_{i=1}^v\\frac{|D^i|}{|D|}Ent(D^i)$$  \n",
    "   \n",
    "     \n",
    " <br>  \n",
    " \n",
    " 信息增益：属性a的信息增益越大，代表属性a的信息熵之和越小，而属性a的信息熵之和越小，代表划分后集合纯度越高。所以在划分节点的属性选择上，选择信息增益最大的那个属性。\n",
    "         \n",
    "           \n",
    "             \n",
    " 注意：在划分节点属性选择时，沿途出现过的属性不再选择，没出现过的可以选择。  \n",
    "   \n",
    "     \n",
    " #### 2、增益率   \n",
    "   \n",
    "   从信息增益可以看出，使用信息增益时，节点属性的选择偏向于属性取值较多的属性。当属性取值较多时信息增益较高（比如把编号也当作属性选择），但这种偏好于取值较多的属性，会导致过拟合。  \n",
    "     \n",
    "   \n",
    "   $$Gain(D,a)=Ent(D)-\\sum_{i=1}^v\\frac{|D^i|}{|D|}Ent(D^i)$$  \n",
    "     \n",
    "   $$IV(a)=-\\sum_{i=1}^v\\frac{|D^i|}{|D|}log_2\\frac{|D^i|}{|D|}$$  \n",
    "     \n",
    "     \n",
    "   信息增益：\n",
    "  $$Gain_ratio(D,a)=\\frac{Gain(D,a)}{IV(a)}$$  \n",
    "    \n",
    "      \n",
    "   属性a的取值越多IV(a)值可能越大，增益率越小；但同时也反应增益率可能对属性取值较少属性有所偏好。  \n",
    "     \n",
    "       \n",
    "   所以使用增益率时：先从侯选划分属性种找出信息增益高于平均水平的属性。再从中选择增益率最高的属性。  \n",
    "       \n",
    "       \n",
    "#### 3、基尼指数   \n",
    "样本集合的纯度使用基尼指数来度量：  \n",
    "\n",
    " $$ Gini(D)= \\sum_{k=1}^n \\sum_{k'\\neq k}^n p^k p^{k'}$$  \n",
    " $$\\qquad\\quad =1-\\sum_{k=1}^n {(p^k)}^2$$  \n",
    "   \n",
    "   基尼指数反应了从数据集D中随机抽取两个样本，，其类被标记不一样的概率，因此Gini(D)越小，数据集D纯度越大。  \n",
    "     \n",
    "       \n",
    "   所以划分节点的属性a的基尼指数为(选择基尼指数最小的属性)：  \n",
    "    $$ Gini(D,a)= \\sum_{i=1}^n\\frac{|D^v|}{|D|}Gini(D^i)$$  \n",
    "      \n",
    "        \n",
    "<br>  \n",
    "\n",
    "### 剪枝处理  \n",
    "\n",
    "决策树在建树的过程中某些属性不断的重复，且为了更好的学的训练的样本而导致分支过多，这些都导致了决策树过拟合。防止决策树过拟合的手段之一是减枝处理，防止决策树过分依赖某些属性。  \n",
    "    \n",
    "      \n",
    "#### 1、预剪枝\n",
    "指在决策树生成的过程中，对每个节点的划分前后进行估计，若当前节点的划分不能带来决策树泛化性能的提升，则停止划分并将当前节点作为叶节点  \n",
    "![](../../img/Pattern_recognition/decision_tree/dt2.PNG)   \n",
    "  \n",
    "    \n",
    "      \n",
    "#### 2、后剪枝\n",
    "指先生成决策树，然后对非叶节点进行后减枝，若泛化性能提升，则进行后减枝。\n",
    "![](../../img/Pattern_recognition/decision_tree/dt3.PNG)  \n",
    "  \n",
    "    \n",
    "      \n",
    "### 连续值  \n",
    "若特征的取值是连续的数值，而非离散值时，该怎么处理。  \n",
    "  \n",
    "此时使用连续属性离散化技术：  \n",
    "  \n",
    "  对连续属性a在数据集D中的取值为$a=\\{a_1,a_2,a_3,\\cdots,a_n\\}$,对$t\\in [a_i,a_{i+1})$的任意t取值划分都是$a_i$,所以把两属性的中位数做候选$$T_a=\\{ \\frac{a^i+a^{i+1}}{2}  \\;\\; | 1 \\leq i \\leq n-1\\}$$  \n",
    "    \n",
    "      \n",
    "   处理的方法就是：用一个t值，代表一段区间，那么就成了连续值的表示方法。  \n",
    "     \n",
    "       \n",
    "   那么此时的信息增益就可以用离散化的了。取使信息增益最大的T_a（那段区间的信息增益最大；且把各段区间有当成了后续属性):  \n",
    "     \n",
    "$$Gain(D,a)=\\underset{t \\in T_a}{max}\\;\\; Gain(D,a,t)=\\underset{t \\in T_a}{max} \\;\\;Ent(D)-\\sum_{\\lambda \\in (-,+) } \\frac{|D_t^\\lambda|}{|D|}Ent(D_t^\\lambda)$$\n",
    "        \n",
    "          \n",
    "  注意：与离散属性不同，若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性。   \n",
    "    \n",
    "      \n",
    "<br>  \n",
    "  \n",
    "### 缺失值\n",
    "不完整样本，某些样本特征值缺失。  \n",
    "问题：1、如何在属性值缺失的情况下选择划分属性  \n",
    "$\\qquad\\;$2、给定划分属性，若样本在该属性上取式，如何对样本划分  \n",
    "  \n",
    "    \n",
    "<br>  \n",
    "\n",
    "问题1、解决方案  \n",
    "  \n",
    "    \n",
    "      \n",
    "<br>  \n",
    "\n",
    "### 多变量决策树  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
