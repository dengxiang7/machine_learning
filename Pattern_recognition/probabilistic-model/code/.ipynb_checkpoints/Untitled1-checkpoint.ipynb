{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.60400987e-01 2.83463792e-93 2.39599013e-01]\n",
      " [2.94253976e-01 2.35294072e-01 4.70451952e-01]\n",
      " [5.50137836e-01 2.50239173e-01 1.99622991e-01]]\n",
      "2.976810842392683\n",
      "-638.1797205301901\n",
      "-624.0985087516349\n",
      "2.3273865717720383\n",
      "2.76147681021908\n",
      "3.0053110266868623\n",
      "[[0.46917295 0.18742985 0.3433972 ]\n",
      " [0.39180005 0.43471607 0.17348387]\n",
      " [0.28572537 0.14262985 0.57164478]]\n",
      "-5.429691562839409\n",
      "4.438393464209216\n",
      "-1.7321921088580061\n",
      "-8.5896847077492\n",
      "-0.13740858060890376\n",
      "-6.435273548955698\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# states = [\"box1\", \"box2\", \"box3\"]\n",
    "# n_states = len(states)\n",
    "\n",
    "# observations = [\"A\", \"B\",\"C\",\"D\"]\n",
    "# n_observations = len(observations)\n",
    "\n",
    "# # observations = [\"red\", \"white\"]\n",
    "# # n_observations = len(observations)\n",
    "\n",
    "# model=hmm.MultinomialHMM(n_components=n_states,n_iter=50000, tol=0.00001)\n",
    "# # 三个观测序列，用来训练\n",
    "# # X = np.array([[0,1,0,1],[0,0,0,1],[1,0,1,1]])\n",
    "# X = [[0,0,1,1,2,2,3,3],[0,1,1,2,1,1,3,3],[0,2,1,2,1,2,3],[0,3],[0,2,1,2,1,0,1,2,3,3],[1,0,1,0,0,3,3,3],[1,0,1,2,3,2,2],[0,1,3,1,1,2,2,3,3],[0,1,0,0,0,2,3,2,2,3],[0,1,3]]\n",
    "\n",
    "# for i in X:\n",
    "#     model.fit([i])\n",
    "\n",
    "# print(model.startprob_)\n",
    "# print(model.transmat_)\n",
    "# print(model.emissionprob_)\n",
    "\n",
    "#print(model.score(X))\n",
    "\n",
    "# seen = np.array([[0,0,1,1,2,2,3,3]]).T\n",
    "# print (model.score(seen))\n",
    "\n",
    "# 初始化概率\n",
    "start_probability=np.array([1,1,1])\n",
    "# 转移概率\n",
    "transition_probability = np.array([\n",
    "  [1,1,1],\n",
    " [1,1,1],\n",
    " [1,1,1]\n",
    "])\n",
    "\n",
    "start_probability1=np.array([0.5,0.5,0])\n",
    "# 转移概率\n",
    "transition_probability1 = np.array([\n",
    "  [0.5,0,0.5],\n",
    " [0,0.5,0.5],\n",
    " [0.5,0.5,0]\n",
    "])\n",
    "\n",
    "#发射状态概率\n",
    "emission_probability = np.array([\n",
    "  [0,  0.5, 0,  0.5],\n",
    " [1,  0, 0, 0 ],\n",
    " [1,  0, 0,  0 ]\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "model = hmm.GaussianHMM(n_components=3,  n_iter=1000000, tol=0.00001,covariance_type=\"full\")\n",
    "model.\n",
    "X1 = np.array([[0],[0],[1],[1],[2],[2],[3],[3]])\n",
    "X2 = np.array([[0],[1],[1],[2],[1],[1],[3],[3]])\n",
    "X3 = np.array([[0],[2],[1],[2],[1],[2],[3]])\n",
    "X4 = np.array([[0], [3]])\n",
    "X5 = np.array([[0],[2],[1],[2],[1],[0],[1],[2],[3],[3]])\n",
    "X6 = np.array([[1],[0],[1],[0],[0],[3],[3],[3]])\n",
    "X7 = np.array([[1],[0],[1],[2],[3],[2],[2]])\n",
    "X8 = np.array([[0],[1],[3],[1],[1],[2],[2],[3],[3]])\n",
    "X9 = np.array([[0],[1],[0],[0],[0],[2],[3],[2],[2],[3]])\n",
    "X10 = np.array([[0],[1],[3]])\n",
    "\n",
    "X = np.vstack((X1,X2,X3,X4,X5,X6,X7,X8,X9,X10))\n",
    "\n",
    "X_lens = [8,8,7,2,10,8,7,9,10,3]\n",
    "model.fit(X,X_lens)\n",
    "\n",
    "\n",
    "print(model.transmat_)\n",
    "\n",
    "seen1 =np.array([[0,1,1,1,2,3,3,3]]).T\n",
    "seen2 =np.array([[3,0,3,1,2,1,0,0]]).T\n",
    "seen3 =np.array([[2,3,2,1,0,1,0]]).T\n",
    "seen4 =np.array([[0,3,1,1,1,2,3]]).T\n",
    "\n",
    "seen5=np.array([[1,0,3,1,3,2,1,0]]).T\n",
    "seen6=np.array([[0,0,1,1,2,2,3,3]]).T\n",
    "\n",
    "print (model.score(seen1))\n",
    "print (model.score(seen2))\n",
    "print (model.score(seen3))\n",
    "print (model.score(seen4))\n",
    "print (model.score(seen5))\n",
    "print (model.score(seen6))\n",
    "\n",
    "model2 = hmm.GaussianHMM(n_components=3,n_iter=1000000, tol=0.00001,covariance_type=\"full\")\n",
    "Y1 = np.array([[3],[3],[2],[2],[1],[1],[0],[0]])\n",
    "Y2 = np.array([[3],[3],[0],[1],[2],[1],[0]])\n",
    "Y3 = np.array([[2],[3],[2],[3],[2],[1],[0],[1],[0]])\n",
    "Y4 = np.array([[3], [3],[1],[1],[0]])\n",
    "Y5 = np.array([[3],[0],[3],[0],[2],[1],[1],[0],[0]])\n",
    "Y6 = np.array([[2],[3],[3],[2],[2],[1],[0]])\n",
    "Y7 = np.array([[1],[3],[3],[1],[2],[0],[0],[0],[0]])\n",
    "Y8 = np.array([[1],[1],[0],[1],[1],[3],[3],[3],[2],[3]])\n",
    "Y9 = np.array([[3],[3],[0],[3],[3],[1],[2],[0],[0]])\n",
    "Y10 = np.array([[3],[3],[2],[0],[0],[0]])\n",
    "\n",
    "\n",
    "Y = np.vstack((Y1,Y2,Y3,Y4,Y5,Y6,Y7,Y8,Y9,Y10))\n",
    "\n",
    "Y_lens = [8,7,9,5,9,7,9,10,9,6]\n",
    "model2.fit(Y,Y_lens)\n",
    "print(model2.transmat_)\n",
    "\n",
    "print (model2.score(seen1))\n",
    "print (model2.score(seen2))\n",
    "print (model2.score(seen3))\n",
    "print (model2.score(seen4))\n",
    "print (model2.score(seen5))\n",
    "print (model2.score(seen6))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004516580942612666\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(-5.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module hmmlearn.hmm in hmmlearn:\n",
      "\n",
      "NAME\n",
      "    hmmlearn.hmm - The :mod:`hmmlearn.hmm` module implements hidden Markov models.\n",
      "\n",
      "CLASSES\n",
      "    hmmlearn.base._BaseHMM(sklearn.base.BaseEstimator)\n",
      "        GMMHMM\n",
      "        GaussianHMM\n",
      "        MultinomialHMM\n",
      "    \n",
      "    class GMMHMM(hmmlearn.base._BaseHMM)\n",
      "     |  Hidden Markov Model with Gaussian mixture emissions.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_components : int\n",
      "     |      Number of states in the model.\n",
      "     |  \n",
      "     |  n_mix : int\n",
      "     |      Number of states in the GMM.\n",
      "     |  \n",
      "     |  covariance_type : string, optional\n",
      "     |      String describing the type of covariance parameters to\n",
      "     |      use.  Must be one of\n",
      "     |  \n",
      "     |      * \"spherical\" --- each state uses a single variance value that\n",
      "     |        applies to all features.\n",
      "     |      * \"diag\" --- each state uses a diagonal covariance matrix.\n",
      "     |      * \"full\" --- each state uses a full (i.e. unrestricted)\n",
      "     |        covariance matrix.\n",
      "     |      * \"tied\" --- all mixture components of each state use **the same** full\n",
      "     |        covariance matrix (note that this is not the same as for\n",
      "     |        `GaussianHMM`).\n",
      "     |  \n",
      "     |      Defaults to \"diag\".\n",
      "     |  \n",
      "     |  min_covar : float, optional\n",
      "     |      Floor on the diagonal of the covariance matrix to prevent\n",
      "     |      overfitting. Defaults to 1e-3.\n",
      "     |  \n",
      "     |  startprob_prior : array, shape (n_components, ), optional\n",
      "     |      Parameters of the Dirichlet prior distribution for\n",
      "     |      :attr:`startprob_`.\n",
      "     |  \n",
      "     |  transmat_prior : array, shape (n_components, n_components), optional\n",
      "     |      Parameters of the Dirichlet prior distribution for each row\n",
      "     |      of the transition probabilities :attr:`transmat_`.\n",
      "     |  \n",
      "     |  weights_prior : array, shape (n_mix, ), optional\n",
      "     |      Parameters of the Dirichlet prior distribution for\n",
      "     |      :attr:`weights_`.\n",
      "     |  \n",
      "     |  means_prior, means_weight : array, shape (n_mix, ), optional\n",
      "     |      Mean and precision of the Normal prior distribtion for\n",
      "     |      :attr:`means_`.\n",
      "     |  \n",
      "     |  covars_prior, covars_weight : array, shape (n_mix, ), optional\n",
      "     |      Parameters of the prior distribution for the covariance matrix\n",
      "     |      :attr:`covars_`.\n",
      "     |  \n",
      "     |      If :attr:`covariance_type` is \"spherical\" or \"diag\" the prior is\n",
      "     |      the inverse gamma distribution, otherwise --- the inverse Wishart\n",
      "     |      distribution.\n",
      "     |  \n",
      "     |  algorithm : string, optional\n",
      "     |      Decoder algorithm. Must be one of \"viterbi\" or \"map\".\n",
      "     |      Defaults to \"viterbi\".\n",
      "     |  \n",
      "     |  random_state: RandomState or an int seed, optional\n",
      "     |      A random number generator instance.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Convergence threshold. EM will stop if the gain in log-likelihood\n",
      "     |      is below this value.\n",
      "     |  \n",
      "     |  verbose : bool, optional\n",
      "     |      When ``True`` per-iteration convergence reports are printed\n",
      "     |      to :data:`sys.stderr`. You can diagnose convergence via the\n",
      "     |      :attr:`monitor_` attribute.\n",
      "     |  \n",
      "     |  init_params : string, optional\n",
      "     |      Controls which parameters are initialized prior to training. Can\n",
      "     |      contain any combination of 's' for startprob, 't' for transmat, 'm'\n",
      "     |      for means, 'c' for covars, and 'w' for GMM mixing weights.\n",
      "     |      Defaults to all parameters.\n",
      "     |  \n",
      "     |  params : string, optional\n",
      "     |      Controls which parameters are updated in the training process.  Can\n",
      "     |      contain any combination of 's' for startprob, 't' for transmat, 'm' for\n",
      "     |      means, and 'c' for covars, and 'w' for GMM mixing weights.\n",
      "     |      Defaults to all parameters.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  monitor\\_ : ConvergenceMonitor\n",
      "     |      Monitor object used to check the convergence of EM.\n",
      "     |  \n",
      "     |  startprob\\_ : array, shape (n_components, )\n",
      "     |      Initial state occupation distribution.\n",
      "     |  \n",
      "     |  transmat\\_ : array, shape (n_components, n_components)\n",
      "     |      Matrix of transition probabilities between states.\n",
      "     |  \n",
      "     |  weights\\_ : array, shape (n_components, n_mix)\n",
      "     |      Mixture weights for each state.\n",
      "     |  \n",
      "     |  means\\_ : array, shape (n_components, n_mix)\n",
      "     |      Mean parameters for each mixture component in each state.\n",
      "     |  \n",
      "     |  covars\\_ : array\n",
      "     |      Covariance parameters for each mixture components in each state.\n",
      "     |  \n",
      "     |      The shape depends on :attr:`covariance_type`::\n",
      "     |  \n",
      "     |          (n_components, n_mix)                          if \"spherical\",\n",
      "     |          (n_components, n_mix, n_features)              if \"diag\",\n",
      "     |          (n_components, n_mix, n_features, n_features)  if \"full\"\n",
      "     |          (n_components, n_features, n_features)         if \"tied\",\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GMMHMM\n",
      "     |      hmmlearn.base._BaseHMM\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_components=1, n_mix=1, min_covar=0.001, startprob_prior=1.0, transmat_prior=1.0, weights_prior=1.0, means_prior=0.0, means_weight=0.0, covars_prior=None, covars_weight=None, algorithm='viterbi', covariance_type='diag', random_state=None, n_iter=10, tol=0.01, verbose=False, params='stmcw', init_params='stmcw')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from hmmlearn.base._BaseHMM:\n",
      "     |  \n",
      "     |  decode(self, X, lengths=None, algorithm=None)\n",
      "     |      Find most likely state sequence corresponding to ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      algorithm : string\n",
      "     |          Decoder algorithm. Must be one of \"viterbi\" or \"map\".\n",
      "     |          If not given, :attr:`decoder` is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log probability of the produced state sequence.\n",
      "     |      \n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          Labels for each sample from ``X`` obtained via a given\n",
      "     |          decoder ``algorithm``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score_samples : Compute the log probability under the model and\n",
      "     |          posteriors.\n",
      "     |      score : Compute the log probability under the model.\n",
      "     |  \n",
      "     |  fit(self, X, lengths=None)\n",
      "     |      Estimate model parameters.\n",
      "     |      \n",
      "     |      An initialization step is performed before entering the\n",
      "     |      EM algorithm. If you want to avoid this step for a subset of\n",
      "     |      the parameters, pass proper ``init_params`` keyword argument\n",
      "     |      to estimator's constructor.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, )\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  get_stationary_distribution(self)\n",
      "     |      Compute the stationary distribution of states.\n",
      "     |  \n",
      "     |  predict(self, X, lengths=None)\n",
      "     |      Find most likely state sequence corresponding to ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          Labels for each sample from ``X``.\n",
      "     |  \n",
      "     |  predict_proba(self, X, lengths=None)\n",
      "     |      Compute the posterior probability for each state in the model.\n",
      "     |      \n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      posteriors : array, shape (n_samples, n_components)\n",
      "     |          State-membership probabilities for each sample from ``X``.\n",
      "     |  \n",
      "     |  sample(self, n_samples=1, random_state=None)\n",
      "     |      Generate random samples from the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n_samples : int\n",
      "     |          Number of samples to generate.\n",
      "     |      \n",
      "     |      random_state : RandomState or an int seed\n",
      "     |          A random number generator instance. If ``None``, the object's\n",
      "     |          ``random_state`` is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array, shape (n_samples, n_features)\n",
      "     |          Feature matrix.\n",
      "     |      \n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          State sequence produced by the model.\n",
      "     |  \n",
      "     |  score(self, X, lengths=None)\n",
      "     |      Compute the log probability under the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log likelihood of ``X``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score_samples : Compute the log probability under the model and\n",
      "     |          posteriors.\n",
      "     |      decode : Find most likely state sequence corresponding to ``X``.\n",
      "     |  \n",
      "     |  score_samples(self, X, lengths=None)\n",
      "     |      Compute the log probability under the model and compute posteriors.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log likelihood of ``X``.\n",
      "     |      \n",
      "     |      posteriors : array, shape (n_samples, n_components)\n",
      "     |          State-membership probabilities for each sample in ``X``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score : Compute the log probability under the model.\n",
      "     |      decode : Find most likely state sequence corresponding to ``X``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class GaussianHMM(hmmlearn.base._BaseHMM)\n",
      "     |  Hidden Markov Model with Gaussian emissions.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  n_components : int\n",
      "     |      Number of states.\n",
      "     |  \n",
      "     |  covariance_type : string, optional\n",
      "     |      String describing the type of covariance parameters to\n",
      "     |      use.  Must be one of\n",
      "     |  \n",
      "     |      * \"spherical\" --- each state uses a single variance value that\n",
      "     |        applies to all features.\n",
      "     |      * \"diag\" --- each state uses a diagonal covariance matrix.\n",
      "     |      * \"full\" --- each state uses a full (i.e. unrestricted)\n",
      "     |        covariance matrix.\n",
      "     |      * \"tied\" --- all states use **the same** full covariance matrix.\n",
      "     |  \n",
      "     |      Defaults to \"diag\".\n",
      "     |  \n",
      "     |  min_covar : float, optional\n",
      "     |      Floor on the diagonal of the covariance matrix to prevent\n",
      "     |      overfitting. Defaults to 1e-3.\n",
      "     |  \n",
      "     |  startprob_prior : array, shape (n_components, ), optional\n",
      "     |      Parameters of the Dirichlet prior distribution for\n",
      "     |      :attr:`startprob_`.\n",
      "     |  \n",
      "     |  transmat_prior : array, shape (n_components, n_components), optional\n",
      "     |      Parameters of the Dirichlet prior distribution for each row\n",
      "     |      of the transition probabilities :attr:`transmat_`.\n",
      "     |  \n",
      "     |  means_prior, means_weight : array, shape (n_components, ), optional\n",
      "     |      Mean and precision of the Normal prior distribtion for\n",
      "     |      :attr:`means_`.\n",
      "     |  \n",
      "     |  covars_prior, covars_weight : array, shape (n_components, ), optional\n",
      "     |      Parameters of the prior distribution for the covariance matrix\n",
      "     |      :attr:`covars_`.\n",
      "     |  \n",
      "     |      If :attr:`covariance_type` is \"spherical\" or \"diag\" the prior is\n",
      "     |      the inverse gamma distribution, otherwise --- the inverse Wishart\n",
      "     |      distribution.\n",
      "     |  \n",
      "     |  algorithm : string, optional\n",
      "     |      Decoder algorithm. Must be one of \"viterbi\" or`\"map\".\n",
      "     |      Defaults to \"viterbi\".\n",
      "     |  \n",
      "     |  random_state: RandomState or an int seed, optional\n",
      "     |      A random number generator instance.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Convergence threshold. EM will stop if the gain in log-likelihood\n",
      "     |      is below this value.\n",
      "     |  \n",
      "     |  verbose : bool, optional\n",
      "     |      When ``True`` per-iteration convergence reports are printed\n",
      "     |      to :data:`sys.stderr`. You can diagnose convergence via the\n",
      "     |      :attr:`monitor_` attribute.\n",
      "     |  \n",
      "     |  params : string, optional\n",
      "     |      Controls which parameters are updated in the training\n",
      "     |      process.  Can contain any combination of 's' for startprob,\n",
      "     |      't' for transmat, 'm' for means and 'c' for covars. Defaults\n",
      "     |      to all parameters.\n",
      "     |  \n",
      "     |  init_params : string, optional\n",
      "     |      Controls which parameters are initialized prior to\n",
      "     |      training.  Can contain any combination of 's' for\n",
      "     |      startprob, 't' for transmat, 'm' for means and 'c' for covars.\n",
      "     |      Defaults to all parameters.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_features : int\n",
      "     |      Dimensionality of the Gaussian emissions.\n",
      "     |  \n",
      "     |  monitor\\_ : ConvergenceMonitor\n",
      "     |      Monitor object used to check the convergence of EM.\n",
      "     |  \n",
      "     |  startprob\\_ : array, shape (n_components, )\n",
      "     |      Initial state occupation distribution.\n",
      "     |  \n",
      "     |  transmat\\_ : array, shape (n_components, n_components)\n",
      "     |      Matrix of transition probabilities between states.\n",
      "     |  \n",
      "     |  means\\_ : array, shape (n_components, n_features)\n",
      "     |      Mean parameters for each state.\n",
      "     |  \n",
      "     |  covars\\_ : array\n",
      "     |      Covariance parameters for each state.\n",
      "     |  \n",
      "     |      The shape depends on :attr:`covariance_type`::\n",
      "     |  \n",
      "     |          (n_components, )                        if \"spherical\",\n",
      "     |          (n_components, n_features)              if \"diag\",\n",
      "     |          (n_components, n_features, n_features)  if \"full\"\n",
      "     |          (n_features, n_features)                if \"tied\",\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from hmmlearn.hmm import GaussianHMM\n",
      "     |  >>> GaussianHMM(n_components=2)  #doctest: +ELLIPSIS\n",
      "     |  GaussianHMM(algorithm='viterbi',...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GaussianHMM\n",
      "     |      hmmlearn.base._BaseHMM\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_components=1, covariance_type='diag', min_covar=0.001, startprob_prior=1.0, transmat_prior=1.0, means_prior=0, means_weight=0, covars_prior=0.01, covars_weight=1, algorithm='viterbi', random_state=None, n_iter=10, tol=0.01, verbose=False, params='stmc', init_params='stmc')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  covars_\n",
      "     |      Return covars as a full matrix.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from hmmlearn.base._BaseHMM:\n",
      "     |  \n",
      "     |  decode(self, X, lengths=None, algorithm=None)\n",
      "     |      Find most likely state sequence corresponding to ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      algorithm : string\n",
      "     |          Decoder algorithm. Must be one of \"viterbi\" or \"map\".\n",
      "     |          If not given, :attr:`decoder` is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log probability of the produced state sequence.\n",
      "     |      \n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          Labels for each sample from ``X`` obtained via a given\n",
      "     |          decoder ``algorithm``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score_samples : Compute the log probability under the model and\n",
      "     |          posteriors.\n",
      "     |      score : Compute the log probability under the model.\n",
      "     |  \n",
      "     |  fit(self, X, lengths=None)\n",
      "     |      Estimate model parameters.\n",
      "     |      \n",
      "     |      An initialization step is performed before entering the\n",
      "     |      EM algorithm. If you want to avoid this step for a subset of\n",
      "     |      the parameters, pass proper ``init_params`` keyword argument\n",
      "     |      to estimator's constructor.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, )\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  get_stationary_distribution(self)\n",
      "     |      Compute the stationary distribution of states.\n",
      "     |  \n",
      "     |  predict(self, X, lengths=None)\n",
      "     |      Find most likely state sequence corresponding to ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          Labels for each sample from ``X``.\n",
      "     |  \n",
      "     |  predict_proba(self, X, lengths=None)\n",
      "     |      Compute the posterior probability for each state in the model.\n",
      "     |      \n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      posteriors : array, shape (n_samples, n_components)\n",
      "     |          State-membership probabilities for each sample from ``X``.\n",
      "     |  \n",
      "     |  sample(self, n_samples=1, random_state=None)\n",
      "     |      Generate random samples from the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n_samples : int\n",
      "     |          Number of samples to generate.\n",
      "     |      \n",
      "     |      random_state : RandomState or an int seed\n",
      "     |          A random number generator instance. If ``None``, the object's\n",
      "     |          ``random_state`` is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array, shape (n_samples, n_features)\n",
      "     |          Feature matrix.\n",
      "     |      \n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          State sequence produced by the model.\n",
      "     |  \n",
      "     |  score(self, X, lengths=None)\n",
      "     |      Compute the log probability under the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log likelihood of ``X``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score_samples : Compute the log probability under the model and\n",
      "     |          posteriors.\n",
      "     |      decode : Find most likely state sequence corresponding to ``X``.\n",
      "     |  \n",
      "     |  score_samples(self, X, lengths=None)\n",
      "     |      Compute the log probability under the model and compute posteriors.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log likelihood of ``X``.\n",
      "     |      \n",
      "     |      posteriors : array, shape (n_samples, n_components)\n",
      "     |          State-membership probabilities for each sample in ``X``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score : Compute the log probability under the model.\n",
      "     |      decode : Find most likely state sequence corresponding to ``X``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MultinomialHMM(hmmlearn.base._BaseHMM)\n",
      "     |  Hidden Markov Model with multinomial (discrete) emissions\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  \n",
      "     |  n_components : int\n",
      "     |      Number of states.\n",
      "     |  \n",
      "     |  startprob_prior : array, shape (n_components, ), optional\n",
      "     |      Parameters of the Dirichlet prior distribution for\n",
      "     |      :attr:`startprob_`.\n",
      "     |  \n",
      "     |  transmat_prior : array, shape (n_components, n_components), optional\n",
      "     |      Parameters of the Dirichlet prior distribution for each row\n",
      "     |      of the transition probabilities :attr:`transmat_`.\n",
      "     |  \n",
      "     |  algorithm : string, optional\n",
      "     |      Decoder algorithm. Must be one of \"viterbi\" or \"map\".\n",
      "     |      Defaults to \"viterbi\".\n",
      "     |  \n",
      "     |  random_state: RandomState or an int seed, optional\n",
      "     |      A random number generator instance.\n",
      "     |  \n",
      "     |  n_iter : int, optional\n",
      "     |      Maximum number of iterations to perform.\n",
      "     |  \n",
      "     |  tol : float, optional\n",
      "     |      Convergence threshold. EM will stop if the gain in log-likelihood\n",
      "     |      is below this value.\n",
      "     |  \n",
      "     |  verbose : bool, optional\n",
      "     |      When ``True`` per-iteration convergence reports are printed\n",
      "     |      to :data:`sys.stderr`. You can diagnose convergence via the\n",
      "     |      :attr:`monitor_` attribute.\n",
      "     |  \n",
      "     |  params : string, optional\n",
      "     |      Controls which parameters are updated in the training\n",
      "     |      process.  Can contain any combination of 's' for startprob,\n",
      "     |      't' for transmat, 'e' for emissionprob.\n",
      "     |      Defaults to all parameters.\n",
      "     |  \n",
      "     |  init_params : string, optional\n",
      "     |      Controls which parameters are initialized prior to\n",
      "     |      training.  Can contain any combination of 's' for\n",
      "     |      startprob, 't' for transmat, 'e' for emissionprob.\n",
      "     |      Defaults to all parameters.\n",
      "     |  \n",
      "     |  Attributes\n",
      "     |  ----------\n",
      "     |  n_features : int\n",
      "     |      Number of possible symbols emitted by the model (in the samples).\n",
      "     |  \n",
      "     |  monitor\\_ : ConvergenceMonitor\n",
      "     |      Monitor object used to check the convergence of EM.\n",
      "     |  \n",
      "     |  startprob\\_ : array, shape (n_components, )\n",
      "     |      Initial state occupation distribution.\n",
      "     |  \n",
      "     |  transmat\\_ : array, shape (n_components, n_components)\n",
      "     |      Matrix of transition probabilities between states.\n",
      "     |  \n",
      "     |  emissionprob\\_ : array, shape (n_components, n_features)\n",
      "     |      Probability of emitting a given symbol when in each state.\n",
      "     |  \n",
      "     |  Examples\n",
      "     |  --------\n",
      "     |  >>> from hmmlearn.hmm import MultinomialHMM\n",
      "     |  >>> MultinomialHMM(n_components=2)  #doctest: +ELLIPSIS\n",
      "     |  MultinomialHMM(algorithm='viterbi',...\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultinomialHMM\n",
      "     |      hmmlearn.base._BaseHMM\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, n_components=1, startprob_prior=1.0, transmat_prior=1.0, algorithm='viterbi', random_state=None, n_iter=10, tol=0.01, verbose=False, params='ste', init_params='ste')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from hmmlearn.base._BaseHMM:\n",
      "     |  \n",
      "     |  decode(self, X, lengths=None, algorithm=None)\n",
      "     |      Find most likely state sequence corresponding to ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      algorithm : string\n",
      "     |          Decoder algorithm. Must be one of \"viterbi\" or \"map\".\n",
      "     |          If not given, :attr:`decoder` is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log probability of the produced state sequence.\n",
      "     |      \n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          Labels for each sample from ``X`` obtained via a given\n",
      "     |          decoder ``algorithm``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score_samples : Compute the log probability under the model and\n",
      "     |          posteriors.\n",
      "     |      score : Compute the log probability under the model.\n",
      "     |  \n",
      "     |  fit(self, X, lengths=None)\n",
      "     |      Estimate model parameters.\n",
      "     |      \n",
      "     |      An initialization step is performed before entering the\n",
      "     |      EM algorithm. If you want to avoid this step for a subset of\n",
      "     |      the parameters, pass proper ``init_params`` keyword argument\n",
      "     |      to estimator's constructor.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, )\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Returns self.\n",
      "     |  \n",
      "     |  get_stationary_distribution(self)\n",
      "     |      Compute the stationary distribution of states.\n",
      "     |  \n",
      "     |  predict(self, X, lengths=None)\n",
      "     |      Find most likely state sequence corresponding to ``X``.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          Labels for each sample from ``X``.\n",
      "     |  \n",
      "     |  predict_proba(self, X, lengths=None)\n",
      "     |      Compute the posterior probability for each state in the model.\n",
      "     |      \n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      posteriors : array, shape (n_samples, n_components)\n",
      "     |          State-membership probabilities for each sample from ``X``.\n",
      "     |  \n",
      "     |  sample(self, n_samples=1, random_state=None)\n",
      "     |      Generate random samples from the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      n_samples : int\n",
      "     |          Number of samples to generate.\n",
      "     |      \n",
      "     |      random_state : RandomState or an int seed\n",
      "     |          A random number generator instance. If ``None``, the object's\n",
      "     |          ``random_state`` is used.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X : array, shape (n_samples, n_features)\n",
      "     |          Feature matrix.\n",
      "     |      \n",
      "     |      state_sequence : array, shape (n_samples, )\n",
      "     |          State sequence produced by the model.\n",
      "     |  \n",
      "     |  score(self, X, lengths=None)\n",
      "     |      Compute the log probability under the model.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log likelihood of ``X``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score_samples : Compute the log probability under the model and\n",
      "     |          posteriors.\n",
      "     |      decode : Find most likely state sequence corresponding to ``X``.\n",
      "     |  \n",
      "     |  score_samples(self, X, lengths=None)\n",
      "     |      Compute the log probability under the model and compute posteriors.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape (n_samples, n_features)\n",
      "     |          Feature matrix of individual samples.\n",
      "     |      \n",
      "     |      lengths : array-like of integers, shape (n_sequences, ), optional\n",
      "     |          Lengths of the individual sequences in ``X``. The sum of\n",
      "     |          these should be ``n_samples``.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      logprob : float\n",
      "     |          Log likelihood of ``X``.\n",
      "     |      \n",
      "     |      posteriors : array, shape (n_samples, n_components)\n",
      "     |          State-membership probabilities for each sample in ``X``.\n",
      "     |      \n",
      "     |      See Also\n",
      "     |      --------\n",
      "     |      score : Compute the log probability under the model.\n",
      "     |      decode : Find most likely state sequence corresponding to ``X``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self, N_CHAR_MAX=700)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  get_params(self, deep=True)\n",
      "     |      Get parameters for this estimator.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      deep : bool, default=True\n",
      "     |          If True, will return the parameters for this estimator and\n",
      "     |          contained subobjects that are estimators.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      params : mapping of string to any\n",
      "     |          Parameter names mapped to their values.\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      \n",
      "     |      The method works on simple estimators as well as on nested objects\n",
      "     |      (such as pipelines). The latter have parameters of the form\n",
      "     |      ``<component>__<parameter>`` so that it's possible to update each\n",
      "     |      component of a nested object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **params : dict\n",
      "     |          Estimator parameters.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self : object\n",
      "     |          Estimator instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['GMMHMM', 'GaussianHMM', 'MultinomialHMM']\n",
      "    __warningregistry__ = {'version': 1218, ('Number of distinct clusters ...\n",
      "\n",
      "FILE\n",
      "    d:\\miniconda\\mnconda\\envs\\gluon\\lib\\site-packages\\hmmlearn\\hmm.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(hmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
