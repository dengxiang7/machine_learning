{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch 损失函数  \n",
    "  \n",
    "把模型的输出视为损失函数的一个输入变量。对这个输入变量做一些操作。形成了损失函数  \n",
    "\n",
    "<br>  \n",
    "  \n",
    "**我们优化的目标就是模型的函数，也就是要求梯度的参数，就是模型里的参数**\n",
    "  \n",
    "<br>  \n",
    "  \n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch 存放变量的对象Variable对象，或tensor对象  \n",
    "  \n",
    "\n",
    "Tensor对象：requires_grad=True 的Tensor对象，是启用了自动求微分系统的张量对象。  \n",
    "  \n",
    "$\\qquad\\quad$这个张量内，会开始形成一个反向图，并跟踪记录所有应用于他们的每个操作，使用所谓的动态图计算张量的梯度。 \n",
    "\n",
    "<br>  \n",
    "  \n",
    "<br>    \n",
    "  \n",
    " Tensor对象启用requires_grad=True和不启用 requires_grad=True的区别。  \n",
    " \n",
    "<br>\n",
    "   \n",
    " 启用requires_grad=True的Tensor对象：  那么这个Tensor里的值就是函数$f(x)$里的变量。这些变量和记录下的操作，一起组成了函数。 \n",
    " \n",
    "<br>  \n",
    "  \n",
    " 不启用 requires_grad=True的 Tensor对象：就是简单的记录数值的数组。\n",
    "   \n",
    "<br>  \n",
    "  \n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch 求导  \n",
    "\n",
    "模型函数：$f_\\theta(x)  \\qquad$ (模型函数视为变量的是输入x的样本值，模型函数视为常量的参数是$\\theta$)\n",
    "\n",
    "<br>   \n",
    " \n",
    "损失函数：$Loss(\\theta)= g(f_\\theta(x))$  (损失函数的变量的是$\\theta$，(是模型函数视为常数变量的$\\theta$))\n",
    "  \n",
    "<br>  \n",
    "  \n",
    "所以pytorch对函数的求导，就是对损失函数里的$\\theta$求导。  \n",
    "  \n",
    "<br>   \n",
    "\n",
    "这里的关键 的流程：对参数$\\theta$的张量进行求导。\n",
    "1、建立模型参数$\\theta$的张量，  \n",
    "2、对模型函数，输入样本x,那么输出的就是记录了所有模型操作的参数张量。  \n",
    "3、对这个张量进行求梯度。也就是对模型参数进行求梯度。\n",
    "\n",
    "\n",
    "  \n",
    " <br> \n",
    "  \n",
    "<br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch优化\n",
    "\n",
    "梯度下降算法：总的损失函数$f(x)=\\frac{1}{n} \\sum_{i=1}^n f_i(x) \\quad$求出的梯度$\\Delta f(x)=\\frac{1}{n} \\sum_{i=1}^n \\Delta f_i(x) \\quad$  进行迭代 $x \\leftarrow x - \\eta \\Delta f(x)$ 。\n",
    "\n",
    "<br>  \n",
    " \n",
    "随机梯度下降算法：迭代是随机选择一个样本损失函数的梯度。$x \\leftarrow x - \\eta \\Delta f_i(x)$  \n",
    "\n",
    "<br>  \n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)   \n",
    "  \n",
    "这里由于背后的底层类都是用c++语言写的扩展类。  这里建立optimizer时，optimizer对象里存放的有model.parameters()，模型参数的指针。\n",
    "\n",
    "  \n",
    "<br>  \n",
    "  \n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用pytorch训练一个模型的流程  \n",
    "\n",
    "关键时：model.parameters()模型参数张量是全局的。\n",
    "\n",
    "1、创建optimizer优化对象。这个对象里有模型参数的指针  \n",
    "  \n",
    "2、样本值x输入模型函数 $f_\\theta(x)  \\qquad$ ，得到的输出是，记录了模型操作的参数$\\theta$张量。  \n",
    "  \n",
    "3、记录了模型操作的参数$\\theta$张量。输入损失函数，得到的是：记录模型操作，损失函数操作，模型要求导的参数，损失函数要求导的参数，包含了这些的张量  \n",
    "  \n",
    "4、对最终的张量进行求梯度。张量.backward()  \n",
    "  \n",
    "5、求出了所有参数的梯度后，调用optimizer.step()进行参数的迭代。（optimizer对象里有模型参数的指针，这个指针，指向就是求梯度的张量） \n",
    "  \n",
    "![](../img/pytorch/1.PNG)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9) \n",
    "loss_fn()\n",
    "for input, target in dataset:\n",
    "    optimizer.zero_grad()  # 清空所有被优化过的Variable的梯度.\n",
    "    output = model(input)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
