{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  优化算法--误差逆传播\n",
    "BP算法一般是针对多层前馈网络的训练算法，当然也可以用于其他的网络。BP网络是用BP训练的多层前馈网络。  \n",
    "  \n",
    "### 思想   \n",
    "链式求导法则：  \n",
    "$$u=g(y) \\quad y=\\phi(x)\\quad 则有\\frac{\\partial u}{\\partial x}= \\frac{\\partial u}{\\partial y} \\frac{\\partial y}{\\partial x}$$\n",
    "<br>\n",
    "$$u=g(y_1,y_2,...,y_m)\\quad y_i=\\phi(x_1,x_2,...,x_n)  \\quad 则有 \\frac{\\partial u}{\\partial x_j}=\\sum_{i=1}^m  \\frac{\\partial u}{\\partial y_i} \\frac{\\partial y_j}{\\partial x_j}$$  \n",
    "BP思想是使用链式求导法：要求$\\frac{\\partial u}{\\partial x}$先求$\\frac{\\partial u}{\\partial y}$  \n",
    "  \n",
    "#### 例子讲BP过程\n",
    "数据：$\\;\\{ (x_1,y_1),(x_2,y_2),\\cdots,(x_n,y_n) \\}\\quad x \\in R^d ,y \\in R^l$  \n",
    "![](../../img/Pattern_recognition/neural_networks/BP2.PNG)  \n",
    "求这个网络的参数：  \n",
    "  \n",
    " 这个网络输出层激活函数使用：$sigmod(x)$函数；样例$(x_k,y_k)$的输出层：$\\hat{y^k}=(\\hat{y}_1^K,\\hat{y}_2^K,\\cdots,\\hat{y}_n^K) \\quad 而 \\hat{y}_i^K =f(\\beta_j-\\theta_j)\\quad $所以样本$(x_k,y_k)$的误差为  \n",
    " $\\qquad\\qquad\\qquad\\qquad\\qquad$（损失函数：采用均方误差） $E_k=\\frac{1}{2}\\sum_{i=1}^l(\\hat{y_j}^K- y_j^k)^2$    \n",
    " 学习参数：使用梯度下降法更新参数;则需要知道误差函数对所有参数的导数。则来了一个样例$(x_k,y_k)$后使用梯度下降法更新所有参数的值，则应知道此次样本的损失函数对各个参数的导数。 \n",
    " \n",
    " <br>\n",
    " $$v是任意参数：\\; v \\Leftarrow v+\\Delta v$$  \n",
    " <br>\n",
    " $$已知：E_k=\\frac{1}{2}\\sum_{i=1}^l(\\hat{y_j}^K- y_j^k)^2 \\quad \\hat{y}_i^K =f(\\beta_j-\\theta_j)\\quad f(x)=\\frac{1}{1+e^{(-x)}}\\quad \\beta_j=\\sum_{i=1}^qw_{hj}b_h \\quad b_h=f(b_h-\\gamma_h) \\quad b_h=\\sum_{i=1}^l v_{ih}x_i$$  \n",
    " <br>\n",
    "  $$已知sigmod导数性质：f'(x)=f(x)(1-f(x)\\;)$$  \n",
    "  <br>\n",
    "    $$\\frac{\\partial E_k}{\\partial w_{hj}}=\\frac{\\partial E_k}{\\partial \\hat{y_j}^k} \\cdot \\frac{\\partial \\hat{y_j}^k}{\\partial\\beta_{j}} \\cdot \\frac{\\partial\\beta_{j}}{\\partial w_{hj}}$$  \n",
    "   <br>  \n",
    "   $$\\frac{\\partial E_k}{\\partial w_{hj}}= (\\hat{y_j}^K- y_j^k)\\cdot \\hat{y_j}^k(1-\\hat{y_j}^k) \\cdot b_h \\quad 令为：g_j\\quad \\color{#F00}{公式一}$$   \n",
    "   <br>  \n",
    "   $$故：w_{hj} \\Leftarrow w_{hj} +\\eta g_j$$   \n",
    "   <br>  \n",
    "  $$\\frac{\\partial E_k}{\\partial v_{ih}}=\\sum_{j=1}^q\\frac{\\partial E_k}{\\partial \\hat{y_j}^k} \\cdot \\frac{\\partial \\hat{y_j}^k}{\\partial\\beta_{j}} \\cdot \\frac{\\partial\\beta_{j}}{b_{h}}\\cdot \\frac{\\partial b_h}{\\partial v_{ih}}$$     \n",
    "   <br>    \n",
    "   $$\\frac{\\partial E_k}{\\partial v_{ih}}= \\sum_{j=1}^q(\\hat{y_j}^K- y_j^k)\\cdot \\hat{y_j}^k(1-\\hat{y_j}^k) \\cdot w_{hj} \\cdot b_h(1-b_h)\\cdot x_i\\quad $$    \n",
    " <br>      \n",
    "$$\\frac{\\partial E_k}{\\partial v_{ih}}= b_h(1-b_h) \\cdot \\sum_{j=1}^q g_j\\cdot w_{hj} \\cdot x_i\\quad 令为：e_h\\quad \\color{#F00}{公式二}$$   \n",
    " <br> \n",
    "$$故：v_{ih} \\Leftarrow v_{ih} +\\eta e_h$$    \n",
    "  \n",
    "    \n",
    " <br>\n",
    " <br>\n",
    " 由上述过程可以看出误差逆传播的过程为：  \n",
    "  \n",
    "  由公式一和公式二：可以看出，越靠近输出层的参数，先求导，调整该层的参数；然后再将求的导逆向传播到远离输出层的隐层作为该层参数求导的系数参与该层参数的求导。  \n",
    "  \n",
    "  所以整个过程是：先将样例$(x_k,y_k)$提供给输入层神经元，然后逐层将信号前传，知道产生输出结果，然后计算输出层的误差，然后将误差逆向传播，调整各层的权重。  \n",
    "   \n",
    "     \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
