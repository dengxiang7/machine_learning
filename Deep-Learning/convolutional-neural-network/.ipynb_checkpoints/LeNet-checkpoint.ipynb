{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet 卷积神经网络\n",
    "\n",
    " 卷积神经网络：就是含卷积的神经网络；LeNet是早期的卷积神经网络，LeNet卷积神经网络：分为卷积层和全连接层两个部分  \n",
    "   \n",
    " <br>\n",
    " 卷积层：\n",
    " <br>\n",
    "   $\\quad\\quad$卷积层块：里基本单元为卷积层后接最大池化层，而卷积层块是由基本单元堆成。其中卷积层用来识别线条和物体的局部，最大池化层用来降低位置的敏感度  \\\n",
    "     \n",
    "### 网络结构     \n",
    "\n",
    "![](../../img/Pattern_recognition/neural_networks/lenet2.PNG)\n",
    "\n",
    "   \n",
    "   <br>  \n",
    "   <br>  \n",
    "   \n",
    " \n",
    "### 层次分析    \n",
    "  \n",
    "   $\\qquad\\qquad\\quad输入\\;\\Rightarrow\\;卷积C1\\;\\Rightarrow\\;池化P2\\;\\Rightarrow\\;卷积C3\\;\\Rightarrow\\;池化P4\\;\\Rightarrow\\;卷积C5\\;\\Rightarrow\\;全连接隐层F6\\;\\Rightarrow\\;输出O7$ \n",
    "      \n",
    "      \n",
    "卷积层C1:$\\quad\\;$在整个图片上进行卷积；大致相当于进行缩放，整体上可以特区特征  \n",
    "$\\qquad\\qquad\\;\\;$输入图片大小为$32 \\times 32$  \n",
    "$\\qquad\\qquad\\;\\;$通道数6，卷积核大小为$5 \\times 5$的卷积层;  \n",
    "$\\qquad\\qquad\\;\\;$特征图数目：输出6个特征图，特征图大小为$28 \\times 28$，  \n",
    "$\\qquad\\qquad\\;\\;$训练参数个数：6个通道，每个通道1个$5 \\times 5$卷积核，每个通道配一个阈值：$6\\times5 \\times 5 +6=156$ ，  \n",
    "$\\qquad\\qquad\\;\\;$神经网络：$输入节点\\;\\Rightarrow\\;权重（卷积核）\\;\\Rightarrow\\;输出节点$，；所共有$6\\times28 \\times 28 =4704$个神经元\n",
    "![](../../img/Pattern_recognition/neural_networks/lenet5.PNG)  \n",
    "  \n",
    " <br>   \n",
    "    \n",
    " 池化P2:$\\quad\\;\\;$为$2 \\times 2$的最大池化层  \n",
    "$\\qquad\\qquad\\;\\;$训练参数个数：没有训练参数  \n",
    "$\\qquad\\qquad\\;\\;$特征图数目：输出6个特征图，特征图大小为$14 \\times 14$  \n",
    "$\\qquad\\qquad\\;\\;$神经元个数：$6\\times114 \\times 14 =1176$\n",
    "   \n",
    " <br>  \n",
    " \n",
    " 卷积层C3:$\\quad$ 输入6个特征图，图片大小为$14 \\times 14$  \n",
    "$\\qquad\\qquad\\;\\;$通道数16，卷积核大小为$5 \\times 5$的卷积层;由于输入通道数为6；所以每一个输出通道有6个卷积核.  \n",
    "$\\qquad\\qquad\\;\\;$特征图数目：输出16个特征图，特征图大小为$10 \\times 10$.  \n",
    "$\\qquad\\qquad\\;\\;$训练参数个数：$16\\times（6\\times5 \\times 5) +16=2416$  \n",
    "$\\qquad\\qquad\\;\\;$神经网络：$输入节点\\;\\Rightarrow\\;权重（卷积核）\\;\\Rightarrow\\;输出节点$，；所共有$16\\times10 \\times 10 =1600$个神经元  \n",
    " ![](../../img/Pattern_recognition/neural_networks/lenet4.PNG)   \n",
    "   \n",
    "   <br> \n",
    "   \n",
    "池化P4:$\\quad\\;\\;$为$2 \\times 2$的最大池化层  \n",
    "$\\qquad\\qquad\\;\\;$训练参数个数：没有训练参数  \n",
    "$\\qquad\\qquad\\;\\;$特征图数目：输出16个特征图，特征图大小为$5 \\times 5$  \n",
    "$\\qquad\\qquad\\;\\;$神经元个数：$16\\times5 \\times 5 =400$  \n",
    "  \n",
    "    \n",
    " <br>  \n",
    " \n",
    " 卷积层C5:$\\quad$ 输入16个特征图，图片大小为$5 \\times 5$  \n",
    "$\\qquad\\qquad\\;\\;$通道数120，卷积核大小为$5 \\times 5$的卷积层;由于输入通道数为16；所以每一个输出通道有16个卷积核.  \n",
    "$\\qquad\\qquad\\;\\;$特征图数目：输出120个特征图，特征图大小为$1 \\times 1$.  \n",
    "$\\qquad\\qquad\\;\\;$训练参数个数：$120\\times（16\\times5 \\times 5) +120=48120$  \n",
    "$\\qquad\\qquad\\;\\;$神经网络：$输入节点\\;\\Rightarrow\\;权重（卷积核）\\;\\Rightarrow\\;输出节点$，；所共有$120\\times1  =120$个神经元   \n",
    "    \n",
    "  <br>\n",
    "    \n",
    "连接层F6:$\\quad$ 输入120个节点数据  \n",
    "$\\qquad\\qquad\\;\\;$本层神经元节点数 84个  \n",
    "$\\qquad\\qquad\\;\\;$训练参数个数 $120 \\times 84 + 84=10164$  \n",
    "    \n",
    " <br>   \n",
    "    \n",
    "  输出层O7:$\\quad$ 输入84个节点数据  \n",
    " $\\qquad\\qquad\\;\\;$本层神经元节点数 10个  \n",
    " $\\qquad\\qquad\\;\\;$训练参数个数$84 \\times 10 =840$    \n",
    "   <br>\n",
    "   <br>\n",
    "   <br>\n",
    "   \n",
    "#### 在以前计算能力差所以为了减少参数个数，把从S2到C3的全连接改为不全连接;现在可以全连接\n",
    "![](../../img/Pattern_recognition/neural_networks/lenet3.PNG)  \n",
    "  \n",
    "    \n",
    "      \n",
    "  <br>  \n",
    "    \n",
    "### LeNet 训练过程  \n",
    "![](../../img/Pattern_recognition/neural_networks/lenet6.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on\n",
      "epoch 1, loss 2.3424, train acc 0.147, test acc 0.132, time 2.6 sec\n",
      "epoch 2, loss 2.2801, train acc 0.159, test acc 0.179, time 2.5 sec\n",
      "epoch 3, loss 2.2750, train acc 0.157, test acc 0.179, time 2.5 sec\n",
      "epoch 4, loss 2.2758, train acc 0.158, test acc 0.179, time 2.5 sec\n",
      "epoch 5, loss 2.2749, train acc 0.163, test acc 0.179, time 2.6 sec\n",
      "epoch 6, loss 2.2729, train acc 0.160, test acc 0.132, time 2.6 sec\n",
      "epoch 7, loss 2.2738, train acc 0.163, test acc 0.179, time 2.5 sec\n",
      "epoch 8, loss 2.2734, train acc 0.162, test acc 0.179, time 2.5 sec\n",
      "epoch 9, loss 2.2733, train acc 0.162, test acc 0.179, time 2.5 sec\n",
      "epoch 10, loss 2.2728, train acc 0.161, test acc 0.179, time 2.5 sec\n"
     ]
    }
   ],
   "source": [
    "# 案列CNN训练手写数字识别\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import autograd, gluon, init, nd\n",
    "from mxnet.gluon import loss as gloss, nn\n",
    "import time\n",
    "import datafile.tools as tl\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Conv2D(channels=6, kernel_size=5, activation='sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(channels=16, kernel_size=5, activation='sigmoid'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        # Dense会默认将(批量大小, 通道, 高, 宽)形状的输入转换成\n",
    "        # (批量大小, 通道 * 高 * 宽)形状的输入\n",
    "        nn.Dense(120, activation='sigmoid'),\n",
    "        nn.Dense(84, activation='sigmoid'),\n",
    "        nn.Dense(10))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "batch_size = 120\n",
    "train_label,train_img=tl.readallimgandwritedata('datafile/usps_train_5.jf','datafile/num_img.txt')\n",
    "test_label,test_img=tl.readallimgandwritedata('datafile/usps_test_4.jf','datafile/test_num_img.txt')\n",
    "#print(train_img.shape[0])\n",
    "#tl.showimg2(train_img,train_label)\n",
    "\n",
    "\n",
    "def readbatch_size(batch_size,train_img,train_label):\n",
    "    num_examples =train_img.shape[0]  # 返回数量样本数\n",
    "    indices = list(range(num_examples)) # 1000个样本列表化\n",
    "    random.shuffle(indices)  # 样本的读取顺序是随机的\n",
    "    for i in range(0, num_examples, batch_size): #按batch_size步长取数\n",
    "        img=[]\n",
    "        label=[]\n",
    "        j = indices[i: min(i + batch_size, num_examples)]\n",
    "        for q in range(len(j)):\n",
    "            img.append(train_img[j[q]].reshape(1,256))\n",
    "            label.append(train_label[j[q]])\n",
    "        img=np.array(img)\n",
    "        label=np.array(label)\n",
    "        yield  img.reshape((len(j),1,16,16)) ,label\n",
    "    \n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    for X, y in data_iter:\n",
    "        # 如果ctx代表GPU及相应的显存，将数据复制到显存上\n",
    "        acc_sum += (net(X).argmax(axis=1) == y).sum()\n",
    "        n += y.size\n",
    "    return acc_sum.asscalar() / n\n",
    "    \n",
    "def train_ch5(net, train_img,train_label,test_img,test_label,batch_size, trainer,\n",
    "              num_epochs):\n",
    "    print('training on')\n",
    "    loss = gloss.SoftmaxCrossEntropyLoss()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X,y in readbatch_size(batch_size,train_img,train_label):\n",
    "            X=nd.array(X)\n",
    "            y=nd.array(y)\n",
    "           \n",
    "            with autograd.record():\n",
    "                y_hat = net(X)\n",
    "                l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            y = y.astype('float32')\n",
    "            train_l_sum += l.asscalar()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().asscalar()\n",
    "            n += y.size\n",
    "        nt=0.0\n",
    "        tacc_sum=0.0\n",
    "        for X,y in readbatch_size(batch_size,test_img,test_label):\n",
    "            X=nd.array(X)\n",
    "            y=nd.array(y)\n",
    "            tacc_sum += (net(X).argmax(axis=1) == y).sum().asscalar()\n",
    "            nt += y.size\n",
    "       \n",
    "        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n",
    "              'time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / n, train_acc_sum / n, tacc_sum/nt,\n",
    "                 time.time() - start))\n",
    "\n",
    "lr, num_epochs = 0.9,10\n",
    "net.initialize(force_reinit=True,  init=init.Xavier())\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': lr})\n",
    "train_ch5(net, train_img,train_label,test_img,test_label,batch_size, trainer,num_epochs)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
