{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN （图卷积）  \n",
    "  \n",
    "## 无向图  \n",
    "  \n",
    "  定义一个无向图$G(V,E)$ , 顶点集为$V=\\{v_1,v_2,……，v_n\\}$，边集为$E$；对于边集有两中设定1、0-1设定，2、权重W设定。  \n",
    "    \n",
    "  对于一个无向图有3个重要矩阵和一个重要性质矩阵。  \n",
    "    \n",
    "* #### 邻接矩阵A (adjacency matrix)  \n",
    "  \n",
    "  用来表示节点间的连接关系。两种设定：1、0-1设定  2、权重设定W。 （无向图的邻接矩阵是对称矩阵。有向图矩阵不是对称矩阵）  ‘\n",
    "    \n",
    "* #### 度矩阵D (degree matrix)    \n",
    "  \n",
    "  一个节点的度：指的是这个节点与其相连的节点的个数。$D_{ii}=\\sum_{j=1}^n A_{ij}$  。（度矩阵是对角矩阵。只有对角线上有元素）  \n",
    "    \n",
    "* #### 特征矩阵X   \n",
    "  \n",
    "   用来表示节点的特征。每个节点有F维。总共N个节点。则 $X=R^{N\\times F}$  \n",
    "     \n",
    "* #### 拉普拉斯矩阵 L  \n",
    "   \n",
    "   拉普拉斯矩阵 L 的定义：$L=D-W\\quad$($L=$度矩阵D-邻接矩阵A)$\\;\\;\\;$(对角矩阵-对称矩阵)  \n",
    "     \n",
    "   L的重要性质：  \n",
    "   1、对称矩阵  \n",
    "   2、由于L是对称矩阵，则所有特征值都是实数  \n",
    "   3、对任意向量$f$，都有$f^TLf=\\frac{1}{2}\\sum_{i,j=1}^n A_{ij}(f_i -f_j)^2 \\quad$(图正则化项)  \n",
    "   4、L是半正定的，n个特征值$\\lambda\\geq 0$  \n",
    "     \n",
    "<br>  \n",
    "\n",
    "#### 图的正则化项    \n",
    "   \n",
    "  无向图中两个子图A,B之间度：$W(A ,B)=\\sum_{i\\in A,j\\in B} A_{ij}$\n",
    "  \n",
    "  切图：$cut(A_1,A_2,……，A_k)=\\frac{1}{2} \\sum_{i=1}^k W(A_i ,\\hat{A_i})$\n",
    "   \n",
    "  所以图的正则化项：**就是让子图内度尽可能大，子图间度尽可能小。**  \n",
    "    \n",
    " 则正则化项：$L_{reg}=\\frac{1}{2} \\sum_{i=1}^k W(A_i ,\\hat{A_i})=\\sum_{i,j} A_{ij}||f_i-f_j||^2=f^TLf$   \n",
    "   \n",
    " <br>  \n",
    " \n",
    "  这个正则化项，限制了图模型的能力。因为图的边，不仅包含了两个顶点之间的相似性，可能还包含一些额外的重要信息。  \n",
    "    \n",
    "<br>  \n",
    "\n",
    "## GCN(图卷积)  \n",
    "  \n",
    "  思想：在每个类别的特征图X上做卷积得到的特征z。(是针对这个类别的特征)。好过在整个数据X做卷积得到的特征。**而邻接矩阵A,刚好表明了一个样本点其附近属于同一类别的样本。故：从邻接矩阵A中提取一个样本的附近属于同一类别的样本组成特征矩阵X，在这个特征矩阵上做卷积，效果更好。**  \n",
    "    \n",
    " GCN的过程：  \n",
    "   \n",
    "   1、筛选样本 $AH \\quad$（H是l层所得到的特征，在输入层就是X） ($A\\in R^{N \\times N} \\quad X \\in R^{N\\times M} \\quad$ 每一行是一个样本) \n",
    "     \n",
    "$$\n",
    "       \\begin{pmatrix}\n",
    "       0 &1 & \\cdots &  1 \\\\\n",
    "        1 & 0 & \\cdots &0 \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\ddots  \\\\\n",
    "        1 & 1 & \\cdots & 0 \\\\\n",
    "        \\end{pmatrix} \n",
    "        \\times\n",
    "         \\begin{pmatrix}\n",
    "        x_{11} & x_{12} & \\cdots &  x_{18} \\\\\n",
    "        x_{21} & x_{22}& \\cdots & x_{28} \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\ddots  \\\\\n",
    "        x_{71} & x_{72} & \\cdots & x_{78} \\\\\n",
    "        \\end{pmatrix}\n",
    "   $$  \n",
    "     \n",
    "   这里筛选样本时，由于邻接矩阵A的特性，去除了样本点自身，所以现在要加上。  \n",
    "  $$\\hat{A}=A+I $$  \n",
    "  \n",
    "  \n",
    "  $$\\text{筛选样本：}\\hat{A}H$$  \n",
    "    \n",
    "   2、对筛选样本组成的特征，共享权重W卷积核，做卷积运算 $\\hat{A}HW$  \n",
    "     \n",
    "   3、为了降低梯度训练难度，对A进行对称归一化。即A得每行和为1。 $\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}HW$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
