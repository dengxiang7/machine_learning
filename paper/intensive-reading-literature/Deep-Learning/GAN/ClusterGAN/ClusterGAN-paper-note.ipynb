{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 聚类GAN: 对抗生成网络中潜在聚类空间\n",
    "\n",
    "### 摘要\n",
    "对抗生成网络在无监督学习中已经获得了显著的成果。而聚类是无监督学习中一个重要的问题。我们可以利用GAN的潜在反向映射空间去聚类。但是我们证明了聚类结构在GAN的潜在空间不是保持不变的。 在这篇文章中提出一个新的机制clusterGAN。  \n",
    "  \n",
    "**通过从一个one-hot编码变量和一个连续潜在变量的混合变量中采样潜在变量。再联合训练一个逆网络（将数据投影到潜在空间），和特定的聚类损失函数，就可以再潜在空间中实现聚类了**    \n",
    "  \n",
    "实验结果显示，尽管鉴别器从未接触过这类向量，但GANs可以保留跨类别的潜在空间插值。   \n",
    "\n",
    " \n",
    "### 引言\n",
    "###### 1、1 动机\n",
    "表示学习能够使机器学习模型破译数据中潜在的语义和找出隐藏的关键元素变量。这些强大的表示能够在不同的任务中传递知识。但是是什么因素决定了一种表示比另一种表示更好呢？  经过一些研究在不依赖于目标任务的通用优先级中，那些表现良好的表征有一些共同的特点。  \n",
    "  \n",
    "聚类在无监督学习中得到了广泛的研究，有各种各样的方法寻求有效的算法、特点问题的度量问题，验证问题等。  \n",
    "  \n",
    "尽管聚类任务的主要焦点是把原始数据进行分类。但是研究已经表明了：如果聚类是在降维的同时获得的，那么聚类的结果将会更好。而实际上真实数据也来自低维流形。  \n",
    "  \n",
    "**在最近一段时间，许多无监督学习任务的研究成果是由深度生成模型驱动，其中最突出的两个成果就是1、变分自编码器（VAE）。2、对抗生成网络（GAN）。\n",
    "生成模型本身的流行取决于这些模型捕捉高维概率分布、缺失数据插补和处理多模态输出的能力。GAN和VAE的目标都是匹配真实数据分布，同时提供从潜在空间z到输入空间x的映射。**  \n",
    "  \n",
    "**GAN性质：  \n",
    "GAN的潜在空间不仅提供了降维，而且产生了新颖的应用。  \n",
    "潜在空间的扰动可以生成对抗性的样本，这些对抗性的样本可以用来产生健壮的分类。  \n",
    "使用GAN的压缩感知依赖于找到一组潜在的向量，这组向量能够使重构误差最小。  \n",
    "GAN训练最吸引人的结果之一是在潜在空间内插值。  \n",
    "简单向量的算术性质表现为向量的操作会导致生成语义的变化  \n",
    "GAN和传统的降维技术相比是缺乏可解释性**\n",
    "  \n",
    "    \n",
    "<br>  \n",
    "\n",
    "$\\color{red}{ 使用GAN聚类的动机：}$  \n",
    "$\\color{red}{GAN提供了一种手段来理解：在潜在空间中从一种类型向另一种类型变化时，高维特征的改变。 }$   \n",
    "$\\color{red}{这种变化的关键因素有：聚类，可解释性，插值能力；  利用这些关键因素可以进行聚类 。  }$  \n",
    "$\\color{red}{这些变化的内在因素就是我们使用GAN进行聚类的重要动机。“我们能否设计一种在潜在空间进行聚类的GAN训练方法？”}$    \n",
    "  \n",
    "<br>  \n",
    "\n",
    "###### 1、2  相关工作\n",
    "深度网络用于降维工作是从自编码器变体开始的（如堆栈式去噪自编码器，稀疏自编码器，deep CCA）。  \n",
    "  \n",
    "深度无监督子空间聚类的结构：也建立在（ 编码器-解码器）框架的基础上。  \n",
    "最近的一些工作已经解决了在自编码器上实现联合聚类和降维的问题。解决方法是初始化聚类中心并把它嵌入到堆栈式自编码器内。然后使用交替优化的方法提升聚类，并且在真实数据集上报告最新的结果。这种方法在聚类精度和聚类速度上都提高了不少。 **最著名的算法是DEC ， DCN**   \n",
    "  \n",
    "有趣的是变分自编码器并没有明确的聚类目标。但是通过改进可以达到聚类的目的。又因为GAN在生成高可信样本方面表现得自编码器好。所以我们直觉上觉得GAN强大的潜在表征空间也能提供优异的聚类性能。  \n",
    "  \n",
    "**在文献[4]的开创性工作中，研究了GAN潜在空间中的可解释表征学习。作者训练一个GAN，在损失函数中添加一个附加项，目标是，最大化生成器的随机噪声变量子集和产生的输出之间的互信息。**   \n",
    "  \n",
    "InfoGAN的关键目标是创建可解释的和分离的潜在变量。 虽然InfoGAN确实使用了离散的潜在变量，但它并不是专门为聚类而设计的。 在本文中，证明了他们提出的体系结构在聚类方面优于InfoGAN。  \n",
    "  \n",
    "**另一个重要的生成模型家族V-AE，还有额外的优势， 即推断网，编码器。这使得通过合适的算法设计能让从x到z的映射能够保持聚类结构。**  \n",
    "  \n",
    "不幸的是，在GANs中不存在这样的推理机制，更不用说潜在空间中存在聚类的可能性。  \n",
    "  \n",
    "为了弥补V-AE和GAN之间的差距，各种方法，如对抗学习推理（ALI）[8]、双向生成对抗网络（BiGAN）[7]都引入了一个推理网络，该网络被训练成匹配编码器E和解码器G网络学习的（x，z）的联合分布。  \n",
    "  \n",
    "  \n",
    "##### 1.3 作者的主要工作  \n",
    "  \n",
    "  要解决的问题：GAN潜在空间聚类问题。  \n",
    "    \n",
    "我们发现：即使GAN的潜在变量保留了观测数据的信息。但是基于潜在分布的潜在样本被平滑的分散，导致没有观测到聚类簇。   \n",
    "  \n",
    "作者提出了三种算法思想：来弥补这种情况。    \n",
    "  \n",
    "1、利用离散潜在变量的和连续潜在变量的混合，在潜在空间中创建一个非光滑的几何。  \n",
    "  \n",
    "2、由于问题是非凸的，作者提出了一种新的适应离散连续混合的反向传播算法，以及一个显式的逆映射网络来获得给定数据点的潜在变量。  \n",
    "  \n",
    "3、作者提出，将GAN网络与有特定聚类损失函数的逆映射网络一起联合训练。 目的是让投影空间中的几何距离来反应变量的几何距离。  \n",
    "    \n",
    "    \n",
    "我们比较了ClusterGAN和其他可能的基于GAN的聚类算法，比如InfoGAN，以及对不同数据集的多个聚类标准。这都说明ClusterGAN对于聚类任务的优越性能。  \n",
    "  \n",
    "我们证明ClusterGAN在不同的类（使用ong-hot潜在变量编码）之间惊人地保持了良好的插值，即使鉴别器从未接触过这样的样本。  \n",
    "  \n",
    "该公式具有足够的通用性，可以提供一个元框架，在GAN训练中加入聚类的附加属性。  \n",
    "  \n",
    "<br>  \n",
    "  \n",
    "### 先验的  离散-连续  潜在变量  \n",
    "\n",
    "#### 2.1背景  \n",
    "  \n",
    " 生成对抗网络（GAN）由两个部分组成：生成器G和判别器D。 生成器和判别器通常用神经网络实现，其参数分别是$\\theta_g$和$\\theta_d$。  \n",
    "   \n",
    " 生成器可以看作从潜在空间z到数据空间x的映射：$G: z \\rightarrow x$  \n",
    "   \n",
    " 判别器可以看作从数据空间到真实数值的映射，这个真实数值可以对应数据样本属于真实数据样本的概率：$D: x \\rightarrow R$   \n",
    "   \n",
    "#### 2.2 Vanilla GAN 在潜在空间中没有很好的进行聚类   \n",
    "  \n",
    "**使用GAN进行聚类的一种可行方法就是：将数据反向传播到潜在空间（使用反向传播编码），然后对潜在空间进行聚类。然而这种方法的结果确非常糟糕**  \n",
    "\n",
    "**原因：** 如果反向传播确实成功了，那么反向投影的数据分布应该和数据本身的潜在空间分布z，极其相似。 而GAN的潜在分布z通常选择高斯分布或者均匀分布。所以我们在高斯分布或者均匀分布的潜在空间中聚类，并不能得到好的结果。因此，即使潜在空间可能包含关于数据的全部信息，潜在空间中的距离几何并不反映固有的聚类。  \n",
    "作者从一个先验的高斯混合分布中抽样，并获得了不同的样本，即使在有限的数据区域。然而即使潜在分布选择的是高斯混合分布，上述方法也不能得到很好的聚类。 \n",
    "  \n",
    "已经观测到高斯混合分布趋向于拥挤，且变得多余。使用范畴变量提升空间可以有效解决这个问题。  \n",
    "  \n",
    "但传统上，潜空间的连续性被认为是实现良好插值目标的先决条件。  \n",
    "  \n",
    "换句话说，插值似乎与聚类目标发生了冲突。  \n",
    "  \n",
    "本文证明了ClusterGAN如何同时获得良好的插值和良好的聚类。  \n",
    "  \n",
    "#### 2.3 从 离散-连续 混合物中采样  \n",
    "  \n",
    "  ClusterGAN从由正态随机变量和one-hot编码向量级联而成的先验分布中随机采样。  \n",
    "    \n",
    " 即：$z=(z_n ，z_c) \\qquad z_n \\text{~}N(0,\\sigma^2Id_n) \\qquad z_c = e_k  \\qquad  k \\text{~} \\mu \\{1,2,……，k\\}$  \n",
    "   \n",
    " k是聚类个数，$e_k$是$R^K$中第k个元素向量。  \n",
    "   \n",
    "另外，我们选择$\\sigma$的方法为 ：one-hot向量为GAN训练提供了足够的信号，从而导致每个模式只从原始数据中的相应类生成样本。更确切的说，我们在所有实验中选择$\\sigma = 0.10 $，目的是让每一个维度 的正态潜在变量$z_{n,j} \\in (-0.6,0.6) << 1 ,\\forall j$都有大的概率。  \n",
    "  \n",
    " 选择小方差σ来保证Z空间中的聚类是可分离的。  \n",
    "   \n",
    "这种混合的先验知识能够使我们设计一种在潜在空间中聚类的算法。  \n",
    "  \n",
    "#### 2.4 基于解码器改进反向传播算法    \n",
    "  \n",
    "在[6]和[19]的先前工作中，探索了在潜在空间z中恢复潜在最优向量$z^*$的优化问题工作,$Z^*=\\underset {z}{arg\\;min}\\;L(g(z),x) + \\lambda||z||_p$  \n",
    "  \n",
    " 其中L是合适的损失函数，g(z)，是生成器，$||.||_p $表示p范数。  \n",
    "   \n",
    "但是，对于传统的使用潜在先验去进行聚类来说这种方法并不充分（即使这种方法的反向传播算法是无损的，并且能够准确地恢复潜在向量）。  \n",
    "  \n",
    "更糟糕的是，上诉优化问题存在两个方面问题，1、上述优化问题在z上是非凸的（G被实现为一个神经网络），2、基于不同的初始化，可能获得z空间的不同嵌入。（导致初始化对优化速度和能否获得最值的影响非常大）  \n",
    "    \n",
    "解决上述问题的一些方法是:多次用不同的初始化来重启网络，来获得最优$z^*$ 。或者在每个迭代步骤随机剪切z   \n",
    "   \n",
    "\n",
    "**这些不会导致聚类，是因为没有解决根本的问题，即从z中分离流行采样这个导致无法完成聚类的根本问题。**        \n",
    "  \n",
    "本文作者提供的采样方法是：  \n",
    "  \n",
    "  使用的损失函数为：$L(g(z),x)=||g(z)-x||_1$，因为我们从正态分布中取样，所以我们使用正则化子$||z_n||_2^2$,用来只惩罚正态变量。  \n",
    "    \n",
    "  我们使用K重启动，每个采样zc来自不同的one-hot组成，并仅针对对应的正态变量进行优化，保持zc不变  \n",
    "    \n",
    " 优化算法使用：用于Backprop decoding算法的优化是选用Adam  \n",
    "   \n",
    "     \n",
    "###### 算法一流程的总结  \n",
    "  \n",
    "  1、输入：真实样本x，生成函数G，聚类个数K，正则化参数$\\lambda$，Adam迭代τ  \n",
    "    \n",
    " 2、潜在最优嵌入$z^*$  \n",
    " \n",
    " 3、for  k ∈ {1,2, . . . K}   do \n",
    "   \n",
    " $\\qquad$4、从$ z_n^0 \\text{~}N(0,\\sigma^2Id_n)$中采用  \n",
    "   \n",
    " $\\qquad$5、初始化$z_k^0\\text{~}(z_n^0 ，e_k)$(ek是K维中的第K个基本单位向量)   \n",
    "   \n",
    " $\\qquad$6、 for t ∈ {1,2, . . . τ} do   (优化周期)  \n",
    "   \n",
    " $\\qquad\\qquad\\quad$7、获得损失函数的梯度  ![](img/1.PNG)\n",
    "   \n",
    "$\\qquad\\qquad\\quad$8、使用g迭代更新$z_n^t$,来达到最小化损失。  \n",
    "  \n",
    "$\\qquad\\qquad\\quad$9、剪切$z_n^t$ ![](img/2.PNG)  \n",
    "  \n",
    "$\\qquad\\qquad\\quad$10、$ z_k^t  \\leftarrow z_n^t ，e_k$  \n",
    "  \n",
    "$\\qquad\\qquad\\quad$11、如果$z_k^τ$是目前最小的损失，就把它赋给$z^*$  \n",
    "  \n",
    "12、返回$z^*$\n",
    "   \n",
    " \n",
    "<br>  \n",
    " \n",
    " \n",
    " <br>\n",
    " \n",
    " ### 完美的线性生成聚类\n",
    " \n",
    " 下面的引理表明，对于离散的连续混合体，我们只需要线性生成就可以在生成的空间中生成高斯混合体。   \n",
    "   \n",
    " * 引理一：  \n",
    "   \n",
    "    在线性生成的空间中，仅使用zn的聚类不能恢复高斯数据的混合。更进一步，存在一个生成器G(.)能够从离散-连续混合映射到高斯混合。  \n",
    "    \n",
    "      \n",
    " * 证明：  \n",
    "   \n",
    "    如果潜在空间仅有连续部分$ z_n \\text{~}N(0,\\sigma^2Id_n)$，然后根据线性特性，任何线性生成都只能在生成的空间中产生高斯分布。现在证明存在一个生成器G（.）在生成数据$X \\text{~}N(\\mu_\\omega,\\sigma^2Id_n)$上存在一个离散-连续混合的映射。（$\\omega \\text{~} \\mu\\{1,2,……，K\\}$ ，服从的是均匀分布，K是混合的个数）。这是可能的，如果我们让$z_n \\text{~}N(0,\\sigma^2Id_n) \\qquad z_c \\text{~} e_k  \\qquad  k \\text{~} \\mu \\{1,2,……，k\\}$ ，并且让$G(z_n,z_c)=z_n+Az_c  ，$，A=diag\\[u1,……,uk\\]，这里u1,uk是均值。  \n",
    "        \n",
    "<br>        \n",
    "      \n",
    "为了说明这个定理以及传统的先验分布$p_z$对于聚类的缺点，我们做一个小实验。真实样本从$R^{100}$的10个高斯混合数中提取样本。高斯的均值从$\\mu(-0.3,0.3)^100$中采样，每个组成高斯分布的方差被固定为$\\sigma=0.12$  。我们用$z \\text{~}N(0,I_{10})$来训练一个GAN，其中生成器是多层感知机，其有两个隐藏层，每个隐藏层有256个单元。作为比较，我们用z从one-hot编码的正态向量中采样 来训练一个GAN, (其中分类变量的维度是10) .  \n",
    "  \n",
    "<br>  \n",
    "\n",
    "在完成训练后。训练后，线性生成器使用算法1恢复潜在向量，非线性生成器使用随机初始化重新启动10个。线性发生器将潜在向量完美地聚集在一起。但是非线性生成器的性能很差。对于MNIST这样的实际数据集，当我们使用从均匀、正态或混合高斯中提取的潜在向量来训练GAN时，情况变得更糟。如图3所示，这些设置都没有成功地在潜在空间中集群。  \n",
    "  \n",
    "<br> \n",
    "\n",
    "<br>  \n",
    "\n",
    "### 针对数据中不同类别的分离模型  \n",
    "  \n",
    "  令人惊讶的是，在没有额外损失项的情况下，以完全无监督的方式进行训练，每个one-hot 编码的组成从原始数据中的特定类生成点。  \n",
    "    \n",
    " 例如，z=（zn，ek）在MNIST中生成了一个特定的数字π（k），对于多次采样从 zn∼N（0，σ2Idn）中（π表示置换）。  \n",
    "   \n",
    "这是算法1成功的必要的第一步。    \n",
    "  \n",
    "    \n",
    "<br>  \n",
    "  \n",
    "<br>  \n",
    "\n",
    "### 潜在空间内插保持不变  \n",
    "  \n",
    "  高斯分布的传统GAN的潜在空间要求不同的类在潜在空间中连续分布，允许类间插值，这是GANs的一个关键优势。在ClusterGAN中，潜在向量zc是以一个热分布采样的，为了在类之间插值，我们必须从一个热向量上的凸组合中采样。  \n",
    "   \n",
    "   虽然这些向量在训练过程中从未被采样，但我们意外地发现ClusterGAN中非常平滑的类间插值。  \n",
    "     \n",
    "       \n",
    "         \n",
    "<br>  \n",
    "  \n",
    "<br>  \n",
    "  \n",
    "### ClusterGAN  \n",
    "  \n",
    "  尽管上述方法使GAN能够在潜在空间中聚集，但是如果我们在minimax目标中有一个特定于聚类的损失项，那么它可能会表现得更好。对于MNIST来说，数字笔画与数据中的类别很好地对应。  \n",
    "    \n",
    "  **但是对于更复杂的数据集，我们需要在GAN训练中加强结构**  \n",
    "    \n",
    "**确保这一点的一种方法是强制精确地恢复潜在向量**  \n",
    "  \n",
    "**因此我们介绍一种编码$\\epsilon: x \\rightarrow z$ ，这是一个参数为$\\theta$的神经网络**  \n",
    "  \n",
    "**GAN网络的目标为：**    \n",
    "  \n",
    " ![](img/3.PNG)     \n",
    "      \n",
    " 其中H（，.）是交叉熵损失 ，正则化系数$β_n$和$β_c$的相对大小可以灵活地选择保持潜在代码的离散和连续部分的重要性。    \n",
    "   \n",
    "    \n",
    " 我们可以想象正则化的其他变体，其将E（G（z））映射为接近各自簇的质心，例如kE（G（z（i））−µc（i）k2 2，其精神与K均值类似。该方法中的GAN训练包括联合更新ΘGandΘE的参数（算法2）。  \n",
    "   \n",
    "   如图7所示，在我们的架构中，x都接近G（E（x）），z也接近E（G（z））。尽管我们的架构正在针对一种类型的周期损耗进行优化，但这两种损耗都很小。优化后的损耗更小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
